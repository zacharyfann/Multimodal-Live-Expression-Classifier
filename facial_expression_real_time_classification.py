# -*- coding: utf-8 -*-
"""Facial Expression Real-Time Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ypLOwrKuIshZu6EVGTuClzyQ4JkMJP7i
"""

#Combining data from cache1-3 for fine-tuning
from google.colab import drive
import pickle
from collections import Counter
import os

# Mount Google Drive
drive.mount('/content/drive')

# Paths to the cache files
cache_paths = {
    'cache1': '/content/drive/My Drive/cache1.pkl',
    'cache2': '/content/drive/My Drive/cache2.pkl',
    'cache3': '/content/drive/My Drive/cache3.pkl'
}

# Emotion map for reference
EMOTION_MAP = {0: "Neutral", 1: "Anger", 2: "Disgust", 3: "Fear", 4: "Happy", 5: "Sad", 6: "Surprise", 7: "Other"}

# Function to safely load and inspect a cache
def load_cache_safely(path):
    if not os.path.exists(path):
        print(f"Warning: {path} not found. Skipping.")
        return None, None

    with open(path, 'rb') as f:
        data = pickle.load(f)

    print(f"\nStructure of {os.path.basename(path)}:")
    print(f"Type: {type(data)}")
    print(f"Length: {len(data) if hasattr(data, '__len__') else 'N/A'}")
    if hasattr(data, '__getitem__') and len(data) > 0:
        first_item = data[0]
        print(f"First item type: {type(first_item)}")
        print(f"First item length: {len(first_item) if hasattr(first_item, '__len__') else 'N/A'}")
        if isinstance(first_item, (list, tuple)):
            print(f"First item fields: {[type(field) for field in first_item]}")

    # Assume structure: either a single list of pairs (full cache) or tuple (train, val)
    if isinstance(data, tuple):
        if len(data) == 2:
            train_pairs, val_pairs = data
        else:
            print(f"Unexpected tuple length {len(data)}; treating as single dataset.")
            train_pairs = data
            val_pairs = []
    else:
        # Assume single list of pairs
        train_pairs = data
        val_pairs = []

    return train_pairs, val_pairs

# Load and combine the caches
combined_train = []
combined_val = []
all_train_labels = []

for cache_name, path in cache_paths.items():
    train_pairs, val_pairs = load_cache_safely(path)

    combined_train.extend(train_pairs)
    combined_val.extend(val_pairs)

    # Extract labels (handle both int and dict structures)
    train_labels = []
    skipped = 0
    for pair in train_pairs:
        if not isinstance(pair, (list, tuple)) or len(pair) < 2:
            skipped += 1
            continue
        annotation = pair[1]
        if isinstance(annotation, int):
            label = annotation
        elif isinstance(annotation, dict) and 'expr' in annotation and isinstance(annotation['expr'], int):
            label = annotation['expr']
        else:
            skipped += 1
            if skipped <= 5:  # Print first 5 warnings only
                print(f"Warning: Unexpected pair structure in {cache_name}: {pair[:3] if len(pair) >= 3 else pair}")
            continue
        if 0 <= label <= 7:  # Valid label range
            train_labels.append(label)
        else:
            skipped += 1

    all_train_labels.extend(train_labels)
    print(f"Loaded {cache_name}: {len(train_pairs)} train, {len(val_pairs)} val samples")
    print(f"  - Valid labels extracted: {len(train_labels)}, skipped: {skipped}")

    # Per-cache counts
    cache_counts = Counter(train_labels)
    print(f"  Per-cache counts:")
    for label, count in sorted(cache_counts.items()):
        name = EMOTION_MAP.get(label, f"Unknown_{label}")
        print(f"    {name} (label {label}): {count}")

# Compute class counts for combined train
train_counts = Counter(all_train_labels)
print("\nCombined Train Class Counts:")
for label, count in sorted(train_counts.items()):
    name = EMOTION_MAP.get(label, f"Unknown_{label}")
    print(f"{name} (label {label}): {count}")

# Save combined cache (preserves full structure: paths, labels/annotations, VA, AU, etc.)
output_path = '/content/drive/My Drive/combined_cache.pkl'
with open(output_path, 'wb') as f:
    pickle.dump((combined_train, combined_val), f)
print(f"\nSaved combined cache to {output_path} (full annotations preserved)")

import pickle
import os
from collections import defaultdict
from tqdm import tqdm
import random

# Mount drive
from google.colab import drive
drive.mount('/content/drive')


base_dir = '/content/drive/My Drive/'
existing_val_path = os.path.join(base_dir, 'validation_cache_5k.pkl')
cache_paths = {
    1: os.path.join(base_dir, 'cache1.pkl'),
    2: os.path.join(base_dir, 'cache2.pkl'),
    3: os.path.join(base_dir, 'cache3.pkl'),
    4: os.path.join(base_dir, 'cache4.pkl'),
    5: os.path.join(base_dir, 'cache5.pkl')
}
new_val_save_path = os.path.join(base_dir, 'validation_cache_enhanced.pkl')

# Step 1: Load existing validation set
with open(existing_val_path, 'rb') as f:
    existing_val_pairs = pickle.load(f)
print(f"Loaded {len(existing_val_pairs)} existing val pairs.")

# Step 2: Load the 5th subset
with open(cache_paths[5], 'rb') as f:
    subset5_pairs = pickle.load(f)
print(f"Loaded {len(subset5_pairs)} pairs from subset 5.")

# Step 3: Define validity criteria
# Assuming: expr >= 0 (valid class), va[0] >= -1.0 and va[1] >= -1.0 (normalized range), all au >= 0 (present)
def is_valid_example(pair):
    _, labels = pair
    expr = labels['expr']
    va = labels['va']
    au = labels['au']
    if expr < 0:  # Invalid expression label
        return False
    if va[0] < -1.0 or va[1] < -1.0:  # Invalid VA
        return False
    if any(a < 0 for a in au):  # Any missing AU unit
        return False
    return True

# Filter subset5 for only valid examples
valid_subset5_pairs = [pair for pair in tqdm(subset5_pairs, desc="Filtering valid examples in subset5")]
print(f"Valid examples in subset5: {len(valid_subset5_pairs)}")

# Step 4: Prevent data leakage - Collect all train and existing val image paths
train_paths = set()
existing_val_paths = set([pair[0] for pair in existing_val_pairs])  # Image paths

# Load paths from subsets 1-4 (train pools)
for subset_id in range(1, 5):
    with open(cache_paths[subset_id], 'rb') as f:
        subset_pairs = pickle.load(f)
    subset_paths = {pair[0] for pair in subset_pairs}
    train_paths.update(subset_paths)
    print(f"Subset {subset_id} paths: {len(subset_paths)}")

all_forbidden_paths = train_paths.union(existing_val_paths)
print(f"Total forbidden paths (train + existing val): {len(all_forbidden_paths)}")

# Filter valid_subset5 to exclude any overlapping paths
non_leaking_pairs = [pair for pair in tqdm(valid_subset5_pairs, desc="Removing leaks") if pair[0] not in all_forbidden_paths]
print(f"Non-leaking valid pairs from subset5: {len(non_leaking_pairs)}")

# Step 5: Pool with existing val - shuffle to randomize
enhanced_val_pairs = existing_val_pairs + non_leaking_pairs
random.shuffle(enhanced_val_pairs)
print(f"Total enhanced val pairs: {len(enhanced_val_pairs)}")

# Step 6: Save the new validation cache
with open(new_val_save_path, 'wb') as f:
    pickle.dump(enhanced_val_pairs, f)
print(f"Saved enhanced validation set to {new_val_save_path}")

# Optional: Quick stats on enhanced val
expr_labels = [pair[1]['expr'] for pair in enhanced_val_pairs]
va_labels = [pair[1]['va'] for pair in enhanced_val_pairs]
au_labels = [pair[1]['au'] for pair in enhanced_val_pairs]
from collections import Counter
print("Enhanced Val EXPR distribution:", Counter([lbl for lbl in expr_labels if lbl >= 0]))
valid_va_count = sum(1 for va in va_labels if va[0] >= -1.0 and va[1] >= -1.0)
print(f"Valid VA samples in enhanced val: {valid_va_count} / {len(va_labels)}")
au_full_count = sum(1 for au in au_labels if all(a >= 0 for a in au))
print(f"Fully labeled AU samples: {au_full_count} / {len(au_labels)}")

import pickle
import os
from collections import Counter
import random
from tqdm import tqdm

# Assuming you've already run the previous code to create enhanced_val_pairs
# Load the enhanced val if not in memory
enhanced_val_path = '/content/drive/My Drive/validation_cache_enhanced.pkl'  # From previous code
with open(enhanced_val_path, 'rb') as f:
    enhanced_val_pairs = pickle.load(f)
print(f"Loaded {len(enhanced_val_pairs)} enhanced val pairs.")

# Modified class_balancer: Fixed target count per class (3376), with up/down-sampling
def class_balancer_fixed(pairs, target_count=3376, target_classes=None):
    """
    Balances the dataset on EXPR classes to a fixed target_count per class.
    - Upsamples classes with < target_count (with replacement).
    - Downsamples classes with > target_count (without replacement).
    - Keeps invalid (expr==-1) examples as-is.
    Optional: target_classes list to balance only specific classes (e.g., [0,1,2,3,4,5,6,7] for all).
    """
    expr_labels = [lbl['expr'] for _, lbl in pairs]
    valid_indices = [i for i, lbl in enumerate(expr_labels) if lbl >= 0]
    if not valid_indices:
        return pairs  # No valid to balance

    valid_expr = [expr_labels[i] for i in valid_indices]
    unique = list(set(valid_expr)) if target_classes is None else [c for c in target_classes if c in valid_expr]
    if not unique:
        return pairs

    sampled_indices = []
    for cls in unique:
        cls_indices = [valid_indices[j] for j in range(len(valid_expr)) if valid_expr[j] == cls]
        current_count = len(cls_indices)
        if current_count < target_count:
            # Upsample with replacement
            sampled_cls = random.choices(cls_indices, k=target_count)
        else:
            # Downsample without replacement
            sampled_cls = random.sample(cls_indices, target_count)
        sampled_indices.extend(sampled_cls)

    # Add back invalid examples (expr==-1)
    neg_indices = [i for i, lbl in enumerate(expr_labels) if lbl == -1]
    sampled_indices.extend(neg_indices)

    random.shuffle(sampled_indices)
    balanced_pairs = [pairs[i] for i in sampled_indices]
    return balanced_pairs

# Apply fixed balancing to enhanced val
balanced_val_pairs = class_balancer_fixed(enhanced_val_pairs, target_count=3376, target_classes=list(range(8)))

print(f"Balanced val size: {len(balanced_val_pairs)} (approx {len([p for p in balanced_val_pairs if p[1]['expr'] >= 0])} valid + invalids)")

# Quick stats: Check distribution
balanced_expr = [pair[1]['expr'] for pair in balanced_val_pairs]
print("Balanced Val EXPR distribution:", Counter(balanced_expr))

# Save the balanced version
balanced_val_save_path = '/content/drive/My Drive/validation_cache_balanced_fixed.pkl'
with open(balanced_val_save_path, 'wb') as f:
    pickle.dump(balanced_val_pairs, f)
print(f"Saved fixed balanced validation set to {balanced_val_save_path}")

import pickle
import os
from collections import Counter
import random
from tqdm import tqdm

# Load the enhanced val if not in memory
enhanced_val_path = '/content/drive/My Drive/validation_cache_enhanced.pkl'
with open(enhanced_val_path, 'rb') as f:
    enhanced_val_pairs = pickle.load(f)
print(f"Loaded {len(enhanced_val_pairs)} enhanced val pairs.")

# Modified class_balancer: Balance to fixed total size (10k), with equal shares for valid classes
def class_balancer_subset(pairs, total_target=10000, target_classes=None):
    """
    Balances valid EXPR classes equally, adds all invalids, then subsamples to total_target.
    Assumes equal allocation for valid classes; adjusts per-class target dynamically.
    - Upsamples/downsamples classes with < /> share.
    - Keeps all invalid (expr==-1) examples.
    Optional: target_classes list to balance only specific classes.
    """
    expr_labels = [lbl['expr'] for _, lbl in pairs]
    valid_indices = [i for i, lbl in enumerate(expr_labels) if lbl >= 0]
    neg_indices = [i for i, lbl in enumerate(expr_labels) if lbl == -1]
    num_invalids = len(neg_indices)

    if not valid_indices:
        # If no valids, subsample invalids to total_target
        if num_invalids > total_target:
            sampled_neg = random.sample(neg_indices, total_target)
            return [pairs[i] for i in sampled_neg]
        return pairs  # All invalids if under

    valid_expr = [expr_labels[i] for i in valid_indices]
    unique = list(set(valid_expr)) if target_classes is None else [c for c in target_classes if c in valid_expr]
    num_valid_classes = len(unique)

    # Allocate: invalids full, rest to valids (equal per class)
    per_class_target = (total_target - num_invalids) // num_valid_classes
    if per_class_target < 1:
        per_class_target = 1  # Min 1 per class

    sampled_indices = neg_indices[:]  # All invalids
    for cls in unique:
        cls_indices = [valid_indices[j] for j in range(len(valid_expr)) if valid_expr[j] == cls]
        current_count = len(cls_indices)
        if current_count < per_class_target:
            # Upsample with replacement
            sampled_cls = random.choices(cls_indices, k=per_class_target)
        else:
            # Downsample without replacement
            sampled_cls = random.sample(cls_indices, per_class_target)
        sampled_indices.extend(sampled_cls)

    # If still over total_target (due to invalids), subsample proportionally
    current_total = len(sampled_indices)
    if current_total > total_target:
        sampled_indices = random.sample(sampled_indices, total_target)

    random.shuffle(sampled_indices)
    balanced_pairs = [pairs[i] for i in sampled_indices]
    return balanced_pairs

# Apply subset balancing to enhanced val (all 8 classes)
subset_balanced_val_pairs = class_balancer_subset(enhanced_val_pairs, total_target=10000, target_classes=list(range(8)))

print(f"Subset balanced val size: {len(subset_balanced_val_pairs)}")

# Quick stats: Check distribution
subset_balanced_expr = [pair[1]['expr'] for pair in subset_balanced_val_pairs]
print("Subset Balanced Val EXPR distribution:", Counter(subset_balanced_expr))

# Save the subset balanced version
subset_balanced_save_path = '/content/drive/My Drive/validation_cache_subset_balanced_10k.pkl'
with open(subset_balanced_save_path, 'wb') as f:
    pickle.dump(subset_balanced_val_pairs, f)
print(f"Saved subset balanced validation set (10k) to {subset_balanced_save_path}")

# Ensemble 2 initial training
from google.cloud import storage
from google.colab import auth, drive
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import (GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Input, MultiHeadAttention, RandomRotation, RandomCrop, RandomZoom)
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau
import random
import pickle
from sklearn.utils.class_weight import compute_class_weight
import glob
from tensorflow.keras import regularizers
from tqdm import tqdm
from tensorflow.keras.applications import EfficientNetV2S
from tensorflow.keras.applications.efficientnet_v2 import preprocess_input
from tensorflow.keras import ops
from collections import Counter
import logging
import os
logging.basicConfig(level=logging.DEBUG)

auth.authenticate_user()
drive.mount('/content/drive')

# Check cache contents
cache_id = 2
cache_path = f'/content/drive/My Drive/cache{cache_id}.pkl'
print(f"\nInspecting contents of cache{cache_id}.pkl")
with open(cache_path, 'rb') as f:
    train_pairs = pickle.load(f)
print(f"Total samples in cache{cache_id}: {len(train_pairs)}")
expr_labels = [pair[1]['expr'] for pair in train_pairs]
va_labels = [pair[1]['va'] for pair in train_pairs]
au_labels = [pair[1]['au'] for pair in train_pairs]
print("EXPR class distribution:", Counter(expr_labels))
valid_va = [va for va in va_labels if va[0] >= -1.5 and va[1] >= -1.5]
print(f"Valid VA samples: {len(valid_va)} (out of {len(va_labels)})")
au_valid_counts = [sum(1 for au in au_row if au >= 0) for au_row in au_labels]
print(f"AU samples with all valid labels: {sum(1 for c in au_valid_counts if c == 12)}")
print("Sample pair structure:", train_pairs[0] if train_pairs else "Empty")

# Load new validation set
val_cache_path = '/content/drive/My Drive/validation_cache_subset_balanced_10k.pkl'
with open(val_cache_path, 'rb') as f:
    val_pairs = pickle.load(f)
print(f"Loaded {len(val_pairs)} validation pairs.")

# Class balancer - adapt for multi-task, balance on EXPR
def class_balancer(pairs):
    expr_labels = np.array([lbl['expr'] for _, lbl in pairs])
    valid_indices = [i for i, lbl in enumerate(expr_labels) if lbl >= 0]
    if not valid_indices:
        return pairs
    valid_expr = expr_labels[valid_indices]
    unique = np.unique(valid_expr)
    max_count = max(np.sum(valid_expr == cls) for cls in unique)
    sampled_indices = []
    for cls in unique:
        cls_indices = [valid_indices[j] for j in range(len(valid_expr)) if valid_expr[j] == cls]
        sampled_cls = random.choices(cls_indices, k=max_count)
        sampled_indices.extend(sampled_cls)
    random.shuffle(sampled_indices)
    neg_indices = [i for i, lbl in enumerate(expr_labels) if lbl == -1]
    sampled_indices.extend(neg_indices)
    random.shuffle(sampled_indices)
    return [pairs[i] for i in sampled_indices]

# Apply sampler to pairs
train_subset_size = len(train_pairs)
val_subset_size = len(val_pairs)
train_pairs_subset = class_balancer(random.sample(list(tqdm(train_pairs, desc="Sampling train subset")), min(train_subset_size, len(train_pairs))))
val_pairs_subset = random.sample(list(tqdm(val_pairs, desc="Sampling val subset")), min(val_subset_size, len(val_pairs)))

# Extract image paths and labels
train_image_paths = [pair[0] for pair in train_pairs_subset]
train_labels = [pair[1] for pair in train_pairs_subset]
val_image_paths = [pair[0] for pair in val_pairs_subset]
val_labels = [pair[1] for pair in val_pairs_subset]
print(f"Train samples: {len(train_labels)}, val samples: {len(val_labels)}")

# Convert labels to arrays
expr_train = np.array([lbl['expr'] for lbl in train_labels], dtype=np.int32)
va_train = np.array([lbl['va'] for lbl in train_labels], dtype=np.float32)
au_train = np.array([lbl['au'] for lbl in train_labels], dtype=np.int32)
train_label_dict = {'expr': expr_train, 'va': va_train, 'au': au_train}

expr_val = np.array([lbl['expr'] for lbl in val_labels], dtype=np.int32)
va_val = np.array([lbl['va'] for lbl in val_labels], dtype=np.float32)
au_val = np.array([lbl['au'] for lbl in val_labels], dtype=np.int32)
val_label_dict = {'expr': expr_val, 'va': va_val, 'au': au_val}

# Check valid labels in validation set
valid_expr_val = expr_val[expr_val >= 0]
print(f"Valid expr labels in val_dataset: {len(valid_expr_val)} out of {len(expr_val)}")

# Custom losses and metrics
@tf.keras.utils.register_keras_serializable()
def masked_focal_sparse_categorical_crossentropy(gamma=4.0, label_smoothing=0.05):
    def loss_fn(y_true_expr, y_pred_expr):
        y_true_expr = tf.cast(y_true_expr, tf.int32)
        mask = tf.not_equal(y_true_expr, -1)
        mask_f = tf.cast(mask, tf.float32)
        y_true_safe = tf.where(mask, y_true_expr, 0)
        alpha = tf.gather(class_weights_tensor, y_true_safe)
        y_true_one_hot = tf.one_hot(y_true_safe, depth=8)
        y_true_one_hot = (1 - label_smoothing) * y_true_one_hot + (label_smoothing / 8)
        ce = tf.keras.losses.categorical_crossentropy(y_true_one_hot, y_pred_expr, from_logits=False)
        pt = tf.exp(-ce)
        focal_loss = alpha * ((1 - pt) ** gamma) * ce
        focal_loss = tf.where(mask, focal_loss, tf.zeros_like(focal_loss))
        return tf.reduce_sum(focal_loss) / (tf.reduce_sum(mask_f) + tf.keras.backend.epsilon())
    return loss_fn

def ccc(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    mu_true = tf.reduce_mean(y_true)
    mu_pred = tf.reduce_mean(y_pred)
    var_true = tf.math.reduce_variance(y_true)
    var_pred = tf.math.reduce_variance(y_pred)
    cov = tf.reduce_mean((y_true - mu_true) * (y_pred - mu_pred))
    return (2 * cov) / (var_true + var_pred + (mu_true - mu_pred)**2 + 1e-6)

def va_loss(y_true_va, y_pred_va):
    mask_v = tf.greater(y_true_va[:,0], -1.5)
    mask_a = tf.greater(y_true_va[:,1], -1.5)
    v_true = tf.boolean_mask(y_true_va[:,0], mask_v)
    v_pred = tf.boolean_mask(y_pred_va[:,0], mask_v)
    a_true = tf.boolean_mask(y_true_va[:,1], mask_a)
    a_pred = tf.boolean_mask(y_pred_va[:,1], mask_a)
    ccc_v = ccc(v_true, v_pred)
    ccc_a = ccc(a_true, a_pred)
    return (1 - ccc_v + 1 - ccc_a) / 2

def au_loss(y_true_au, y_pred_au):
    mask = tf.not_equal(y_true_au, -1)
    y_true_masked = tf.where(mask, tf.cast(y_true_au, tf.float32), 0.0)
    return tf.keras.losses.binary_crossentropy(y_true_masked, y_pred_au)

@tf.keras.utils.register_keras_serializable()
class MulticlassF1Score(tf.keras.metrics.Metric):
    def __init__(self, num_classes=8, name='f1', **kwargs):
        super(MulticlassF1Score, self).__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.per_class_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fp = [self.add_weight(name=f'fp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fn = [self.add_weight(name=f'fn_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.int32)
        mask = tf.not_equal(y_true, -1)
        y_true = tf.boolean_mask(y_true, mask)
        y_pred = tf.boolean_mask(y_pred, mask)
        y_pred_classes = tf.argmax(y_pred, axis=-1, output_type=tf.int32)
        for i in range(self.num_classes):
            class_mask_true = tf.equal(y_true, i)
            class_mask_pred = tf.equal(y_pred_classes, i)
            tp = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, class_mask_pred), tf.float32))
            fp = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not(class_mask_true), class_mask_pred), tf.float32))
            fn = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, tf.logical_not(class_mask_pred)), tf.float32))
            self.per_class_tp[i].assign_add(tp)
            self.per_class_fp[i].assign_add(fp)
            self.per_class_fn[i].assign_add(fn)
    def result(self):
        prec = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fp[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        rec = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fn[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        f1_per_class = [2 * (prec[i] * rec[i]) / (prec[i] + rec[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        return tf.reduce_mean(tf.stack(f1_per_class))
    def reset_state(self):
        for i in range(self.num_classes):
            self.per_class_tp[i].assign(0.0)
            self.per_class_fp[i].assign(0.0)
            self.per_class_fn[i].assign(0.0)

@tf.keras.utils.register_keras_serializable()
class MulticlassPrecision(tf.keras.metrics.Metric):
    def __init__(self, num_classes=8, name='precision', **kwargs):
        super(MulticlassPrecision, self).__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.per_class_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fp = [self.add_weight(name=f'fp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.int32)
        mask = tf.not_equal(y_true, -1)
        y_true_masked = tf.boolean_mask(y_true, mask)
        y_pred_masked = tf.boolean_mask(y_pred, mask)
        y_pred_classes = tf.argmax(y_pred_masked, axis=-1, output_type=tf.int32)
        for i in range(self.num_classes):
            class_mask_true = tf.equal(y_true_masked, i)
            class_mask_pred = tf.equal(y_pred_classes, i)
            tp = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, class_mask_pred), tf.float32))
            fp = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not(class_mask_true), class_mask_pred), tf.float32))
            self.per_class_tp[i].assign_add(tp)
            self.per_class_fp[i].assign_add(fp)
    def result(self):
        precision_per_class = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fp[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        return tf.reduce_mean(tf.stack(precision_per_class))
    def reset_state(self):
        for i in range(self.num_classes):
            self.per_class_tp[i].assign(0.0)
            self.per_class_fp[i].assign(0.0)

@tf.keras.utils.register_keras_serializable()
class MulticlassRecall(tf.keras.metrics.Metric):
    def __init__(self, num_classes=8, name='recall', **kwargs):
        super(MulticlassRecall, self).__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.per_class_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fn = [self.add_weight(name=f'fn_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.int32)
        mask = tf.not_equal(y_true, -1)
        y_true_masked = tf.boolean_mask(y_true, mask)
        y_pred_masked = tf.boolean_mask(y_pred, mask)
        y_pred_classes = tf.argmax(y_pred_masked, axis=-1, output_type=tf.int32)
        for i in range(self.num_classes):
            class_mask_true = tf.equal(y_true_masked, i)
            class_mask_pred = tf.equal(y_pred_classes, i)
            tp = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, class_mask_pred), tf.float32))
            fn = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, tf.logical_not(class_mask_pred)), tf.float32))
            self.per_class_tp[i].assign_add(tp)
            self.per_class_fn[i].assign_add(fn)
    def result(self):
        recall_per_class = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fn[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        return tf.reduce_mean(tf.stack(recall_per_class))
    def reset_state(self):
        for i in range(self.num_classes):
            self.per_class_tp[i].assign(0.0)
            self.per_class_fn[i].assign(0.0)

@tf.keras.utils.register_keras_serializable()
class VA_CCC(tf.keras.metrics.Metric):
    def __init__(self, name='va_ccc', **kwargs):
        super().__init__(name=name, **kwargs)
        self.ccc_v_sum = self.add_weight(shape=(), name='ccc_v_sum', initializer='zeros')
        self.ccc_a_sum = self.add_weight(shape=(), name='ccc_a_sum', initializer='zeros')
        self.count = self.add_weight(shape=(), name='count', initializer='zeros')
    def update_state(self, y_true_va, y_pred_va, sample_weight=None):
        mask_v = tf.greater(y_true_va[:,0], -1.5)
        mask_a = tf.greater(y_true_va[:,1], -1.5)
        v_true = tf.boolean_mask(y_true_va[:,0], mask_v)
        v_pred = tf.boolean_mask(y_pred_va[:,0], mask_v)
        a_true = tf.boolean_mask(y_true_va[:,1], mask_a)
        a_pred = tf.boolean_mask(y_pred_va[:,1], mask_a)
        self.ccc_v_sum.assign_add(ccc(v_true, v_pred))
        self.ccc_a_sum.assign_add(ccc(a_true, a_pred))
        self.count.assign_add(1.0)
    def result(self):
        return (self.ccc_v_sum + self.ccc_a_sum) / (2 * self.count + tf.keras.backend.epsilon())
    def reset_state(self):
        self.ccc_v_sum.assign(0.0)
        self.ccc_a_sum.assign(0.0)
        self.count.assign(0.0)

@tf.keras.utils.register_keras_serializable()
class AU_F1(tf.keras.metrics.Metric):
    def __init__(self, name='au_f1', **kwargs):
        super().__init__(name=name, **kwargs)
        self.per_au_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(12)]
        self.per_au_fp = [self.add_weight(name=f'fp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(12)]
        self.per_au_fn = [self.add_weight(name=f'fn_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(12)]
    def update_state(self, y_true_au, y_pred_au, sample_weight=None):
        mask = tf.not_equal(y_true_au, -1)
        y_true_masked = tf.where(mask, tf.cast(y_true_au, tf.float32), tf.zeros_like(y_true_au, dtype=tf.float32))
        y_pred_bin = tf.round(y_pred_au)
        for i in range(12):
            tp = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(y_true_masked[:,i], tf.int32) == 1, tf.cast(y_pred_bin[:,i], tf.int32) == 1), tf.float32))
            fp = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(y_true_masked[:,i], tf.int32) == 0, tf.cast(y_pred_bin[:,i], tf.int32) == 1), tf.float32))
            fn = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(y_true_masked[:,i], tf.int32) == 1, tf.cast(y_pred_bin[:,i], tf.int32) == 0), tf.float32))
            self.per_au_tp[i].assign_add(tp)
            self.per_au_fp[i].assign_add(fp)
            self.per_au_fn[i].assign_add(fn)
    def result(self):
        prec = [self.per_au_tp[i] / (self.per_au_tp[i] + self.per_au_fp[i] + tf.keras.backend.epsilon()) for i in range(12)]
        rec = [self.per_au_tp[i] / (self.per_au_tp[i] + self.per_au_fn[i] + tf.keras.backend.epsilon()) for i in range(12)]
        f1_per_au = [2 * (prec[i] * rec[i]) / (prec[i] + rec[i] + tf.keras.backend.epsilon()) for i in range(12)]
        return tf.reduce_mean(tf.stack(f1_per_au))
    def reset_state(self):
        for i in range(12):
            self.per_au_tp[i].assign(0.0)
            self.per_au_fp[i].assign(0.0)
            self.per_au_fn[i].assign(0.0)

class PerClassMetricsCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        y_true_all = []
        y_pred_all = []
        for batch in val_dataset:
            images, labels = batch
            preds = self.model.predict(images, verbose=0)
            y_true_all.extend(labels['expr'].numpy())
            y_pred_all.extend(np.argmax(preds['expr'], axis=-1))
        y_true = np.array(y_true_all)
        y_pred = np.array(y_pred_all)
        mask = y_true != -1
        y_true = y_true[mask]
        y_pred = y_pred[mask]
        for cls in range(8):
            tp = np.sum((y_true == cls) & (y_pred == cls))
            fp = np.sum((y_true != cls) & (y_pred == cls))
            fn = np.sum((y_true == cls) & (y_pred != cls))
            prec = tp / (tp + fp + 1e-6)
            rec = tp / (tp + fn + 1e-6)
            f1 = 2 * prec * rec / (prec + rec + 1e-6)
            print(f"Class {cls}: F1 = {f1:.4f}, Prec = {prec:.4f}, Rec = {rec:.4f}")

# Data augmentation layers
rotation_layer = RandomRotation(factor=0.3)
crop_layer = RandomCrop(height=224, width=224)
zoom_layer = RandomZoom(0.1)

def load_and_preprocess_image(path, label):
    try:
        img = tf.io.read_file(path)
        img = tf.image.decode_jpeg(img, channels=3)
        img = tf.image.resize(img, [256, 256])
        img = tf.cast(img, tf.float32) / 255.0
        img = crop_layer(img)
        img = zoom_layer(img)
        img = tf.image.random_flip_left_right(img)
        img = tf.image.random_brightness(img, max_delta=0.15)
        img = tf.cast(img, tf.float32)
        img = tf.image.random_contrast(img, lower=0.9, upper=1.1)
        img = tf.image.random_hue(img, 0.07)
        img = tf.image.random_saturation(img, 0.8, 1.6)
        img = tf.image.random_flip_up_down(img)
        img = rotation_layer(img)
        full_img = preprocess_input(img * 255.0)
        return full_img, label
    except Exception as e:
        tf.print("Error loading", path, ":", e)
        dummy_img = tf.zeros((224, 224, 3), dtype=tf.float32)
        return dummy_img, {'expr': tf.constant(-1, dtype=tf.int32),
                           'va': tf.constant([-2.0, -2.0], dtype=tf.float32),
                           'au': tf.constant([-1]*12, dtype=tf.int32)}

# Strategy setup
strategy = tf.distribute.get_strategy()
batch_size = 512
from tensorflow.keras.mixed_precision import set_global_policy
set_global_policy('mixed_float16')
shuffle_buffer = 1024

num_epochs_per_model = 50

# Val dataset
val_dataset = tf.data.Dataset.from_tensor_slices((val_image_paths, val_label_dict))
val_dataset = val_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
validation_steps = len(val_image_paths) // batch_size

# Checkpoint dir and latest
checkpoint_dir = '/content/drive/My Drive/EXPR_Recognition Models/'
checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'new_mtl_model_cache2_epoch_*.weights.h5'))
latest_checkpoint = max(checkpoint_files, key=os.path.getctime) if checkpoint_files else None
latest_epoch = 0  # Reset to 0 due to potential checkpoint incompatibility

# Set initial unfreeze to 250
initial_unfreeze = 250

# Class weights for EXPR
valid_expr_labels = expr_train[expr_train >= 0]
unique_classes = np.unique(valid_expr_labels)
class_weights = compute_class_weight('balanced', classes=unique_classes, y=valid_expr_labels)
class_weights_dict = dict(zip(unique_classes, class_weights))
boost_factors = {
                 1: 6.0,  # Anger (very low F1—up from 5.0 for more emphasis)
                 2: 5.0,  # Disgust (low, keep aggressive)
                 3: 4.0,  # Fear (low, stable)
                 4: 1.0,  # Happy (high, no boost)
                 5: 2.0,  # Sad (medium, fine)
                 6: 3.0,  # Surprise (low F1—up from 2.0 to match anger/disgust level)
                 7: 1.5}  # Other (medium-low, unchanged)
for cls, factor in boost_factors.items():
    if cls in class_weights_dict:
        class_weights_dict[cls] *= factor
class_weights_list = [class_weights_dict.get(i, 1.0) for i in range(8)]
class_weights_tensor = tf.constant(class_weights_list, dtype=tf.float32)
print("EXPR Class weights:", class_weights_dict)

# Train dataset
train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, train_label_dict))
train_dataset = train_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.shuffle(buffer_size=shuffle_buffer).batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)
steps_per_epoch = len(train_image_paths) // batch_size

# Model creation
with strategy.scope():
    def create_model():
        full_input = Input(shape=(224, 224, 3), name='full')
        base_model = EfficientNetV2S(
            weights='imagenet',
            include_top=False,
            input_tensor=full_input,
            input_shape=(224, 224, 3),
            name='efficientnetv2-s'
        )
        base_model.trainable = True
        for layer in base_model.layers[:-initial_unfreeze]:
            layer.trainable = False
        base_output = GlobalAveragePooling2D()(base_model.output)
        fused = Dropout(0.8)(base_output)
        fused_with_seq = ops.expand_dims(fused, axis=1)
        attention = MultiHeadAttention(num_heads=4, key_dim=128)(fused_with_seq, fused_with_seq)
        attention = ops.squeeze(attention, axis=1)
        x = BatchNormalization()(attention)
        x = Dropout(0.7)(x)
        # Shared head with task-specific branches
        shared = Dense(256, activation='relu')(x)
        expr_branch = Dense(128, activation='relu')(shared)
        va_branch = Dense(128, activation='relu')(shared)
        au_branch = Dense(128, activation='relu')(shared)
        expr_out = Dense(8, activation='softmax', dtype='float32', kernel_regularizer=regularizers.l2(1e-2), name='expr')(expr_branch)
        va_out = Dense(2, activation='tanh', name='va')(va_branch)
        au_out = Dense(12, activation='sigmoid', name='au')(au_branch)
        model = Model(inputs=full_input, outputs={'expr': expr_out, 'va': va_out, 'au': au_out})
        return model

    def compile_model(model):
        # Cosine annealing scheduler
        total_steps = steps_per_epoch * num_epochs_per_model
        lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
            initial_learning_rate=5e-6,  # Increased from 1e-6
            decay_steps=total_steps
        )
        model.compile(
            optimizer=tf.keras.optimizers.AdamW(learning_rate=lr_schedule),
            loss={
                'expr': masked_focal_sparse_categorical_crossentropy(),
                'va': va_loss,
                'au': au_loss
            },
            loss_weights={
                'expr': 1.0,
                'va': 1.0,
                'au': 1.0
            },
            metrics={
                'expr': [tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),
                         MulticlassF1Score(name='f1'),
                         MulticlassPrecision(name='precision'),
                         MulticlassRecall(name='recall')],
                'va': [VA_CCC()],
                'au': [AU_F1()]
            }
        )
        return model

# Create and load model
model = create_model()
if latest_checkpoint:
    try:
        print(f"Loading initial checkpoint: {latest_checkpoint}")
        model.load_weights(latest_checkpoint)
    except Exception as e:
        print(f"Checkpoint load failed: {e}. Training from scratch.")
        latest_epoch = 0


# Callbacks
early_stop = EarlyStopping(monitor='val_expr_f1', patience=50, restore_best_weights=True, mode='max')
checkpoint = ModelCheckpoint(os.path.join(checkpoint_dir, f'new_mtl_model_cache{cache_id}_epoch_{{epoch:02d}}.weights.h5'), save_weights_only=True, save_freq='epoch')
tensorboard = TensorBoard(log_dir=f'./logs/cache{cache_id}')
reduce_lr = ReduceLROnPlateau(monitor='val_expr_f1', factor=0.5, patience=10, min_lr=1e-7, verbose=1)

model = compile_model(model)

# Fit
model.fit(
    train_dataset,
    epochs=num_epochs_per_model + latest_epoch,
    initial_epoch=latest_epoch,
    steps_per_epoch=steps_per_epoch,
    validation_data=val_dataset,
    validation_steps=validation_steps,
    callbacks=[early_stop, checkpoint, tensorboard, PerClassMetricsCallback()]
)

# Ensemble 2 second training

# Checkpoint dir and latest
checkpoint_dir = '/content/drive/My Drive/EXPR_Recognition Models/'
checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'new_mtl_model_cache2_epoch_*.weights.h5'))
latest_checkpoint = max(checkpoint_files, key=os.path.getctime) if checkpoint_files else None
latest_epoch = 20  # Reset to 0 due to potential checkpoint incompatibility

# Set initial unfreeze to 250
initial_unfreeze = 250

# Class weights for EXPR
valid_expr_labels = expr_train[expr_train >= 0]
unique_classes = np.unique(valid_expr_labels)
class_weights = compute_class_weight('balanced', classes=unique_classes, y=valid_expr_labels)
class_weights_dict = dict(zip(unique_classes, class_weights))
boost_factors = {
                 1: 6.0,  # Anger (very low F1—up from 5.0 for more emphasis)
                 2: 5.0,  # Disgust (low, keep aggressive)
                 3: 4.0,  # Fear (low, stable)
                 4: 1.0,  # Happy (high, no boost)
                 5: 2.0,  # Sad (medium, fine)
                 6: 3.0,  # Surprise (low F1—up from 2.0 to match anger/disgust level)
                 7: 1.5}  # Other (medium-low, unchanged)
for cls, factor in boost_factors.items():
    if cls in class_weights_dict:
        class_weights_dict[cls] *= factor
class_weights_list = [class_weights_dict.get(i, 1.0) for i in range(8)]
class_weights_tensor = tf.constant(class_weights_list, dtype=tf.float32)
print("EXPR Class weights:", class_weights_dict)

# Train dataset
train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, train_label_dict))
train_dataset = train_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.shuffle(buffer_size=shuffle_buffer).batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)
steps_per_epoch = len(train_image_paths) // batch_size

# Model creation
with strategy.scope():
    def create_model():
        full_input = Input(shape=(224, 224, 3), name='full')
        base_model = EfficientNetV2S(
            weights='imagenet',
            include_top=False,
            input_tensor=full_input,
            input_shape=(224, 224, 3),
            name='efficientnetv2-s'
        )
        base_model.trainable = True
        for layer in base_model.layers[:-initial_unfreeze]:
            layer.trainable = False
        base_output = GlobalAveragePooling2D()(base_model.output)
        fused = Dropout(0.8)(base_output)
        fused_with_seq = ops.expand_dims(fused, axis=1)
        attention = MultiHeadAttention(num_heads=4, key_dim=128)(fused_with_seq, fused_with_seq)
        attention = ops.squeeze(attention, axis=1)
        x = BatchNormalization()(attention)
        x = Dropout(0.7)(x)
        # Shared head with task-specific branches
        shared = Dense(256, activation='relu')(x)
        expr_branch = Dense(128, activation='relu')(shared)
        va_branch = Dense(128, activation='relu')(shared)
        au_branch = Dense(128, activation='relu')(shared)
        expr_out = Dense(8, activation='softmax', dtype='float32', kernel_regularizer=regularizers.l2(1e-2), name='expr')(expr_branch)
        va_out = Dense(2, activation='tanh', name='va')(va_branch)
        au_out = Dense(12, activation='sigmoid', name='au')(au_branch)
        model = Model(inputs=full_input, outputs={'expr': expr_out, 'va': va_out, 'au': au_out})
        return model

    def compile_model(model):
        # Cosine annealing scheduler
        total_steps = steps_per_epoch * num_epochs_per_model
        lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
            initial_learning_rate=5e-6,  # Increased from 1e-6
            decay_steps=total_steps
        )
        model.compile(
            optimizer=tf.keras.optimizers.AdamW(learning_rate=lr_schedule),
            loss={
                'expr': masked_focal_sparse_categorical_crossentropy(),
                'va': va_loss,
                'au': au_loss
            },
            loss_weights={
                'expr': 1.0,
                'va': 1.0,
                'au': 1.0
            },
            metrics={
                'expr': [tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),
                         MulticlassF1Score(name='f1'),
                         MulticlassPrecision(name='precision'),
                         MulticlassRecall(name='recall')],
                'va': [VA_CCC()],
                'au': [AU_F1()]
            }
        )
        return model

# Create and load model
model = create_model()
if latest_checkpoint:
    try:
        print(f"Loading initial checkpoint: {latest_checkpoint}")
        model.load_weights(latest_checkpoint)
    except Exception as e:
        print(f"Checkpoint load failed: {e}. Training from scratch.")
        latest_epoch = 0


# Callbacks
early_stop = EarlyStopping(monitor='val_expr_f1', patience=20, restore_best_weights=True, mode='max')
checkpoint = ModelCheckpoint(os.path.join(checkpoint_dir, f'new_mtl_model_cache{cache_id}_epoch_{{epoch:02d}}.weights.h5'), save_weights_only=True, save_freq='epoch')
tensorboard = TensorBoard(log_dir=f'./logs/cache{cache_id}')
reduce_lr = ReduceLROnPlateau(monitor='val_expr_f1', factor=0.5, patience=10, min_lr=1e-7, verbose=1)

model = compile_model(model)

# Fit
model.fit(
    train_dataset,
    epochs=num_epochs_per_model + latest_epoch,
    initial_epoch=latest_epoch,
    steps_per_epoch=steps_per_epoch,
    validation_data=val_dataset,
    validation_steps=validation_steps,
    callbacks=[early_stop, checkpoint, tensorboard, PerClassMetricsCallback()]
)

# Fine-tuned single model on combined cache (1-3)
from google.cloud import storage
from google.colab import auth, drive
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import (GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Input, MultiHeadAttention, RandomRotation, RandomCrop, RandomZoom, GaussianNoise)
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau
import random
import pickle
from sklearn.utils.class_weight import compute_class_weight
import glob
from tensorflow.keras import regularizers
from tqdm import tqdm
from tensorflow.keras.applications import EfficientNetV2S
from tensorflow.keras.applications.efficientnet_v2 import preprocess_input
from tensorflow.keras import ops
from collections import Counter
import logging
import os
from imblearn.over_sampling import RandomOverSampler  # For mild oversampling
logging.basicConfig(level=logging.DEBUG)

auth.authenticate_user()
drive.mount('/content/drive')

# Load combined cache (1-3) for train
train_cache_path = '/content/drive/My Drive/combined_cache.pkl'
print(f"\nLoading combined cache from {train_cache_path}")
with open(train_cache_path, 'rb') as f:
    combined_data = pickle.load(f)
train_pairs = combined_data[0]  # Use train part of combined
print(f"Total samples in combined train: {len(train_pairs)}")

# Inspect contents
expr_labels = [pair[1]['expr'] for pair in train_pairs if 'expr' in pair[1]]
print("EXPR class distribution:", Counter(expr_labels))

# Load validation set (cache5)
val_cache_path = '/content/drive/My Drive/validation_cache_subset_balanced_10k.pkl'
with open(val_cache_path, 'rb') as f:
    val_pairs = pickle.load(f)
print(f"Loaded {len(val_pairs)} validation pairs.")

# Mild class balancer
def mild_class_balancer(pairs):
    expr_labels = [pair[1]['expr'] for pair in pairs if 'expr' in pair[1]]
    valid_pairs = [pair for pair in pairs if pair[1].get('expr', -1) >= 0]
    minority_labels = [2, 1, 3, 6]  # Disgust, Anger, Fear, Surprise
    target_counts = {2: 2500, 1: 3000, 3: 3000, 6: 3000}

    major_pairs = [p for p in valid_pairs if p[1]['expr'] not in minority_labels]
    oversampled_min = []

    for lbl, tgt in target_counts.items():
        lbl_pairs = [p for p in valid_pairs if p[1]['expr'] == lbl]
        current = len(lbl_pairs)
        # Take up to tgt originals first
        num_orig = min(current, tgt)
        oversampled_min.extend(random.sample(lbl_pairs, num_orig))
        # Add extras if needed
        if current < tgt:
            extras_needed = tgt - num_orig
            indices = np.random.choice(len(lbl_pairs), extras_needed, replace=True)
            oversampled_min.extend([lbl_pairs[i] for i in indices])

    balanced_valid = major_pairs + oversampled_min
    # Add back invalid (if any)
    invalid_pairs = [p for p in pairs if p[1].get('expr', -1) < 0]
    balanced_pairs = balanced_valid + invalid_pairs
    random.shuffle(balanced_pairs)

    print(f"Balanced counts: {Counter(p[1]['expr'] for p in balanced_pairs if 'expr' in p[1])}")
    return balanced_pairs

# Apply mild balancer to train
train_pairs = mild_class_balancer(train_pairs)
val_pairs = val_pairs

# Extract image paths and labels
train_image_paths = [pair[0] for pair in train_pairs]
train_labels = [pair[1] for pair in train_pairs]
val_image_paths = [pair[0] for pair in val_pairs]
val_labels = [pair[1] for pair in val_pairs]
print(f"Train samples after balancing: {len(train_labels)}, val samples: {len(val_labels)}")

# Convert labels to arrays
expr_train = np.array([lbl['expr'] for lbl in train_labels], dtype=np.int32)
va_train = np.array([lbl['va'] for lbl in train_labels], dtype=np.float32)
au_train = np.array([lbl['au'] for lbl in train_labels], dtype=np.int32)
train_label_dict = {'expr': expr_train, 'va': va_train, 'au': au_train}

expr_val = np.array([lbl['expr'] for lbl in val_labels], dtype=np.int32)
va_val = np.array([lbl['va'] for lbl in val_labels], dtype=np.float32)
au_val = np.array([lbl['au'] for lbl in val_labels], dtype=np.int32)
val_label_dict = {'expr': expr_val, 'va': va_val, 'au': au_val}

# Check valid labels in validation set
valid_expr_val = expr_val[expr_val >= 0]
print(f"Valid expr labels in val_dataset: {len(valid_expr_val)} out of {len(expr_val)}")

# Custom losses and metrics (updated focal gamma to 2.0)
@tf.keras.utils.register_keras_serializable()
def masked_focal_sparse_categorical_crossentropy(gamma=2.0, label_smoothing=0.05):  # Softer gamma=2
    def loss_fn(y_true_expr, y_pred_expr):
        y_true_expr = tf.cast(y_true_expr, tf.int32)
        mask = tf.not_equal(y_true_expr, -1)
        mask_f = tf.cast(mask, tf.float32)
        y_true_safe = tf.where(mask, y_true_expr, 0)
        alpha = tf.gather(class_weights_tensor, y_true_safe)
        y_true_one_hot = tf.one_hot(y_true_safe, depth=8)
        y_true_one_hot = (1 - label_smoothing) * y_true_one_hot + (label_smoothing / 8)
        ce = tf.keras.losses.categorical_crossentropy(y_true_one_hot, y_pred_expr, from_logits=False)
        pt = tf.exp(-ce)
        focal_loss = alpha * ((1 - pt) ** gamma) * ce
        focal_loss = tf.where(mask, focal_loss, tf.zeros_like(focal_loss))
        return tf.reduce_sum(focal_loss) / (tf.reduce_sum(mask_f) + tf.keras.backend.epsilon())
    return loss_fn

def ccc(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    mu_true = tf.reduce_mean(y_true)
    mu_pred = tf.reduce_mean(y_pred)
    var_true = tf.math.reduce_variance(y_true)
    var_pred = tf.math.reduce_variance(y_pred)
    cov = tf.reduce_mean((y_true - mu_true) * (y_pred - mu_pred))
    return (2 * cov) / (var_true + var_pred + (mu_true - mu_pred)**2 + 1e-6)

def va_loss(y_true_va, y_pred_va):
    mask_v = tf.greater(y_true_va[:,0], -1.5)
    mask_a = tf.greater(y_true_va[:,1], -1.5)
    v_true = tf.boolean_mask(y_true_va[:,0], mask_v)
    v_pred = tf.boolean_mask(y_pred_va[:,0], mask_v)
    a_true = tf.boolean_mask(y_true_va[:,1], mask_a)
    a_pred = tf.boolean_mask(y_pred_va[:,1], mask_a)
    ccc_v = ccc(v_true, v_pred)
    ccc_a = ccc(a_true, a_pred)
    return (1 - ccc_v + 1 - ccc_a) / 2

def au_loss(y_true_au, y_pred_au):
    mask = tf.not_equal(y_true_au, -1)
    y_true_masked = tf.where(mask, tf.cast(y_true_au, tf.float32), 0.0)
    return tf.keras.losses.binary_crossentropy(y_true_masked, y_pred_au)

@tf.keras.utils.register_keras_serializable()
class MulticlassF1Score(tf.keras.metrics.Metric):
    def __init__(self, num_classes=8, name='f1', **kwargs):
        super(MulticlassF1Score, self).__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.per_class_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fp = [self.add_weight(name=f'fp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fn = [self.add_weight(name=f'fn_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.int32)
        mask = tf.not_equal(y_true, -1)
        y_true = tf.boolean_mask(y_true, mask)
        y_pred = tf.boolean_mask(y_pred, mask)
        y_pred_classes = tf.argmax(y_pred, axis=-1, output_type=tf.int32)
        for i in range(self.num_classes):
            class_mask_true = tf.equal(y_true, i)
            class_mask_pred = tf.equal(y_pred_classes, i)
            tp = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, class_mask_pred), tf.float32))
            fp = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not(class_mask_true), class_mask_pred), tf.float32))
            fn = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, tf.logical_not(class_mask_pred)), tf.float32))
            self.per_class_tp[i].assign_add(tp)
            self.per_class_fp[i].assign_add(fp)
            self.per_class_fn[i].assign_add(fn)
    def result(self):
        prec = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fp[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        rec = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fn[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        f1_per_class = [2 * (prec[i] * rec[i]) / (prec[i] + rec[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        return tf.reduce_mean(tf.stack(f1_per_class))
    def reset_state(self):
        for i in range(self.num_classes):
            self.per_class_tp[i].assign(0.0)
            self.per_class_fp[i].assign(0.0)
            self.per_class_fn[i].assign(0.0)

@tf.keras.utils.register_keras_serializable()
class MulticlassPrecision(tf.keras.metrics.Metric):
    def __init__(self, num_classes=8, name='precision', **kwargs):  # Fixed: num_classes=8
        super(MulticlassPrecision, self).__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.per_class_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fp = [self.add_weight(name=f'fp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.int32)
        mask = tf.not_equal(y_true, -1)
        y_true_masked = tf.boolean_mask(y_true, mask)
        y_pred_masked = tf.boolean_mask(y_pred, mask)
        y_pred_classes = tf.argmax(y_pred_masked, axis=-1, output_type=tf.int32)
        for i in range(self.num_classes):
            class_mask_true = tf.equal(y_true_masked, i)
            class_mask_pred = tf.equal(y_pred_classes, i)
            tp = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, class_mask_pred), tf.float32))
            fp = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not(class_mask_true), class_mask_pred), tf.float32))
            self.per_class_tp[i].assign_add(tp)
            self.per_class_fp[i].assign_add(fp)
    def result(self):
        precision_per_class = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fp[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        return tf.reduce_mean(tf.stack(precision_per_class))
    def reset_state(self):
        for i in range(self.num_classes):
            self.per_class_tp[i].assign(0.0)
            self.per_class_fp[i].assign(0.0)

@tf.keras.utils.register_keras_serializable()
class MulticlassRecall(tf.keras.metrics.Metric):
    def __init__(self, num_classes=8, name='recall', **kwargs):
        super(MulticlassRecall, self).__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.per_class_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fn = [self.add_weight(name=f'fn_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.int32)
        mask = tf.not_equal(y_true, -1)
        y_true_masked = tf.boolean_mask(y_true, mask)
        y_pred_masked = tf.boolean_mask(y_pred, mask)
        y_pred_classes = tf.argmax(y_pred_masked, axis=-1, output_type=tf.int32)
        for i in range(self.num_classes):
            class_mask_true = tf.equal(y_true_masked, i)
            class_mask_pred = tf.equal(y_pred_classes, i)
            tp = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, class_mask_pred), tf.float32))
            fn = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, tf.logical_not(class_mask_pred)), tf.float32))
            self.per_class_tp[i].assign_add(tp)
            self.per_class_fn[i].assign_add(fn)
    def result(self):
        recall_per_class = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fn[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        return tf.reduce_mean(tf.stack(recall_per_class))
    def reset_state(self):
        for i in range(self.num_classes):
            self.per_class_tp[i].assign(0.0)
            self.per_class_fn[i].assign(0.0)

@tf.keras.utils.register_keras_serializable()
class VA_CCC(tf.keras.metrics.Metric):
    def __init__(self, name='va_ccc', **kwargs):
        super().__init__(name=name, **kwargs)
        self.ccc_v_sum = self.add_weight(shape=(), name='ccc_v_sum', initializer='zeros')
        self.ccc_a_sum = self.add_weight(shape=(), name='ccc_a_sum', initializer='zeros')
        self.count = self.add_weight(shape=(), name='count', initializer='zeros')
    def update_state(self, y_true_va, y_pred_va, sample_weight=None):
        mask_v = tf.greater(y_true_va[:,0], -1.5)
        mask_a = tf.greater(y_true_va[:,1], -1.5)
        v_true = tf.boolean_mask(y_true_va[:,0], mask_v)
        v_pred = tf.boolean_mask(y_pred_va[:,0], mask_v)
        a_true = tf.boolean_mask(y_true_va[:,1], mask_a)
        a_pred = tf.boolean_mask(y_pred_va[:,1], mask_a)
        self.ccc_v_sum.assign_add(ccc(v_true, v_pred))
        self.ccc_a_sum.assign_add(ccc(a_true, a_pred))
        self.count.assign_add(1.0)
    def result(self):
        return (self.ccc_v_sum + self.ccc_a_sum) / (2 * self.count + tf.keras.backend.epsilon())
    def reset_state(self):
        self.ccc_v_sum.assign(0.0)
        self.ccc_a_sum.assign(0.0)
        self.count.assign(0.0)

@tf.keras.utils.register_keras_serializable()
class AU_F1(tf.keras.metrics.Metric):
    def __init__(self, name='au_f1', **kwargs):
        super().__init__(name=name, **kwargs)
        self.per_au_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(12)]
        self.per_au_fp = [self.add_weight(name=f'fp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(12)]
        self.per_au_fn = [self.add_weight(name=f'fn_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(12)]
    def update_state(self, y_true_au, y_pred_au, sample_weight=None):
        mask = tf.not_equal(y_true_au, -1)
        y_true_masked = tf.where(mask, tf.cast(y_true_au, tf.float32), tf.zeros_like(y_true_au, dtype=tf.float32))
        y_pred_bin = tf.round(y_pred_au)
        for i in range(12):
            tp = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(y_true_masked[:,i], tf.int32) == 1, tf.cast(y_pred_bin[:,i], tf.int32) == 1), tf.float32))
            fp = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(y_true_masked[:,i], tf.int32) == 0, tf.cast(y_pred_bin[:,i], tf.int32) == 1), tf.float32))
            fn = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(y_true_masked[:,i], tf.int32) == 1, tf.cast(y_pred_bin[:,i], tf.int32) == 0), tf.float32))
            self.per_au_tp[i].assign_add(tp)
            self.per_au_fp[i].assign_add(fp)
            self.per_au_fn[i].assign_add(fn)
    def result(self):
        prec = [self.per_au_tp[i] / (self.per_au_tp[i] + self.per_au_fp[i] + tf.keras.backend.epsilon()) for i in range(12)]
        rec = [self.per_au_tp[i] / (self.per_au_tp[i] + self.per_au_fn[i] + tf.keras.backend.epsilon()) for i in range(12)]
        f1_per_au = [2 * (prec[i] * rec[i]) / (prec[i] + rec[i] + tf.keras.backend.epsilon()) for i in range(12)]
        return tf.reduce_mean(tf.stack(f1_per_au))
    def reset_state(self):
        for i in range(12):
            self.per_au_tp[i].assign(0.0)
            self.per_au_fp[i].assign(0.0)
            self.per_au_fn[i].assign(0.0)

class PerClassMetricsCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        y_true_all = []
        y_pred_all = []
        for batch in val_dataset:
            images, labels = batch
            preds = self.model.predict(images, verbose=0)
            y_true_all.extend(labels['expr'].numpy())
            y_pred_all.extend(np.argmax(preds['expr'], axis=-1))
        y_true = np.array(y_true_all)
        y_pred = np.array(y_pred_all)
        mask = y_true != -1
        y_true = y_true[mask]
        y_pred = y_pred[mask]
        for cls in range(8):
            tp = np.sum((y_true == cls) & (y_pred == cls))
            fp = np.sum((y_true != cls) & (y_pred == cls))
            fn = np.sum((y_true == cls) & (y_pred != cls))
            prec = tp / (tp + fp + 1e-6)
            rec = tp / (tp + fn + 1e-6)
            f1 = 2 * prec * rec / (prec + rec + 1e-6)
            print(f"Class {cls}: F1 = {f1:.4f}, Prec = {prec:.4f}, Rec = {rec:.4f}")

# Enhanced data augmentation layers (added GaussianNoise for subtlety)
rotation_layer = RandomRotation(factor=0.3)
crop_layer = RandomCrop(height=224, width=224)
zoom_layer = RandomZoom(0.1)
noise_layer = GaussianNoise(0.01)  # Light noise for robustness

def load_and_preprocess_image(path, label):
    try:
        img = tf.io.read_file(path)
        img = tf.image.decode_jpeg(img, channels=3)
        img = tf.image.resize(img, [256, 256])
        img = tf.cast(img, tf.float32) / 255.0
        img = crop_layer(img)
        img = zoom_layer(img)
        img = tf.image.random_flip_left_right(img)
        img = tf.image.random_brightness(img, max_delta=0.15)
        img = tf.cast(img, tf.float32)
        img = tf.image.random_contrast(img, lower=0.9, upper=1.1)
        img = tf.image.random_hue(img, 0.07)
        img = tf.image.random_saturation(img, 0.8, 1.6)
        img = tf.image.random_flip_up_down(img)
        img = rotation_layer(img)
        img = noise_layer(img)  # Added for majority subtlety
        full_img = preprocess_input(img * 255.0)
        return full_img, label
    except Exception as e:
        tf.print("Error loading", path, ":", e)
        dummy_img = tf.zeros((224, 224, 3), dtype=tf.float32)
        return dummy_img, {'expr': tf.constant(-1, dtype=tf.int32),
                           'va': tf.constant([-2.0, -2.0], dtype=tf.float32),
                           'au': tf.constant([-1]*12, dtype=tf.int32)}

# Strategy setup
strategy = tf.distribute.get_strategy()
batch_size = 128
from tensorflow.keras.mixed_precision import set_global_policy
set_global_policy('mixed_float16')
shuffle_buffer = 1024

num_epochs = 15

# Val dataset
val_dataset = tf.data.Dataset.from_tensor_slices((val_image_paths, val_label_dict))
val_dataset = val_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
validation_steps = len(val_image_paths) // batch_size

# Checkpoint dir and latest (update for fine-tune)
checkpoint_dir = '/content/drive/My Drive/EXPR_Recognition Models/'
checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'new_mtl_model_cache2_epoch_*.weights.h5'))
latest_checkpoint = max(checkpoint_files, key=os.path.getctime) if checkpoint_files else None
latest_epoch = 60  # Start fresh or from best prior

# Dampened class weights: 1 / sqrt(freq), capped
total_samples = len(expr_train)
class_freq = np.bincount(expr_train[expr_train >= 0], minlength=8)
class_weights = np.array([1 / np.sqrt(freq / total_samples + 1e-6) if freq > 0 else 1.0 for freq in class_freq])
class_weights = np.clip(class_weights, 1.0, 3.0)  # Cap boosts at 3x
class_weights_tensor = tf.constant(class_weights, dtype=tf.float32)
print("Dampened EXPR Class weights:", dict(enumerate(class_weights)))

# Train dataset
train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, train_label_dict))
train_dataset = train_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.shuffle(buffer_size=shuffle_buffer).batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)
steps_per_epoch = len(train_image_paths) // batch_size

# Model creation
with strategy.scope():
    def create_model():
        full_input = Input(shape=(224, 224, 3), name='full')
        base_model = EfficientNetV2S(
            weights='imagenet',
            include_top=False,
            input_tensor=full_input,
            input_shape=(224, 224, 3),
            name='efficientnetv2-s'
        )
        base_model.trainable = True
        for layer in base_model.layers[:-250]:  # Initial unfreeze 250
            layer.trainable = False
        base_output = GlobalAveragePooling2D()(base_model.output)
        fused = Dropout(0.8)(base_output)
        fused_with_seq = ops.expand_dims(fused, axis=1)
        attention = MultiHeadAttention(num_heads=4, key_dim=128)(fused_with_seq, fused_with_seq)
        attention = ops.squeeze(attention, axis=1)
        x = BatchNormalization()(attention)
        x = Dropout(0.7)(x)
        # Shared head with task-specific branches
        shared = Dense(256, activation='relu')(x)
        expr_branch = Dense(128, activation='relu')(shared)
        va_branch = Dense(128, activation='relu')(shared)
        au_branch = Dense(128, activation='relu')(shared)
        expr_out = Dense(8, activation='softmax', dtype='float32', kernel_regularizer=regularizers.l2(1e-2), name='expr')(expr_branch)
        va_out = Dense(2, activation='tanh', name='va')(va_branch)
        au_out = Dense(12, activation='sigmoid', name='au')(au_branch)
        model = Model(inputs=full_input, outputs={'expr': expr_out, 'va': va_out, 'au': au_out})
        return model

    def compile_model(model):
        # Cosine annealing scheduler
        total_steps = steps_per_epoch * num_epochs
        lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
            initial_learning_rate=1e-5,  # Set to 1e-5
            decay_steps=total_steps
        )
        model.compile(
            optimizer=tf.keras.optimizers.AdamW(learning_rate=lr_schedule, weight_decay=0.01),  # Added wd=0.01
            loss={
                'expr': masked_focal_sparse_categorical_crossentropy(),
                'va': va_loss,
                'au': au_loss
            },
            loss_weights={
                'expr': 0.5,  # Emphasize expr for mains
                'va': 0.25,
                'au': 0.25
            },
            metrics={
                'expr': [tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),
                         MulticlassF1Score(name='f1'),
                         MulticlassPrecision(name='precision'),
                         MulticlassRecall(name='recall')],
                'va': [VA_CCC()],
                'au': [AU_F1()]
            }
        )
        return model

# Create and load model
model = create_model()
if latest_checkpoint:
    try:
        print(f"Loading initial checkpoint: {latest_checkpoint}")
        model.load_weights(latest_checkpoint)
    except Exception as e:
        print(f"Checkpoint load failed: {e}. Training from scratch.")
        latest_epoch = 0

model = compile_model(model)

# Callbacks (monitor macro-F1 equiv via expr_f1)
early_stop = EarlyStopping(monitor='val_expr_f1', patience=15, restore_best_weights=True, mode='max')
checkpoint = ModelCheckpoint(os.path.join(checkpoint_dir, f'finetuned_mtl_model_epoch_{{epoch:02d}}.weights.h5'), save_weights_only=True, save_freq='epoch')
tensorboard = TensorBoard(log_dir='./logs/combined_finetune')
reduce_lr = ReduceLROnPlateau(monitor='val_expr_f1', factor=0.5, patience=10, min_lr=1e-7, verbose=1)

# Fit (epochs=30, initial_epoch=0)
model.fit(
    train_dataset,
    epochs=num_epochs + latest_epoch,
    initial_epoch=latest_epoch,
    steps_per_epoch=steps_per_epoch,
    validation_data=val_dataset,
    validation_steps=validation_steps,
    callbacks=[early_stop, checkpoint, tensorboard, PerClassMetricsCallback(), reduce_lr]
)

# Fine-tuned single model on combined cache (1-3) part 2

# Checkpoint dir and latest
checkpoint_dir = '/content/drive/My Drive/EXPR_Recognition Models/'
checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'finetuned_mtl_model_epoch*.weights.h5'))
latest_checkpoint = max(checkpoint_files, key=os.path.getctime) if checkpoint_files else None
latest_epoch = 60  # Start fresh or from best prior

# Dampened class weights: 1 / sqrt(freq), capped
total_samples = len(expr_train)
class_freq = np.bincount(expr_train[expr_train >= 0], minlength=8)
class_weights = np.array([1 / np.sqrt(freq / total_samples + 1e-6) if freq > 0 else 1.0 for freq in class_freq])
class_weights = np.clip(class_weights, 1.0, 3.0)  # Cap boosts at 3x
class_weights_tensor = tf.constant(class_weights, dtype=tf.float32)
print("Dampened EXPR Class weights:", dict(enumerate(class_weights)))

# Train dataset
train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, train_label_dict))
train_dataset = train_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.shuffle(buffer_size=shuffle_buffer).batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)
steps_per_epoch = len(train_image_paths) // batch_size

# Model creation
with strategy.scope():
    def create_model():
        full_input = Input(shape=(224, 224, 3), name='full')
        base_model = EfficientNetV2S(
            weights='imagenet',
            include_top=False,
            input_tensor=full_input,
            input_shape=(224, 224, 3),
            name='efficientnetv2-s'
        )
        base_model.trainable = True
        for layer in base_model.layers[:-250]:  # Initial unfreeze 250
            layer.trainable = False
        base_output = GlobalAveragePooling2D()(base_model.output)
        fused = Dropout(0.8)(base_output)
        fused_with_seq = ops.expand_dims(fused, axis=1)
        attention = MultiHeadAttention(num_heads=4, key_dim=128)(fused_with_seq, fused_with_seq)
        attention = ops.squeeze(attention, axis=1)
        x = BatchNormalization()(attention)
        x = Dropout(0.7)(x)
        # Shared head with task-specific branches
        shared = Dense(256, activation='relu')(x)
        expr_branch = Dense(128, activation='relu')(shared)
        va_branch = Dense(128, activation='relu')(shared)
        au_branch = Dense(128, activation='relu')(shared)
        expr_out = Dense(8, activation='softmax', dtype='float32', kernel_regularizer=regularizers.l2(1e-2), name='expr')(expr_branch)
        va_out = Dense(2, activation='tanh', name='va')(va_branch)
        au_out = Dense(12, activation='sigmoid', name='au')(au_branch)
        model = Model(inputs=full_input, outputs={'expr': expr_out, 'va': va_out, 'au': au_out})
        return model

    def compile_model(model):
    # Initial float LR; ReduceLROnPlateau will adjust it
    model.compile(
        optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-5, weight_decay=0.01),  # Float LR here
        loss={
            'expr': masked_focal_sparse_categorical_crossentropy(),
            'va': va_loss,
            'au': au_loss
        },
        loss_weights={
            'expr': 0.5,
            'va': 0.25,
            'au': 0.25
        },
        metrics={
            'expr': [tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),
                     MulticlassF1Score(name='f1'),
                     MulticlassPrecision(name='precision'),
                     MulticlassRecall(name='recall')],
            'va': [VA_CCC()],
            'au': [AU_F1()]
        }
    )
    return model
# Create and load model
model = create_model()
if latest_checkpoint:
    try:
        print(f"Loading initial checkpoint: {latest_checkpoint}")
        model.load_weights(latest_checkpoint)
    except Exception as e:
        print(f"Checkpoint load failed: {e}. Training from scratch.")
        latest_epoch = 0

model = compile_model(model)

# Callbacks (monitor macro-F1 equiv via expr_f1)
early_stop = EarlyStopping(monitor='val_expr_f1', patience=15, restore_best_weights=True, mode='max')
checkpoint = ModelCheckpoint(os.path.join(checkpoint_dir, f'finetuned_mtl_model_epoch_{{epoch:02d}}.weights.h5'), save_weights_only=True, save_freq='epoch')
tensorboard = TensorBoard(log_dir='./logs/combined_finetune')
reduce_lr = ReduceLROnPlateau(monitor='val_expr_f1', factor=0.5, patience=10, min_lr=1e-7, verbose=1)  # This now works
# Fit (epochs=30, initial_epoch=0)
model.fit(
    train_dataset,
    epochs=num_epochs + latest_epoch,
    initial_epoch=latest_epoch,
    steps_per_epoch=steps_per_epoch,
    validation_data=val_dataset,
    validation_steps=validation_steps,
    callbacks=[early_stop, checkpoint, tensorboard, PerClassMetricsCallback(), reduce_lr]
)

# Fine-tuned single model on combined cache (1-3) part 3

# Checkpoint dir and latest
checkpoint_dir = '/content/drive/My Drive/EXPR_Recognition Models/'
checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'finetuned_mtl_model_epoch*.weights.h5'))
latest_checkpoint = max(checkpoint_files, key=os.path.getctime) if checkpoint_files else None
latest_epoch = 71  # Start fresh or from best prior

# Dampened class weights: 1 / sqrt(freq), capped
total_samples = len(expr_train)
class_freq = np.bincount(expr_train[expr_train >= 0], minlength=8)
class_weights = np.array([1 / np.sqrt(freq / total_samples + 1e-6) if freq > 0 else 1.0 for freq in class_freq])
class_weights = np.clip(class_weights, 1.0, 3.0)  # Cap boosts at 3x
class_weights_tensor = tf.constant(class_weights, dtype=tf.float32)
print("Dampened EXPR Class weights:", dict(enumerate(class_weights)))

# Train dataset
train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, train_label_dict))
train_dataset = train_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
train_dataset = train_dataset.shuffle(buffer_size=shuffle_buffer).batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)
steps_per_epoch = len(train_image_paths) // batch_size

# Model creation
with strategy.scope():
    def create_model():
        full_input = Input(shape=(224, 224, 3), name='full')
        base_model = EfficientNetV2S(
            weights='imagenet',
            include_top=False,
            input_tensor=full_input,
            input_shape=(224, 224, 3),
            name='efficientnetv2-s'
        )
        base_model.trainable = True
        for layer in base_model.layers[:-250]:  # Initial unfreeze 250
            layer.trainable = False
        base_output = GlobalAveragePooling2D()(base_model.output)
        fused = Dropout(0.8)(base_output)
        fused_with_seq = ops.expand_dims(fused, axis=1)
        attention = MultiHeadAttention(num_heads=4, key_dim=128)(fused_with_seq, fused_with_seq)
        attention = ops.squeeze(attention, axis=1)
        x = BatchNormalization()(attention)
        x = Dropout(0.7)(x)
        # Shared head with task-specific branches
        shared = Dense(256, activation='relu')(x)
        expr_branch = Dense(128, activation='relu')(shared)
        va_branch = Dense(128, activation='relu')(shared)
        au_branch = Dense(128, activation='relu')(shared)
        expr_out = Dense(8, activation='softmax', dtype='float32', kernel_regularizer=regularizers.l2(1e-2), name='expr')(expr_branch)
        va_out = Dense(2, activation='tanh', name='va')(va_branch)
        au_out = Dense(12, activation='sigmoid', name='au')(au_branch)
        model = Model(inputs=full_input, outputs={'expr': expr_out, 'va': va_out, 'au': au_out})
        return model

def compile_model(model):
    # Initial float LR; ReduceLROnPlateau will adjust it
    model.compile(
        optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-5, weight_decay=0.01),  # Float LR here
        loss={
            'expr': masked_focal_sparse_categorical_crossentropy(),
            'va': va_loss,
            'au': au_loss
        },
        loss_weights={
            'expr': 0.5,
            'va': 0.25,
            'au': 0.25
        },
        metrics={
            'expr': [tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),
                     MulticlassF1Score(name='f1'),
                     MulticlassPrecision(name='precision'),
                     MulticlassRecall(name='recall')],
            'va': [VA_CCC()],
            'au': [AU_F1()]
        }
    )
    return model
# Create and load model
model = create_model()
if latest_checkpoint:
    try:
        print(f"Loading initial checkpoint: {latest_checkpoint}")
        model.load_weights(latest_checkpoint)
    except Exception as e:
        print(f"Checkpoint load failed: {e}. Training from scratch.")
        latest_epoch = 0

model = compile_model(model)

# Callbacks (monitor macro-F1 equiv via expr_f1)
early_stop = EarlyStopping(monitor='val_expr_f1', patience=15, restore_best_weights=True, mode='max')
checkpoint = ModelCheckpoint(os.path.join(checkpoint_dir, f'finetuned_mtl_model_epoch_{{epoch:02d}}.weights.h5'), save_weights_only=True, save_freq='epoch')
tensorboard = TensorBoard(log_dir='./logs/combined_finetune')
reduce_lr = ReduceLROnPlateau(monitor='val_expr_f1', factor=0.5, patience=10, min_lr=1e-7, verbose=1)
# Fit
model.fit(
    train_dataset,
    epochs=num_epochs + latest_epoch,
    initial_epoch=latest_epoch,
    steps_per_epoch=steps_per_epoch,
    validation_data=val_dataset,
    validation_steps=validation_steps,
    callbacks=[early_stop, checkpoint, tensorboard, PerClassMetricsCallback(), reduce_lr]
)

# Save model
import os
import tensorflow as tf
from google.colab import drive
from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, MultiHeadAttention
from tensorflow.keras.models import Model
from tensorflow.keras.applications import EfficientNetV2S
from tensorflow.keras.applications.efficientnet_v2 import preprocess_input
from tensorflow.keras import ops
from tensorflow.keras import regularizers
import numpy as np

# Ensure Google Drive is mounted
drive.mount('/content/drive', force_remount=True)

# Define paths
checkpoint_dir = '/content/drive/My Drive/EXPR_Recognition Models/'
weights_path = '/content/drive/My Drive/EXPR_Recognition Models/finetuned_mtl_model_epoch_99.weights.h5'
drive_save_path = os.path.join(checkpoint_dir, 'full_mtl_model.keras')

# Recreate model architecture (must match the original)
def create_model():
    full_input = Input(shape=(224, 224, 3), name='full')
    base_model = EfficientNetV2S(
        weights='imagenet',
        include_top=False,
        input_tensor=full_input,
        input_shape=(224, 224, 3),
        name='efficientnetv2-s'
    )
    base_model.trainable = True
    for layer in base_model.layers[:-250]:
        layer.trainable = False
    base_output = GlobalAveragePooling2D()(base_model.output)
    fused = Dropout(0.8)(base_output)
    fused_with_seq = ops.expand_dims(fused, axis=1)
    attention = MultiHeadAttention(num_heads=4, key_dim=128)(fused_with_seq, fused_with_seq)
    attention = ops.squeeze(attention, axis=1)
    x = BatchNormalization()(attention)
    x = Dropout(0.7)(x)
    shared = Dense(256, activation='relu')(x)
    expr_branch = Dense(128, activation='relu')(shared)
    va_branch = Dense(128, activation='relu')(shared)
    au_branch = Dense(128, activation='relu')(shared)
    expr_out = Dense(8, activation='softmax', dtype='float32', kernel_regularizer=regularizers.l2(1e-2), name='expr')(expr_branch)
    va_out = Dense(2, activation='tanh', name='va')(va_branch)
    au_out = Dense(12, activation='sigmoid', name='au')(au_branch)
    model = Model(inputs=full_input, outputs={'expr': expr_out, 'va': va_out, 'au': au_out})
    return model

# Custom losses as serializable classes
@tf.keras.utils.register_keras_serializable()
class MaskedFocalSparseCategoricalCrossentropy(tf.keras.losses.Loss):
    def __init__(self, gamma=2.0, label_smoothing=0.05, num_classes=8, **kwargs):
        super().__init__(**kwargs)
        self.gamma = gamma
        self.label_smoothing = label_smoothing
        self.num_classes = num_classes
        self.class_weights = tf.constant(np.ones(num_classes, dtype=np.float32))

    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.int32)
        mask = tf.not_equal(y_true, -1)
        mask_f = tf.cast(mask, tf.float32)
        y_true_safe = tf.where(mask, y_true, 0)
        alpha = tf.gather(self.class_weights, y_true_safe)
        y_true_one_hot = tf.one_hot(y_true_safe, depth=self.num_classes)
        y_true_one_hot = (1 - self.label_smoothing) * y_true_one_hot + (self.label_smoothing / self.num_classes)
        ce = tf.keras.losses.categorical_crossentropy(y_true_one_hot, y_pred, from_logits=False)
        pt = tf.exp(-ce)
        focal_loss = alpha * ((1 - pt) ** self.gamma) * ce
        focal_loss = tf.where(mask, focal_loss, tf.zeros_like(focal_loss))
        return tf.reduce_sum(focal_loss) / (tf.reduce_sum(mask_f) + tf.keras.backend.epsilon())

    def get_config(self):
        config = super().get_config()
        config.update({
            'gamma': self.gamma,
            'label_smoothing': self.label_smoothing,
            'num_classes': self.num_classes
        })
        return config

@tf.keras.utils.register_keras_serializable()
class VALoss(tf.keras.losses.Loss):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def call(self, y_true, y_pred):
        mask_v = tf.greater(y_true[:, 0], -1.5)
        mask_a = tf.greater(y_true[:, 1], -1.5)
        v_true = tf.boolean_mask(y_true[:, 0], mask_v)
        v_pred = tf.boolean_mask(y_pred[:, 0], mask_v)
        a_true = tf.boolean_mask(y_true[:, 1], mask_a)
        a_pred = tf.boolean_mask(y_pred[:, 1], mask_a)
        ccc_v = self.ccc(v_true, v_pred)
        ccc_a = self.ccc(a_true, a_pred)
        return (1 - ccc_v + 1 - ccc_a) / 2

    def ccc(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        mu_true = tf.reduce_mean(y_true)
        mu_pred = tf.reduce_mean(y_pred)
        var_true = tf.math.reduce_variance(y_true)
        var_pred = tf.math.reduce_variance(y_pred)
        cov = tf.reduce_mean((y_true - mu_true) * (y_pred - mu_pred))
        return (2 * cov) / (var_true + var_pred + (mu_true - mu_pred)**2 + 1e-6)

    def get_config(self):
        return super().get_config()

@tf.keras.utils.register_keras_serializable()
class AULoss(tf.keras.losses.Loss):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def call(self, y_true, y_pred):
        mask = tf.not_equal(y_true, -1)
        y_true_masked = tf.where(mask, tf.cast(y_true, tf.float32), 0.0)
        return tf.keras.losses.binary_crossentropy(y_true_masked, y_pred)

    def get_config(self):
        return super().get_config()

# Custom metrics as serializable classes
@tf.keras.utils.register_keras_serializable()
class MulticlassF1Score(tf.keras.metrics.Metric):
    def __init__(self, num_classes=8, name='f1', **kwargs):
        super(MulticlassF1Score, self).__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.per_class_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fp = [self.add_weight(name=f'fp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fn = [self.add_weight(name=f'fn_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.int32)
        mask = tf.not_equal(y_true, -1)
        y_true = tf.boolean_mask(y_true, mask)
        y_pred = tf.boolean_mask(y_pred, mask)
        y_pred_classes = tf.argmax(y_pred, axis=-1, output_type=tf.int32)
        for i in range(self.num_classes):
            class_mask_true = tf.equal(y_true, i)
            class_mask_pred = tf.equal(y_pred_classes, i)
            tp = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, class_mask_pred), tf.float32))
            fp = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not(class_mask_true), class_mask_pred), tf.float32))
            fn = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, tf.logical_not(class_mask_pred)), tf.float32))
            self.per_class_tp[i].assign_add(tp)
            self.per_class_fp[i].assign_add(fp)
            self.per_class_fn[i].assign_add(fn)

    def result(self):
        prec = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fp[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        rec = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fn[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        f1_per_class = [2 * (prec[i] * rec[i]) / (prec[i] + rec[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        return tf.reduce_mean(tf.stack(f1_per_class))

    def reset_state(self):
        for i in range(self.num_classes):
            self.per_class_tp[i].assign(0.0)
            self.per_class_fp[i].assign(0.0)
            self.per_class_fn[i].assign(0.0)

    def get_config(self):
        config = super().get_config()
        config.update({'num_classes': self.num_classes})
        return config

@tf.keras.utils.register_keras_serializable()
class MulticlassPrecision(tf.keras.metrics.Metric):
    def __init__(self, num_classes=8, name='precision', **kwargs):
        super(MulticlassPrecision, self).__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.per_class_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fp = [self.add_weight(name=f'fp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.int32)
        mask = tf.not_equal(y_true, -1)
        y_true_masked = tf.boolean_mask(y_true, mask)
        y_pred_masked = tf.boolean_mask(y_pred, mask)
        y_pred_classes = tf.argmax(y_pred_masked, axis=-1, output_type=tf.int32)
        for i in range(self.num_classes):
            class_mask_true = tf.equal(y_true_masked, i)
            class_mask_pred = tf.equal(y_pred_classes, i)
            tp = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, class_mask_pred), tf.float32))
            fp = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not(class_mask_true), class_mask_pred), tf.float32))
            self.per_class_tp[i].assign_add(tp)
            self.per_class_fp[i].assign_add(fp)

    def result(self):
        precision_per_class = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fp[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        return tf.reduce_mean(tf.stack(precision_per_class))

    def reset_state(self):
        for i in range(self.num_classes):
            self.per_class_tp[i].assign(0.0)
            self.per_class_fp[i].assign(0.0)

    def get_config(self):
        config = super().get_config()
        config.update({'num_classes': self.num_classes})
        return config

@tf.keras.utils.register_keras_serializable()
class MulticlassRecall(tf.keras.metrics.Metric):
    def __init__(self, num_classes=8, name='recall', **kwargs):
        super(MulticlassRecall, self).__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.per_class_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fn = [self.add_weight(name=f'fn_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.int32)
        mask = tf.not_equal(y_true, -1)
        y_true_masked = tf.boolean_mask(y_true, mask)
        y_pred_masked = tf.boolean_mask(y_pred, mask)
        y_pred_classes = tf.argmax(y_pred_masked, axis=-1, output_type=tf.int32)
        for i in range(self.num_classes):
            class_mask_true = tf.equal(y_true_masked, i)
            class_mask_pred = tf.equal(y_pred_classes, i)
            tp = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, class_mask_pred), tf.float32))
            fn = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, tf.logical_not(class_mask_pred)), tf.float32))
            self.per_class_tp[i].assign_add(tp)
            self.per_class_fn[i].assign_add(fn)

    def result(self):
        recall_per_class = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fn[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        return tf.reduce_mean(tf.stack(recall_per_class))

    def reset_state(self):
        for i in range(self.num_classes):
            self.per_class_tp[i].assign(0.0)
            self.per_class_fn[i].assign(0.0)

    def get_config(self):
        config = super().get_config()
        config.update({'num_classes': self.num_classes})
        return config

@tf.keras.utils.register_keras_serializable()
class VA_CCC(tf.keras.metrics.Metric):
    def __init__(self, name='va_ccc', **kwargs):
        super().__init__(name=name, **kwargs)
        self.ccc_v_sum = self.add_weight(shape=(), name='ccc_v_sum', initializer='zeros')
        self.ccc_a_sum = self.add_weight(shape=(), name='ccc_a_sum', initializer='zeros')
        self.count = self.add_weight(shape=(), name='count', initializer='zeros')

    def update_state(self, y_true_va, y_pred_va, sample_weight=None):
        mask_v = tf.greater(y_true_va[:, 0], -1.5)
        mask_a = tf.greater(y_true_va[:, 1], -1.5)
        v_true = tf.boolean_mask(y_true_va[:, 0], mask_v)
        v_pred = tf.boolean_mask(y_pred_va[:, 0], mask_v)
        a_true = tf.boolean_mask(y_true_va[:, 1], mask_a)
        a_pred = tf.boolean_mask(y_pred_va[:, 1], mask_a)
        ccc_v = self.ccc(v_true, v_pred)
        ccc_a = self.ccc(a_true, a_pred)
        self.ccc_v_sum.assign_add(ccc_v)
        self.ccc_a_sum.assign_add(ccc_a)
        self.count.assign_add(1.0)

    def ccc(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        mu_true = tf.reduce_mean(y_true)
        mu_pred = tf.reduce_mean(y_pred)
        var_true = tf.math.reduce_variance(y_true)
        var_pred = tf.math.reduce_variance(y_pred)
        cov = tf.reduce_mean((y_true - mu_true) * (y_pred - mu_pred))
        return (2 * cov) / (var_true + var_pred + (mu_true - mu_pred)**2 + 1e-6)

    def result(self):
        return (self.ccc_v_sum + self.ccc_a_sum) / (2 * self.count + tf.keras.backend.epsilon())

    def reset_state(self):
        self.ccc_v_sum.assign(0.0)
        self.ccc_a_sum.assign(0.0)
        self.count.assign(0.0)

    def get_config(self):
        return super().get_config()

@tf.keras.utils.register_keras_serializable()
class AU_F1(tf.keras.metrics.Metric):
    def __init__(self, name='au_f1', num_au=12, **kwargs):
        super().__init__(name=name, **kwargs)
        self.num_au = num_au
        self.per_au_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_au)]
        self.per_au_fp = [self.add_weight(name=f'fp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_au)]
        self.per_au_fn = [self.add_weight(name=f'fn_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_au)]

    def update_state(self, y_true_au, y_pred_au, sample_weight=None):
        mask = tf.not_equal(y_true_au, -1)
        y_true_masked = tf.where(mask, tf.cast(y_true_au, tf.float32), tf.zeros_like(y_true_au, dtype=tf.float32))
        y_pred_bin = tf.round(y_pred_au)
        for i in range(self.num_au):
            tp = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(y_true_masked[:, i], tf.int32) == 1, tf.cast(y_pred_bin[:, i], tf.int32) == 1), tf.float32))
            fp = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(y_true_masked[:, i], tf.int32) == 0, tf.cast(y_pred_bin[:, i], tf.int32) == 1), tf.float32))
            fn = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(y_true_masked[:, i], tf.int32) == 1, tf.cast(y_pred_bin[:, i], tf.int32) == 0), tf.float32))
            self.per_au_tp[i].assign_add(tp)
            self.per_au_fp[i].assign_add(fp)
            self.per_au_fn[i].assign_add(fn)

    def result(self):
        prec = [self.per_au_tp[i] / (self.per_au_tp[i] + self.per_au_fp[i] + tf.keras.backend.epsilon()) for i in range(self.num_au)]
        rec = [self.per_au_tp[i] / (self.per_au_tp[i] + self.per_au_fn[i] + tf.keras.backend.epsilon()) for i in range(self.num_au)]
        f1_per_au = [2 * (prec[i] * rec[i]) / (prec[i] + rec[i] + tf.keras.backend.epsilon()) for i in range(self.num_au)]
        return tf.reduce_mean(tf.stack(f1_per_au))

    def reset_state(self):
        for i in range(self.num_au):
            self.per_au_tp[i].assign(0.0)
            self.per_au_fp[i].assign(0.0)
            self.per_au_fn[i].assign(0.0)

    def get_config(self):
        config = super().get_config()
        config.update({'num_au': self.num_au})
        return config

# Compile model function (updated to use serializable classes)
def compile_model(model):
    model.compile(
        optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-5, weight_decay=0.01),
        loss={
            'expr': MaskedFocalSparseCategoricalCrossentropy(gamma=2.0, label_smoothing=0.05),
            'va': VALoss(),
            'au': AULoss()
        },
        loss_weights={
            'expr': 0.5,
            'va': 0.25,
            'au': 0.25
        },
        metrics={
            'expr': [tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),
                     MulticlassF1Score(num_classes=8, name='f1'),
                     MulticlassPrecision(num_classes=8, name='precision'),
                     MulticlassRecall(num_classes=8, name='recall')],
            'va': [VA_CCC()],
            'au': [AU_F1(num_au=12)]
        }
    )
    return model

# Create and compile model
model = create_model()
model = compile_model(model)

# Load the epoch 99 weights
print(f"Loading weights from: {weights_path}")
try:
    model.load_weights(weights_path)
except Exception as e:
    print(f"Error loading weights: {e}")
    raise

# Save the full model to Google Drive
print(f"Saving full model to: {drive_save_path}")
try:
    model.save(drive_save_path, save_format='keras')
except Exception as e:
    print(f"Error saving model: {e}")
    raise

# Verify save
if os.path.exists(drive_save_path):
    print(f"Model successfully saved to {drive_save_path}")
else:
    print(f"Error: Model save failed at {drive_save_path}")

#Testing on Cache4 Data! (saving to google drive)
import os
import tensorflow as tf
from google.colab import drive
import numpy as np
import pickle
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.layers import (GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Input, MultiHeadAttention)
from tensorflow.keras.models import Model
from tensorflow.keras.applications import EfficientNetV2S
from tensorflow.keras.applications.efficientnet_v2 import preprocess_input
from tensorflow.keras import ops
from tensorflow.keras import regularizers
import pandas as pd

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)
from google.colab import auth, drive
auth.authenticate_user()
# Define paths
model_path = '/content/drive/My Drive/EXPR_Recognition Models/full_mtl_model.keras'
test_cache_path = '/content/drive/My Drive/cache4.pkl'
output_dir = '/content/drive/My Drive/EXPR_Recognition Models/test_results/'
os.makedirs(output_dir, exist_ok=True)

# Load test cache
print(f"Loading test cache from {test_cache_path}")
with open(test_cache_path, 'rb') as f:
    test_pairs = pickle.load(f)
print(f"Loaded {len(test_pairs)} test pairs.")

# Extract image paths and labels
test_image_paths = [pair[0] for pair in test_pairs]
test_labels = [pair[1] for pair in test_pairs]
expr_test = np.array([lbl['expr'] for lbl in test_labels], dtype=np.int32)
va_test = np.array([lbl['va'] for lbl in test_labels], dtype=np.float32)
au_test = np.array([lbl['au'] for lbl in test_labels], dtype=np.int32)
test_label_dict = {'expr': expr_test, 'va': va_test, 'au': au_test}

# Define preprocessing function
def load_and_preprocess_image(path, label):
    try:
        img = tf.io.read_file(path)
        img = tf.image.decode_jpeg(img, channels=3)
        img = tf.image.resize(img, [224, 224])
        img = tf.cast(img, tf.float32) / 255.0
        img = preprocess_input(img * 255.0)
        return img, label
    except Exception as e:
        tf.print("Error loading", path, ":", e)
        dummy_img = tf.zeros((224, 224, 3), dtype=tf.float32)
        return dummy_img, {'expr': tf.constant(-1, dtype=tf.int32),
                          'va': tf.constant([-2.0, -2.0], dtype=tf.float32),
                          'au': tf.constant([-1]*12, dtype=tf.int32)}

# Create test dataset
batch_size = 128
test_dataset = tf.data.Dataset.from_tensor_slices((test_image_paths, test_label_dict))
test_dataset = test_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

# Define custom objects
@tf.keras.utils.register_keras_serializable()
def masked_focal_sparse_categorical_crossentropy(gamma=2.0, label_smoothing=0.05):
    @tf.keras.utils.register_keras_serializable()
    def loss_fn(y_true_expr, y_pred_expr):
        y_true_expr = tf.cast(y_true_expr, tf.int32)
        mask = tf.not_equal(y_true_expr, -1)
        mask_f = tf.cast(mask, tf.float32)
        y_true_safe = tf.where(mask, y_true_expr, 0)
        alpha = tf.gather(tf.constant(np.ones(8, dtype=np.float32)), y_true_safe)  # Placeholder weights
        y_true_one_hot = tf.one_hot(y_true_safe, depth=8)
        y_true_one_hot = (1 - label_smoothing) * y_true_one_hot + (label_smoothing / 8)
        ce = tf.keras.losses.categorical_crossentropy(y_true_one_hot, y_pred_expr, from_logits=False)
        pt = tf.exp(-ce)
        focal_loss = alpha * ((1 - pt) ** gamma) * ce
        focal_loss = tf.where(mask, focal_loss, tf.zeros_like(focal_loss))
        return tf.reduce_sum(focal_loss) / (tf.reduce_sum(mask_f) + tf.keras.backend.epsilon())
    return loss_fn

@tf.keras.utils.register_keras_serializable()
def ccc(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.cast(y_pred, tf.float32)
    mu_true = tf.reduce_mean(y_true)
    mu_pred = tf.reduce_mean(y_pred)
    var_true = tf.math.reduce_variance(y_true)
    var_pred = tf.math.reduce_variance(y_pred)
    cov = tf.reduce_mean((y_true - mu_true) * (y_pred - mu_pred))
    return (2 * cov) / (var_true + var_pred + (mu_true - mu_pred)**2 + 1e-6)

@tf.keras.utils.register_keras_serializable()
def va_loss(y_true_va, y_pred_va):
    mask_v = tf.greater(y_true_va[:,0], -1.5)
    mask_a = tf.greater(y_true_va[:,1], -1.5)
    v_true = tf.boolean_mask(y_true_va[:,0], mask_v)
    v_pred = tf.boolean_mask(y_pred_va[:,0], mask_v)
    a_true = tf.boolean_mask(y_true_va[:,1], mask_a)
    a_pred = tf.boolean_mask(y_pred_va[:,1], mask_a)
    ccc_v = ccc(v_true, v_pred)
    ccc_a = ccc(a_true, a_pred)
    return (1 - ccc_v + 1 - ccc_a) / 2

@tf.keras.utils.register_keras_serializable()
def au_loss(y_true_au, y_pred_au):
    mask = tf.not_equal(y_true_au, -1)
    y_true_masked = tf.where(mask, tf.cast(y_true_au, tf.float32), 0.0)
    return tf.keras.losses.binary_crossentropy(y_true_masked, y_pred_au)

@tf.keras.utils.register_keras_serializable()
class MulticlassF1Score(tf.keras.metrics.Metric):
    def __init__(self, num_classes=8, name='f1', **kwargs):
        super(MulticlassF1Score, self).__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.per_class_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fp = [self.add_weight(name=f'fp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fn = [self.add_weight(name=f'fn_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.int32)
        mask = tf.not_equal(y_true, -1)
        y_true = tf.boolean_mask(y_true, mask)
        y_pred = tf.boolean_mask(y_pred, mask)
        y_pred_classes = tf.argmax(y_pred, axis=-1, output_type=tf.int32)
        for i in range(self.num_classes):
            class_mask_true = tf.equal(y_true, i)
            class_mask_pred = tf.equal(y_pred_classes, i)
            tp = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, class_mask_pred), tf.float32))
            fp = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not(class_mask_true), class_mask_pred), tf.float32))
            fn = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, tf.logical_not(class_mask_pred)), tf.float32))
            self.per_class_tp[i].assign_add(tp)
            self.per_class_fp[i].assign_add(fp)
            self.per_class_fn[i].assign_add(fn)
    def result(self):
        prec = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fp[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        rec = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fn[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        f1_per_class = [2 * (prec[i] * rec[i]) / (prec[i] + rec[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        return tf.reduce_mean(tf.stack(f1_per_class))
    def reset_state(self):
        for i in range(self.num_classes):
            self.per_class_tp[i].assign(0.0)
            self.per_class_fp[i].assign(0.0)
            self.per_class_fn[i].assign(0.0)
    def get_config(self):
        config = super().get_config()
        config.update({'num_classes': self.num_classes})
        return config

@tf.keras.utils.register_keras_serializable()
class MulticlassPrecision(tf.keras.metrics.Metric):
    def __init__(self, num_classes=8, name='precision', **kwargs):
        super(MulticlassPrecision, self).__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.per_class_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fp = [self.add_weight(name=f'fp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.int32)
        mask = tf.not_equal(y_true, -1)
        y_true_masked = tf.boolean_mask(y_true, mask)
        y_pred_masked = tf.boolean_mask(y_pred, mask)
        y_pred_classes = tf.argmax(y_pred_masked, axis=-1, output_type=tf.int32)
        for i in range(self.num_classes):
            class_mask_true = tf.equal(y_true_masked, i)
            class_mask_pred = tf.equal(y_pred_classes, i)
            tp = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, class_mask_pred), tf.float32))
            fp = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not(class_mask_true), class_mask_pred), tf.float32))
            self.per_class_tp[i].assign_add(tp)
            self.per_class_fp[i].assign_add(fp)
    def result(self):
        precision_per_class = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fp[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        return tf.reduce_mean(tf.stack(precision_per_class))
    def reset_state(self):
        for i in range(self.num_classes):
            self.per_class_tp[i].assign(0.0)
            self.per_class_fp[i].assign(0.0)
    def get_config(self):
        config = super().get_config()
        config.update({'num_classes': self.num_classes})
        return config

@tf.keras.utils.register_keras_serializable()
class MulticlassRecall(tf.keras.metrics.Metric):
    def __init__(self, num_classes=8, name='recall', **kwargs):
        super(MulticlassRecall, self).__init__(name=name, **kwargs)
        self.num_classes = num_classes
        self.per_class_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
        self.per_class_fn = [self.add_weight(name=f'fn_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(num_classes)]
    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.int32)
        mask = tf.not_equal(y_true, -1)
        y_true_masked = tf.boolean_mask(y_true, mask)
        y_pred_masked = tf.boolean_mask(y_pred, mask)
        y_pred_classes = tf.argmax(y_pred_masked, axis=-1, output_type=tf.int32)
        for i in range(self.num_classes):
            class_mask_true = tf.equal(y_true_masked, i)
            class_mask_pred = tf.equal(y_pred_classes, i)
            tp = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, class_mask_pred), tf.float32))
            fn = tf.reduce_sum(tf.cast(tf.logical_and(class_mask_true, tf.logical_not(class_mask_pred)), tf.float32))
            self.per_class_tp[i].assign_add(tp)
            self.per_class_fn[i].assign_add(fn)
    def result(self):
        recall_per_class = [self.per_class_tp[i] / (self.per_class_tp[i] + self.per_class_fn[i] + tf.keras.backend.epsilon()) for i in range(self.num_classes)]
        return tf.reduce_mean(tf.stack(recall_per_class))
    def reset_state(self):
        for i in range(self.num_classes):
            self.per_class_tp[i].assign(0.0)
            self.per_class_fn[i].assign(0.0)
    def get_config(self):
        config = super().get_config()
        config.update({'num_classes': self.num_classes})
        return config

@tf.keras.utils.register_keras_serializable()
class VA_CCC(tf.keras.metrics.Metric):
    def __init__(self, name='va_ccc', **kwargs):
        super().__init__(name=name, **kwargs)
        self.ccc_v_sum = self.add_weight(shape=(), name='ccc_v_sum', initializer='zeros')
        self.ccc_a_sum = self.add_weight(shape=(), name='ccc_a_sum', initializer='zeros')
        self.count = self.add_weight(shape=(), name='count', initializer='zeros')
    def update_state(self, y_true_va, y_pred_va, sample_weight=None):
        mask_v = tf.greater(y_true_va[:,0], -1.5)
        mask_a = tf.greater(y_true_va[:,1], -1.5)
        v_true = tf.boolean_mask(y_true_va[:,0], mask_v)
        v_pred = tf.boolean_mask(y_pred_va[:,0], mask_v)
        a_true = tf.boolean_mask(y_true_va[:,1], mask_a)
        a_pred = tf.boolean_mask(y_pred_va[:,1], mask_a)
        self.ccc_v_sum.assign_add(ccc(v_true, v_pred))
        self.ccc_a_sum.assign_add(ccc(a_true, a_pred))
        self.count.assign_add(1.0)
    def result(self):
        return (self.ccc_v_sum + self.ccc_a_sum) / (2 * self.count + tf.keras.backend.epsilon())
    def reset_state(self):
        self.ccc_v_sum.assign(0.0)
        self.ccc_a_sum.assign(0.0)
        self.count.assign(0.0)
    def get_config(self):
        config = super().get_config()
        config.update({'name': self.name})
        return config

@tf.keras.utils.register_keras_serializable()
class AU_F1(tf.keras.metrics.Metric):
    def __init__(self, name='au_f1', **kwargs):
        super().__init__(name=name, **kwargs)
        self.per_au_tp = [self.add_weight(name=f'tp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(12)]
        self.per_au_fp = [self.add_weight(name=f'fp_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(12)]
        self.per_au_fn = [self.add_weight(name=f'fn_{i}', shape=(), initializer='zeros', dtype=tf.float32) for i in range(12)]
    def update_state(self, y_true_au, y_pred_au, sample_weight=None):
        mask = tf.not_equal(y_true_au, -1)
        y_true_masked = tf.where(mask, tf.cast(y_true_au, tf.float32), tf.zeros_like(y_true_au, dtype=tf.float32))
        y_pred_bin = tf.round(y_pred_au)
        for i in range(12):
            tp = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(y_true_masked[:,i], tf.int32) == 1, tf.cast(y_pred_bin[:,i], tf.int32) == 1), tf.float32))
            fp = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(y_true_masked[:,i], tf.int32) == 0, tf.cast(y_pred_bin[:,i], tf.int32) == 1), tf.float32))
            fn = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(y_true_masked[:,i], tf.int32) == 1, tf.cast(y_pred_bin[:,i], tf.int32) == 0), tf.float32))
            self.per_au_tp[i].assign_add(tp)
            self.per_au_fp[i].assign_add(fp)
            self.per_au_fn[i].assign_add(fn)
    def result(self):
        prec = [self.per_au_tp[i] / (self.per_au_tp[i] + self.per_au_fp[i] + tf.keras.backend.epsilon()) for i in range(12)]
        rec = [self.per_au_tp[i] / (self.per_au_tp[i] + self.per_au_fn[i] + tf.keras.backend.epsilon()) for i in range(12)]
        f1_per_au = [2 * (prec[i] * rec[i]) / (prec[i] + rec[i] + tf.keras.backend.epsilon()) for i in range(12)]
        return tf.reduce_mean(tf.stack(f1_per_au))
    def reset_state(self):
        for i in range(12):
            self.per_au_tp[i].assign(0.0)
            self.per_au_fp[i].assign(0.0)
            self.per_au_fn[i].assign(0.0)
    def get_config(self):
        config = super().get_config()
        config.update({'name': self.name})
        return config

# Load model
print(f"Loading model from: {model_path}")
try:
    model = tf.keras.models.load_model(model_path, custom_objects={
        'masked_focal_sparse_categorical_crossentropy': masked_focal_sparse_categorical_crossentropy,
        'loss_fn': masked_focal_sparse_categorical_crossentropy()(y_true_expr=tf.zeros([1], dtype=tf.int32), y_pred_expr=tf.zeros([1, 8], dtype=tf.float32)),
        'va_loss': va_loss,
        'au_loss': au_loss,
        'ccc': ccc,
        'MulticlassF1Score': MulticlassF1Score,
        'MulticlassPrecision': MulticlassPrecision,
        'MulticlassRecall': MulticlassRecall,
        'VA_CCC': VA_CCC,
        'AU_F1': AU_F1
    })
except Exception as e:
    print(f"Error loading model: {e}")
    raise

# Run predictions
print("Running predictions on test set...")
predictions = model.predict(test_dataset, verbose=1)
expr_pred = np.argmax(predictions['expr'], axis=-1)
va_pred = predictions['va']
au_pred = (predictions['au'] > 0.5).astype(np.int32)

# Filter valid expression labels for confusion matrix
valid_mask = expr_test >= 0
y_true_expr = expr_test[valid_mask]
y_pred_expr = expr_pred[valid_mask]

# Define class names for expressions
class_names = ['Neutral', 'Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Other']

# Compute confusion matrix
cm = confusion_matrix(y_true_expr, y_pred_expr, labels=range(8))

# Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix for Expression Classification')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.tight_layout()
cm_path = os.path.join(output_dir, 'confusion_matrix.png')
plt.savefig(cm_path)
plt.close()
print(f"Confusion matrix saved to {cm_path}")

# Generate classification report
report = classification_report(y_true_expr, y_pred_expr, target_names=class_names, output_dict=True, zero_division=0)
report_df = pd.DataFrame(report).transpose()
report_path = os.path.join(output_dir, 'classification_report.csv')
report_df.to_csv(report_path)
print(f"Classification report saved to {report_path}")

# VA prediction visualization (valence-arousal scatter plot)
valid_va_mask = (va_test[:, 0] > -1.5) & (va_test[:, 1] > -1.5)
va_true_valid = va_test[valid_va_mask]
va_pred_valid = va_pred[valid_va_mask]

plt.figure(figsize=(10, 8))
plt.scatter(va_true_valid[:, 0], va_true_valid[:, 1], c='blue', alpha=0.5, label='True VA')
plt.scatter(va_pred_valid[:, 0], va_pred_valid[:, 1], c='red', alpha=0.5, label='Predicted VA')
plt.xlabel('Valence')
plt.ylabel('Arousal')
plt.title('Valence-Arousal Scatter Plot')
plt.legend()
plt.grid(True)
va_plot_path = os.path.join(output_dir, 'va_scatter_plot.png')
plt.savefig(va_plot_path)
plt.close()
print(f"VA scatter plot saved to {va_plot_path}")

# AU prediction metrics
from sklearn.metrics import f1_score

au_f1_scores = []
for i in range(12):
    valid_au_mask = au_test[:, i] >= 0
    true_au = au_test[valid_au_mask, i]
    pred_au = au_pred[valid_au_mask, i]
    if len(true_au) > 0 and len(np.unique(true_au)) > 1:
        f1 = f1_score(true_au, pred_au, average='binary', pos_label=1, zero_division=0)
        au_f1_scores.append(f1)
    else:
        au_f1_scores.append(0.0)

# Plot AU F1-scores with AU labels
au_labels = ['AU1','AU2','AU4','AU6','AU7','AU10','AU12','AU15','AU23','AU24','AU25','AU26']
plt.figure(figsize=(10, 6))
plt.bar(au_labels, au_f1_scores)
plt.xlabel('Action Unit')
plt.ylabel('F1 Score')
plt.title('F1 Scores per Action Unit')
plt.ylim(0, 1)  # Ensure y-axis starts at 0 and goes to 1
plt.grid(True, axis='y')
au_plot_path = os.path.join(output_dir, 'au_f1_scores.png')
plt.savefig(au_plot_path)
plt.close()
print(f"AU F1 scores plot saved to {au_plot_path}")

#Save summary metrics
from sklearn.metrics import f1_score
expr_macro_f1 = f1_score(y_true_expr, y_pred_expr, average='macro', zero_division=0)
au_mean_f1 = np.mean(au_f1_scores)
va_ccc_v = ccc(va_true_valid[:, 0], va_pred_valid[:, 0]) if len(va_true_valid) > 0 else 0.0
va_ccc_a = ccc(va_true_valid[:, 1], va_pred_valid[:, 1]) if len(va_true_valid) > 0 else 0.0
va_mean_ccc = np.mean([va_ccc_v, va_ccc_a])

summary = {
    'Mean Expr F1': expr_macro_f1,
    'Mean AU F1': au_mean_f1,
    'VA CCC': va_mean_ccc
}
summary_df = pd.DataFrame([summary])
summary_path = os.path.join(output_dir, 'test_summary.csv')
summary_df.to_csv(summary_path, index=False)
print(f"Summary metrics saved to {summary_path}")

print("Testing complete. All results saved to", output_dir)

# Full .keras Save for Cache2 (epoch 49) & Cache3 (epoch 42)
from google.colab import drive
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import (GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Input, MultiHeadAttention)
from tensorflow.keras.models import Model
from tensorflow.keras import regularizers
from tensorflow.keras.applications import EfficientNetV2S
from tensorflow.keras.applications.efficientnet_v2 import preprocess_input
from tensorflow.keras import ops
from sklearn.utils.class_weight import compute_class_weight
import pickle
import os
import random
import zipfile  # For size check
import shutil  # For Drive copy

drive.mount('/content/drive')

# Config
num_epochs_per_model = 50
batch_size = 512
initial_unfreeze = 250
checkpoint_dir = '/content/drive/My Drive/EXPR_Recognition Models/'

cache_path = f'/content/drive/My Drive/combined_cache.pkl'
checkpoint_dir = '/content/drive/My Drive/EXPR_Recognition Models/'
weights_path = '/content/drive/My Drive/EXPR_Recognition Models/finetuned_mtl_model_epoch_99.weights'
full_model_save_path = os.path.join(checkpoint_dir, f'full_mtl_model_cache{cache_id}_epoch99.keras')

# Loop over models
for cache_id in [2, 3]:
    epoch_to_load = 49 if cache_id == 2 else 42
    print(f"\n--- Saving Cache{cache_id} (epoch {epoch_to_load}) ---")

    cache_path = f'/content/drive/My Drive/cache{cache_id}.pkl'
    weights_path = os.path.join(checkpoint_dir, f'new_mtl_model_cache{cache_id}_epoch_{epoch_to_load:02d}.weights.h5')
    local_save_path = f'/tmp/full_mtl_model_cache{cache_id}_epoch{epoch_to_load}.keras'  # Local /tmp
    drive_save_path = os.path.join(checkpoint_dir, f'full_mtl_model_cache{cache_id}_epoch{epoch_to_load}.keras')  # Drive copy

    # Debug: Check original file size (full file, not just weights)
    print(f"Original file size: {os.path.getsize(weights_path) / 1e6:.0f} MB (likely includes optimizer state)")

    # Load cache
    with open(cache_path, 'rb') as f:
        train_pairs = pickle.load(f)

    def class_balancer(pairs):
        expr_labels = np.array([lbl['expr'] for _, lbl in pairs])
        valid_indices = [i for i, lbl in enumerate(expr_labels) if lbl >= 0]
        if not valid_indices:
            return pairs
        valid_expr = expr_labels[valid_indices]
        unique = np.unique(valid_expr)
        max_count = max(np.sum(valid_expr == cls) for cls in unique)
        sampled_indices = []
        for cls in unique:
            cls_indices = [valid_indices[j] for j in range(len(valid_expr)) if valid_expr[j] == cls]
            sampled_cls = random.choices(cls_indices, k=max_count)
            sampled_indices.extend(sampled_cls)
        random.shuffle(sampled_indices)
        return [pairs[i] for i in sampled_indices]

    train_pairs_subset = class_balancer(train_pairs)
    train_image_paths = [pair[0] for pair in train_pairs_subset]
    expr_train = np.array([lbl['expr'] for _, lbl in train_pairs_subset], dtype=np.int32)

    steps_per_epoch = len(train_image_paths) // batch_size
    print(f"Steps per epoch: {steps_per_epoch}")

    # Class weights
    valid_expr_labels = expr_train[expr_train >= 0]
    unique_classes = np.unique(valid_expr_labels)
    class_weights = compute_class_weight('balanced', classes=unique_classes, y=valid_expr_labels)
    class_weights_dict = dict(zip(unique_classes, class_weights))
    boost_factors = {1: 6.0, 2: 5.0, 3: 4.0, 4: 1.0, 5: 2.0, 6: 3.0, 7: 1.5}
    for cls, factor in boost_factors.items():
        if cls in class_weights_dict:
            class_weights_dict[cls] *= factor
    class_weights_list = [class_weights_dict.get(i, 1.0) for i in range(8)]
    class_weights_tensor = tf.constant(class_weights_list, dtype=tf.float32)
    print("Class weights:", class_weights_dict)

    # Strategy & precision (set to float32 for full save)
    strategy = tf.distribute.get_strategy()
    original_policy = tf.keras.mixed_precision.global_policy()
    tf.keras.mixed_precision.set_global_policy('float32')  # Full precision for save

    # Custom losses (fixed class with reduction)
    @tf.keras.utils.register_keras_serializable()
    class MaskedFocalSparseCategoricalCrossentropy(tf.keras.losses.Loss):
        def __init__(self, gamma=4.0, label_smoothing=0.05, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='masked_focal_sparse_categorical_crossentropy'):
            super().__init__(reduction=reduction, name=name)
            self.gamma = gamma
            self.label_smoothing = label_smoothing

        def call(self, y_true, y_pred):
            y_true = tf.cast(y_true, tf.int32)
            mask = tf.not_equal(y_true, -1)
            mask_f = tf.cast(mask, tf.float32)
            y_true_safe = tf.where(mask, y_true, 0)
            alpha = tf.gather(class_weights_tensor, y_true_safe)
            y_true_one_hot = tf.one_hot(y_true_safe, depth=8)
            y_true_one_hot = (1 - self.label_smoothing) * y_true_one_hot + (self.label_smoothing / 8)
            ce = tf.keras.losses.categorical_crossentropy(y_true_one_hot, y_pred, from_logits=False)
            pt = tf.exp(-ce)
            focal_loss = alpha * ((1 - pt) ** self.gamma) * ce
            focal_loss = tf.where(mask, focal_loss, tf.zeros_like(focal_loss))
            return tf.reduce_sum(focal_loss) / (tf.reduce_sum(mask_f) + tf.keras.backend.epsilon())

    def ccc(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)
        mu_true = tf.reduce_mean(y_true)
        mu_pred = tf.reduce_mean(y_pred)
        var_true = tf.math.reduce_variance(y_true)
        var_pred = tf.math.reduce_variance(y_pred)
        cov = tf.reduce_mean((y_true - mu_true) * (y_pred - mu_pred))
        return (2 * cov) / (var_true + var_pred + (mu_true - mu_pred)**2 + 1e-6)

    def va_loss(y_true_va, y_pred_va):
        mask_v = tf.greater(y_true_va[:,0], -1.5)
        mask_a = tf.greater(y_true_va[:,1], -1.5)
        v_true = tf.boolean_mask(y_true_va[:,0], mask_v)
        v_pred = tf.boolean_mask(y_pred_va[:,0], mask_v)
        a_true = tf.boolean_mask(y_true_va[:,1], mask_a)
        a_pred = tf.boolean_mask(y_pred_va[:,1], mask_a)
        ccc_v = ccc(v_true, v_pred)
        ccc_a = ccc(a_true, a_pred)
        return (1 - ccc_v + 1 - ccc_a) / 2

    def au_loss(y_true_au, y_pred_au):
        mask = tf.not_equal(y_true_au, -1)
        y_true_masked = tf.where(mask, tf.cast(y_true_au, tf.float32), 0.0)
        return tf.keras.losses.binary_crossentropy(y_true_masked, y_pred_au)

    # Custom objects for load
    custom_objects = {
        'MaskedFocalSparseCategoricalCrossentropy': MaskedFocalSparseCategoricalCrossentropy,
        'va_loss': va_loss,
        'au_loss': au_loss,
        'ccc': ccc
    }

    # Model creation (uncompiled)
    with strategy.scope():
        def create_model():
            full_input = Input(shape=(224, 224, 3), name='full')
            base_model = EfficientNetV2S(
                weights='imagenet',
                include_top=False,
                input_tensor=full_input,
                input_shape=(224, 224, 3),
                name='efficientnetv2-s'
            )
            base_model.trainable = True
            for layer in base_model.layers[:-initial_unfreeze]:
                layer.trainable = False
            base_output = GlobalAveragePooling2D()(base_model.output)
            fused = Dropout(0.8)(base_output)
            fused_with_seq = ops.expand_dims(fused, axis=1)
            attention = MultiHeadAttention(num_heads=4, key_dim=128)(fused_with_seq, fused_with_seq)
            attention = ops.squeeze(attention, axis=1)
            x = BatchNormalization()(attention)
            x = Dropout(0.7)(x)
            shared = Dense(256, activation='relu')(x)
            expr_branch = Dense(128, activation='relu')(shared)
            va_branch = Dense(128, activation='relu')(shared)
            au_branch = Dense(128, activation='relu')(shared)
            expr_out = Dense(8, activation='softmax', dtype='float32', kernel_regularizer=regularizers.l2(1e-2), name='expr')(expr_branch)
            va_out = Dense(2, activation='tanh', name='va')(va_branch)
            au_out = Dense(12, activation='sigmoid', name='au')(au_branch)
            model = Model(inputs=full_input, outputs={'expr': expr_out, 'va': va_out, 'au': au_out})
            return model

        model = create_model()

    # Load weights on uncompiled model
    model.load_weights(weights_path)
    print(f"Loaded weights from {weights_path}")

    # Compile AFTER load
    total_steps = steps_per_epoch * num_epochs_per_model
    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=5e-6, decay_steps=total_steps)
    model.compile(
        optimizer=tf.keras.optimizers.AdamW(learning_rate=lr_schedule),
        loss={
            'expr': MaskedFocalSparseCategoricalCrossentropy(),
            'va': va_loss,
            'au': au_loss
        },
        loss_weights={'expr': 1.0, 'va': 1.0, 'au': 1.0}
    )

    # Save .keras (local first)
    model.save(local_save_path, save_format='keras_v3')
    print(f"Saved full .keras model to {local_save_path}")

    # Copy to Drive
    shutil.copy2(local_save_path, drive_save_path)
    print(f"Copied to Drive: {drive_save_path}")

    # Check sizes (now using drive_save_path for consistency)
    print(f"Full .keras file size: {os.path.getsize(drive_save_path) / 1e6:.0f} MB")
    with zipfile.ZipFile(drive_save_path, 'r') as z:
        weights_files = [f for f in z.namelist() if 'weights.h5' in f]
        if weights_files:
            print(f"Internal weights.h5 size: {z.getinfo(weights_files[0]).file_size / 1e6:.0f} MB (pure weights; original file includes optimizer)")

    # Verify load
    loaded_model = tf.keras.models.load_model(drive_save_path, custom_objects=custom_objects)
    dummy_input = tf.random.normal((1, 224, 224, 3))
    preds = loaded_model.predict(dummy_input, verbose=0)
    print("Test predict keys/shapes:", {k: v.shape for k, v in preds.items()})
    print("Verify success—model full and loadable!")

    # Restore original policy
    tf.keras.mixed_precision.set_global_policy(original_policy)

for batch in val_dataset.take(1):
    images, labels = batch
    preds = model.predict(images, verbose=0)
    print("Sample expr preds:", preds['expr'][:5])
    print("Sample expr labels:", labels['expr'].numpy()[:5])

#Data splitting and caching
import os
from google.cloud import storage
from google.colab import auth, drive
import numpy as np
import random
import pickle
from tqdm import tqdm
from collections import Counter

# Authenticate and mount
auth.authenticate_user()
drive.mount('/content/drive')

# Load main pairs (train only, filter valid EXPR >=0)
main_pickle_path = '/content/drive/My Drive/unified_mtl_pairs_cache.pkl'
with open(main_pickle_path, 'rb') as f:
    train_pairs, val_pairs = pickle.load(f)
valid_main_pairs = [p for p in train_pairs if p[1]['expr'] >= 0]
print(f"Loaded {len(valid_main_pairs)} valid main training pairs.")
print(f"Loaded {len(val_pairs)} original validation pairs.")

# Load synthetic pairs
synthetic_pickle_path = '/content/drive/My Drive/synthetic_mtl_pairs.pkl'
with open(synthetic_pickle_path, 'rb') as f:
    synthetic_pairs = pickle.load(f)
print(f"Loaded {len(synthetic_pairs)} synthetic pairs.")

# Load FANE pairs
fane_pickle_path = '/content/drive/My Drive/fane_mtl_pairs.pkl'
with open(fane_pickle_path, 'rb') as f:
    fane_pairs = pickle.load(f)
print(f"Loaded {len(fane_pairs)} FANE pairs.")

# Create new validation set: 1k FANE + 4k original
# Stratify FANE to ~200 per emotion (angry, disgust, fear, sad, surprise)
fane_by_emotion = {emotion: [] for emotion in ['angry', 'disgust', 'fear', 'sad', 'surprise']}
for pair in fane_pairs:
    expr = pair[1]['expr']
    if expr == 1:
        fane_by_emotion['angry'].append(pair)
    elif expr == 2:
        fane_by_emotion['disgust'].append(pair)
    elif expr == 3:
        fane_by_emotion['fear'].append(pair)
    elif expr == 5:
        fane_by_emotion['sad'].append(pair)
    elif expr == 6:
        fane_by_emotion['surprise'].append(pair)

# Select ~200 per emotion (1k total)
fane_val = []
for emotion in fane_by_emotion:
    samples = random.sample(fane_by_emotion[emotion], min(200, len(fane_by_emotion[emotion])))
    fane_val.extend(samples)
    print(f"Selected {len(samples)} {emotion} samples for validation.")
random.shuffle(fane_val)

# Select 4k from original validation
val_4k = random.sample(val_pairs, min(4000, len(val_pairs)))
print(f"Selected {len(val_4k)} samples from original validation.")

# Combine and save new validation set
new_val_pairs = fane_val + val_4k
random.shuffle(new_val_pairs)
val_cache_path = '/content/drive/My Drive/validation_cache_5k.pkl'
with open(val_cache_path, 'wb') as f:
    pickle.dump(new_val_pairs, f)
print(f"Saved {len(new_val_pairs)} pairs to {val_cache_path}")
print("New validation EXPR distribution:", Counter(pair[1]['expr'] for pair in new_val_pairs))

# Remove validation FANE images from training FANE
fane_val_paths = set(pair[0] for pair in fane_val)
fane_train = [pair for pair in fane_pairs if pair[0] not in fane_val_paths]
print(f"Remaining FANE training pairs: {len(fane_train)}")

# Combine training data: main + synthetic + remaining FANE
all_train_pairs = valid_main_pairs + synthetic_pairs + fane_train
print(f"Total training pairs: {len(all_train_pairs)}")
random.shuffle(all_train_pairs)

# Cap at 100k total, balancing classes
total_target = 100000
num_subsets = 5
subset_size = total_target // num_subsets  # 20000

# Group by class
class_pairs = {cls: [] for cls in range(8)}
for pair in tqdm(all_train_pairs, desc="Grouping training pairs by class"):
    expr = pair[1]['expr']
    if expr >= 0:  # Only valid EXPR
        class_pairs[expr].append(pair)

minority_classes = [1, 2, 3, 5, 6]  # anger, disgust, fear, sad, surprise
majority_classes = [0, 4, 7]  # neutral, happy, other
minority_counts = {1: 3774, 2: 1737, 3: 3062, 5: 15140, 6: 4070}  # From your code
major_counts = {0: 59416, 4: 23415, 7: 79021}
for cls in minority_classes:
    actual = len(class_pairs[cls])
    if actual != minority_counts.get(cls, 0):
        print(f"Warning: Expected {minority_counts.get(cls, 0)} for class {cls}, got {actual}")
for cls in majority_classes:
    actual = len(class_pairs[cls])
    if actual != major_counts.get(cls, 0):
        print(f"Warning: Expected {major_counts.get(cls, 0)} for class {cls}, got {actual}")

# Select 100k balanced
selected_pairs = []
for cls in minority_classes:
    selected_pairs.extend(class_pairs[cls])
major_target = total_target - sum(len(class_pairs[cls]) for cls in minority_classes)
major_reduction = major_target / sum(len(class_pairs[cls]) for cls in majority_classes)
for cls in majority_classes:
    target_count = int(len(class_pairs[cls]) * major_reduction)
    selected_pairs.extend(random.sample(class_pairs[cls], min(target_count, len(class_pairs[cls]))))

# Shuffle and trim
random.shuffle(selected_pairs)
if len(selected_pairs) > total_target:
    selected_pairs = random.sample(selected_pairs, total_target)
print(f"Selected {len(selected_pairs)} training pairs.")

# Split into 5 subsets
subsets = [selected_pairs[i:i + subset_size] for i in range(0, len(selected_pairs), subset_size)]
for i, subset in enumerate(subsets):
    cache_path = f'/content/drive/My Drive/cache{i+1}.pkl'
    with open(cache_path, 'wb') as f:
        pickle.dump(subset, f)
    print(f"Saved cache{i+1} with {len(subset)} pairs to {cache_path}")
    print(f"Cache{i+1} EXPR distribution:", Counter(pair[1]['expr'] for pair in subset))

#Validation Set Generator
import os
from google.colab import auth, drive
import random
import pickle
from tqdm import tqdm
from collections import Counter

# Authenticate and mount
auth.authenticate_user()
drive.mount('/content/drive')

# Load original validation pairs
main_pickle_path = '/content/drive/My Drive/unified_mtl_pairs_cache.pkl'
with open(main_pickle_path, 'rb') as f:
    _, val_pairs = pickle.load(f)
print(f"Loaded {len(val_pairs)} original validation pairs.")

# Load FANE pairs
fane_pickle_path = '/content/drive/My Drive/fane_mtl_pairs.pkl'
with open(fane_pickle_path, 'rb') as f:
    fane_pairs = pickle.load(f)
print(f"Loaded {len(fane_pairs)} FANE pairs.")

# Select 1k FANE samples (stratified ~200 per emotion)
fane_by_emotion = {emotion: [] for emotion in ['angry', 'disgust', 'fear', 'sad', 'surprise']}
for pair in fane_pairs:
    expr = pair[1]['expr']
    if expr == 1:
        fane_by_emotion['angry'].append(pair)
    elif expr == 2:
        fane_by_emotion['disgust'].append(pair)
    elif expr == 3:
        fane_by_emotion['fear'].append(pair)
    elif expr == 5:
        fane_by_emotion['sad'].append(pair)
    elif expr == 6:
        fane_by_emotion['surprise'].append(pair)

fane_val = []
for emotion in fane_by_emotion:
    samples = random.sample(fane_by_emotion[emotion], min(200, len(fane_by_emotion[emotion])))
    fane_val.extend(samples)
    print(f"Selected {len(samples)} {emotion} samples for validation.")
random.shuffle(fane_val)
print(f"Total FANE validation samples: {len(fane_val)}")

# Filter valid original validation pairs (expr >= 0)
valid_val_pairs = [pair for pair in val_pairs if pair[1]['expr'] >= 0]
print(f"Filtered {len(valid_val_pairs)} valid validation pairs (expr >= 0).")

# Stratify 4k valid samples across classes 0-7
class_pairs = {cls: [] for cls in range(8)}
for pair in valid_val_pairs:
    expr = pair[1]['expr']
    class_pairs[expr].append(pair)

# Aim for ~500 per class
target_per_class = 500  # 4000 / 8 ≈ 500
val_4k = []
for cls in range(8):
    samples = random.sample(class_pairs[cls], min(target_per_class, len(class_pairs[cls])))
    val_4k.extend(samples)
    print(f"Selected {len(samples)} samples for class {cls}.")
random.shuffle(val_4k)
print(f"Total valid original validation samples: {len(val_4k)}")

# Combine and save new validation set
new_val_pairs = fane_val + val_4k
random.shuffle(new_val_pairs)
val_cache_path = '/content/drive/My Drive/validation_cache_5k.pkl'
with open(val_cache_path, 'wb') as f:
    pickle.dump(new_val_pairs, f)
print(f"Saved {len(new_val_pairs)} pairs to {val_cache_path}")
print("New validation EXPR distribution:", Counter(pair[1]['expr'] for pair in new_val_pairs))

#Generating Synthetic Images
import os
from google.cloud import storage
from google.colab import auth, drive
import torch
from diffusers import StableDiffusionPipeline
import random
import pickle
from tqdm import tqdm
import io
from PIL import Image

# Authenticate and mount
auth.authenticate_user()
drive.mount('/content/drive')

bucket_name = 'facial-expressions_bucket'
synthetic_prefix = 'Faces Dataset/synthetic/'
client = storage.Client()
bucket = client.get_bucket(bucket_name)

# Set up Stable Diffusion
pipe = StableDiffusionPipeline.from_pretrained(
    "CompVis/stable-diffusion-v1-4",
    torch_dtype=torch.float16,
    safety_checker=None  # Disable if needed, but be cautious
)
pipe = pipe.to("cuda")
pipe.enable_attention_slicing()  # For memory efficiency

# Define emotions and labels
# EXPR classes: Assuming 0=Neutral,1=Anger,2=Disgust,3=Fear,4=Happy,5=Sad,6=Surprise,7=Other
# VA: [-1,1] for valence, arousal
# AU: 12 units, binary list [AU1,2,4,5,6,9,12,15,17,20,25,26]
emotions = {
    'anger': {
        'expr': 1,
        'va': [-0.6, 0.7],
        'au': [0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0],  # AU4,5,17 approx for anger
        'prompt_base': "a realistic close-up photo of a person's face expressing intense anger, high detail, natural lighting, diverse ethnicity and age"
    },
    'disgust': {
        'expr': 2,
        'va': [-0.7, 0.4],
        'au': [0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0],  # AU9,15,17
        'prompt_base': "a realistic close-up photo of a person's face expressing strong disgust, high detail, natural lighting, diverse ethnicity and age"
    },
    'fear': {
        'expr': 3,
        'va': [-0.5, 0.8],
        'au': [1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1],  # AU1,2,4,5,20,26
        'prompt_base': "a realistic close-up photo of a person's face expressing fear, high detail, natural lighting, diverse ethnicity and age"
    },
    'sad': {
        'expr': 5,
        'va': [-0.8, -0.3],
        'au': [1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],  # AU1,4,15
        'prompt_base': "a realistic close-up photo of a person's face expressing deep sadness, high detail, natural lighting, diverse ethnicity and age"
    }
}

# Generate 250 images per emotion (total 1k)
num_per_emotion = 250
synthetic_pairs = []

for emotion, data in emotions.items():
    emotion_prefix = f"{synthetic_prefix}{emotion}/"
    print(f"Generating {num_per_emotion} images for {emotion}...")

    for i in tqdm(range(num_per_emotion)):
        # Vary prompt for diversity
        prompt = data['prompt_base'] + f", variation {random.randint(1,100)}"
        image = pipe(prompt).images[0]

        # Save to buffer and upload to GCS
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format='JPEG')
        img_byte_arr.seek(0)
        blob_name = f"{emotion_prefix}{i:05d}.jpg"
        blob = bucket.blob(blob_name)
        blob.upload_from_file(img_byte_arr, content_type='image/jpeg')

        # Create pair
        gs_path = f"gs://{bucket_name}/{blob_name}"
        labels = {
            'expr': data['expr'],
            'va': data['va'],
            'au': data['au']
        }
        synthetic_pairs.append((gs_path, labels))

# Shuffle for randomness
random.shuffle(synthetic_pairs)

# Pickle the synthetic pairs
pickle_path = '/content/drive/My Drive/synthetic_mtl_pairs.pkl'
with open(pickle_path, 'wb') as f:
    pickle.dump(synthetic_pairs, f)
print(f"Saved {len(synthetic_pairs)} synthetic pairs to {pickle_path}")

#Saved Subset Cache Pairs
import os
from google.cloud import storage
from google.colab import auth, drive
import numpy as np
import random
import pickle
from tqdm import tqdm

# Authenticate and mount
auth.authenticate_user()
drive.mount('/content/drive')

# Load main pairs (only train_pairs used, filter valid EXPR >=0)
main_pickle_path = '/content/drive/My Drive/unified_mtl_pairs_cache.pkl'
with open(main_pickle_path, 'rb') as f:
    train_pairs, _ = pickle.load(f)
valid_main_pairs = [p for p in train_pairs if p[1]['expr'] >= 0]
print(f"Loaded {len(valid_main_pairs)} valid main pairs.")

# Load synthetic pairs
synthetic_pickle_path = '/content/drive/My Drive/synthetic_mtl_pairs.pkl'
with open(synthetic_pickle_path, 'rb') as f:
    synthetic_pairs = pickle.load(f)
print(f"Loaded {len(synthetic_pairs)} synthetic pairs.")

# Load original FANE pairs (angry, disgust, fear, sad)
fane_pickle_path = '/content/drive/My Drive/fane_mtl_pairs.pkl'
with open(fane_pickle_path, 'rb') as f:
    fane_pairs = pickle.load(f)
print(f"Loaded {len(fane_pairs)} original FANE pairs.")


# Combine all FANE
all_fane_pairs = fane_pairs

# Combine all sources
all_pairs = valid_main_pairs + synthetic_pairs + all_fane_pairs
print(f"Total combined pairs: {len(all_pairs)}")

# Shuffle combined pairs
random.shuffle(all_pairs)

# Define classes (include surprise as boosted class)
minority_classes = [1, 2, 3, 5, 6]  # anger, disgust, fear, sad, surprise (now boosted)
majority_classes = [0, 4, 7]  # neutral, happy, other

# Collect pairs by class
class_pairs = {cls: [] for cls in range(8)}
for pair in tqdm(all_pairs, desc="Grouping pairs by class"):
    expr = pair[1]['expr']
    class_pairs[expr].append(pair)

# Target total: 100,000
total_target = 100000
num_subsets = 5
subset_size = total_target // num_subsets  # 20000

# Updated minority counts with FANE surprise
# Assuming FANE surprise adds ~3000
# Recalculated: Anger 3774, Disgust 1737, Fear 3062, Sad 15140, Surprise 5216 (2216 main + 3000 FANE)
minority_counts = {1: 3774, 2: 1737, 3: 3062, 5: 15140, 6: 4070}
major_counts = {0: 59416, 4: 23415, 7: 79021}

# Select 100k balanced
selected_pairs = []

# Add all minorities
for cls in minority_classes:
    selected_pairs.extend(class_pairs[cls])

# Add reduced majors
major_target = total_target - sum(minority_counts.values())  # ~66,071
major_reduction = major_target / sum(major_counts.values())  # ~0.418
for cls in majority_classes:
    target_count = int(major_counts[cls] * major_reduction)
    selected_pairs.extend(random.sample(class_pairs[cls], min(target_count, len(class_pairs[cls]))))

# Shuffle selected
random.shuffle(selected_pairs)

# Trim to exactly 100k if overshot
if len(selected_pairs) > total_target:
    selected_pairs = random.sample(selected_pairs, total_target)

# Split into 5 subsets
subsets = [selected_pairs[i:i + subset_size] for i in range(0, len(selected_pairs), subset_size)]

# Save each subset as cache
for i, subset in enumerate(subsets):
    cache_path = f'/content/drive/My Drive/cache{i+1}.pkl'
    with open(cache_path, 'wb') as f:
        pickle.dump(subset, f)
    print(f"Saved cache{i+1} with {len(subset)} pairs to {cache_path}")

#Saving the FANE data as a singular cache
import os
from google.cloud import storage
from google.colab import auth, drive
import pickle
from tqdm import tqdm
import random

# Authenticate and mount Drive
auth.authenticate_user()
drive.mount('/content/drive')

# GCS setup
client = storage.Client()
bucket_name = 'facial-expressions_bucket'
bucket = client.get_bucket(bucket_name)
fane_prefix = 'Faces Dataset/fane/'

# Define emotion mappings and labels
emotions = {
    'angry': {
        'expr': 1,
        'va': [-0.6, 0.7],
        'au': [0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0]  # AU4,5,17 for anger
    },
    'disgust': {
        'expr': 2,
        'va': [-0.7, 0.4],
        'au': [0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0]  # AU9,15,17
    },
    'fear': {
        'expr': 3,
        'va': [-0.5, 0.8],
        'au': [1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1]  # AU1,2,4,5,20,26
    },
    'sad': {
        'expr': 5,
        'va': [-0.8, -0.3],
        'au': [1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0]  # AU1,4,15
    },
    'surprise': {
        'expr': 6,
        'va': [0.2, 0.9],
        'au': [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]  # AU1,2,5,26
    }
}

# Selected emotions
selected_emotions = ['angry', 'disgust', 'fear', 'sad', 'surprise']

# Fetch and create pairs
fane_pairs = []
for emotion in selected_emotions:
    emotion_prefix = f"{fane_prefix}{emotion}/"
    print(f"Fetching {emotion} images from GCS...")

    blobs = list(bucket.list_blobs(prefix=emotion_prefix))
    for blob in tqdm(blobs):
        if blob.name.lower().endswith(('.jpg', '.jpeg', '.png')):
            gs_path = f"gs://{bucket_name}/{blob.name}"
            labels = emotions[emotion]
            fane_pairs.append((gs_path, labels))

# Shuffle for randomness
random.shuffle(fane_pairs)

# Pickle the FANE pairs to Google Drive
pickle_path = '/content/drive/My Drive/fane_mtl_pairs.pkl'
with open(pickle_path, 'wb') as f:
    pickle.dump(fane_pairs, f)
print(f"Saved {len(fane_pairs)} FANE pairs to {pickle_path}")

#Synthetic Image Generator
import os
from google.cloud import storage
from google.colab import auth, drive
import torch
from diffusers import StableDiffusionPipeline
import random
import pickle
from tqdm import tqdm
import io
from PIL import Image

# Authenticate and mount
auth.authenticate_user()
drive.mount('/content/drive')

bucket_name = 'facial-expressions_bucket'
synthetic_prefix = 'Faces Dataset/synthetic/'
client = storage.Client()
bucket = client.get_bucket(bucket_name)

# Set up Stable Diffusion
pipe = StableDiffusionPipeline.from_pretrained(
    "CompVis/stable-diffusion-v1-4",
    torch_dtype=torch.float16,
    safety_checker=None  # Disable if needed, but be cautious
)
pipe = pipe.to("cuda")
pipe.enable_attention_slicing()  # For memory efficiency

# Define emotions with refined prompts (more human-like: visible eyes, warm lighting, approachable features)
emotions = {
    'anger': {
        'expr': 1,
        'va': [-0.6, 0.7],
        'au': [0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0],  # AU4,5,17
        'prompt_base': "photorealistic close-up of the entire face of a singular diverse person expressing intense anger with furrowed brows, narrowed but bright visible eyes, clenched jaw, tight lips, warm and friendly lighting, approachable and relatable human features, natural skin texture, realistic proportions, no deformities or artifacts, high detail"
    },
    'disgust': {
        'expr': 2,
        'va': [-0.7, 0.4],
        'au': [0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0],  # AU9,15,17
        'prompt_base': "photorealistic close-up of the entire face of a singular diverse person expressing strong disgust with wrinkled nose, raised upper lip, squinted but bright visible eyes, downturned mouth, warm and friendly lighting, approachable and relatable human features, natural skin texture, realistic proportions, no deformities or artifacts, high detail"
    },
    'fear': {
        'expr': 3,
        'va': [-0.5, 0.8],
        'au': [1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1],  # AU1,2,4,5,20,26
        'prompt_base': "photorealistic close-up of the entire face of a singular diverse person expressing fear with wide and bright visible eyes, raised eyebrows, slightly open mouth, tense forehead, warm and friendly lighting, approachable and relatable human features, natural skin texture, realistic proportions, no deformities or artifacts, high detail"
    },
    'sad': {
        'expr': 5,
        'va': [-0.8, -0.3],
        'au': [1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],  # AU1,4,15
        'prompt_base': "photorealistic close-up of the entire face of a singular diverse person expressing deep sadness with downturned mouth, drooping but bright visible eyes, furrowed brows, teary gaze, warm and friendly lighting, approachable and relatable human features, natural skin texture, realistic proportions, no deformities or artifacts, high detail"
    }
}

# Custom numbers per emotion
num_images = {
    'anger': 1600,
    'disgust': 2000,
    'fear': 2000,
    'sad': 500
}

synthetic_pairs = []
negative_prompt = "blurry, deformed, ugly, cartoonish, low quality, overexposed, underexposed, artifacts, dark shadows, scary, eerie, unnatural eyes"

for emotion, data in emotions.items():
    num = num_images[emotion]
    emotion_prefix = f"{synthetic_prefix}{emotion}/"
    print(f"Generating {num} images for {emotion}...")

    for i in tqdm(range(num)):
        # Start index from 251 (add 251 to i)
        file_index = 251 + i

        # Vary prompt for diversity
        prompt = f"{data['prompt_base']}, variation {random.randint(1,100)}, diverse ethnicity and age"
        image = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=50).images[0]

        # Save to buffer and upload to GCS
        img_byte_arr = io.BytesIO()
        image.save(img_byte_arr, format='JPEG')
        img_byte_arr.seek(0)
        blob_name = f"{emotion_prefix}{file_index:05d}.jpg"
        blob = bucket.blob(blob_name)
        blob.upload_from_file(img_byte_arr, content_type='image/jpeg')

        # Create pair
        gs_path = f"gs://{bucket_name}/{blob_name}"
        labels = {
            'expr': data['expr'],
            'va': data['va'],
            'au': data['au']
        }
        synthetic_pairs.append((gs_path, labels))

# Shuffle for randomness
random.shuffle(synthetic_pairs)

# Pickle the synthetic pairs
pickle_path = '/content/drive/My Drive/synthetic_mtl_pairs.pkl'
with open(pickle_path, 'wb') as f:
    pickle.dump(synthetic_pairs, f)
print(f"Saved {len(synthetic_pairs)} synthetic pairs to {pickle_path}")

#Printing data imbalance
import pickle
from google.colab import auth, drive

# Authenticate and mount
auth.authenticate_user()
drive.mount('/content/drive')

# Load main pairs
pickle_path = '/content/drive/My Drive/unified_mtl_pairs_cache.pkl'
with open(pickle_path, 'rb') as f:
    train_pairs, val_pairs = pickle.load(f)
print(f"Total train pairs (unfiltered): {len(train_pairs)}")

# Count invalid and valid
invalid_count = len([p for p in train_pairs if p[1]['expr'] < 0])
valid_count = len([p for p in train_pairs if p[1]['expr'] >= 0])
print(f"Invalid pairs: {invalid_count}")
print(f"Valid pairs: {valid_count}")

# Class distribution (as before)
expr_labels = np.array([p[1]['expr'] for p in train_pairs if p[1]['expr'] >= 0])
unique_classes, counts = np.unique(expr_labels, return_counts=True)
class_names = {0: 'Neutral', 1: 'Anger', 2: 'Disgust', 3: 'Fear', 4: 'Happy', 5: 'Sad', 6: 'Surprise', 7: 'Other'}
for cls, count in zip(unique_classes, counts):
    print(f"{class_names.get(cls, f'Class {cls}')}: {count} examples")

# Recorded Video YOLOv11 Test VIDEO
from google.colab import auth
import os
import random
from google.cloud import storage
from ultralytics import YOLO
from huggingface_hub import hf_hub_download
from IPython.display import Video

# Authenticate with Google Cloud
auth.authenticate_user()

# Set up Google Cloud Storage client
client = storage.Client()
bucket_name = 'facial-expressions_bucket'
prefix = 'Video Test/new_vids/'
bucket = client.bucket(bucket_name)

# List all blobs in the specified prefix
blobs = bucket.list_blobs(prefix=prefix)

# Filter for video files
video_files = [blob.name for blob in blobs if blob.name.lower().endswith(('.mp4', '.avi', '.mov'))]

# Check if there are any videos
if not video_files:
    print("No video files found in the specified bucket path.")
else:
    # Select a random video
    random_video_blob = random.choice(video_files)
    local_video_path = '/content/' + os.path.basename(random_video_blob)

    # Download the video to local Colab storage
    bucket.blob(random_video_blob).download_to_filename(local_video_path)

    print(f"Downloaded video: {local_video_path}")

    # Download the YOLOv11 face detection model from Hugging Face
    model_path = hf_hub_download(repo_id="AdamCodd/YOLOv11n-face-detection", filename="model.pt")

    # Load the YOLOv11 face detection model
    model = YOLO(model_path)

    # Perform face tracking on the video and save the output
    results = model.track(source=local_video_path, save=True, tracker="botsort.yaml")

    # The output video is saved in runs/detect/track/ or track2/ etc.; find the latest track folder
runs_dir = 'runs/detect'
exp_folders = [f for f in os.listdir(runs_dir) if f.startswith('track')]
latest_exp = max(exp_folders, key=lambda x: int(x[len('track'):] or 0)) if exp_folders else 'track'
# Output video is saved as .avi
base_name = os.path.basename(local_video_path).rsplit('.', 1)[0]
output_video_path = os.path.join(runs_dir, latest_exp, base_name + '.avi')

# Convert to MP4 if necessary (for better compatibility in Colab)
output_mp4 = output_video_path.rsplit('.', 1)[0] + '.mp4'
!ffmpeg -i {output_video_path} {output_mp4} -y

# Display the output video in Colab
display(Video(output_mp4, embed=True))

#Watching Test Video
# The output video is saved in runs/detect/track/ or track2/ etc.; find the latest track folder
runs_dir = 'runs/detect'
exp_folders = [f for f in os.listdir(runs_dir) if f.startswith('track')]
latest_exp = max(exp_folders, key=lambda x: int(x[len('track'):] or 0)) if exp_folders else 'track'
# Output video is saved as .avi
base_name = os.path.basename(local_video_path).rsplit('.', 1)[0]
output_video_path = os.path.join(runs_dir, latest_exp, base_name + '.avi')

# Convert to MP4 if necessary (for better compatibility in Colab)
output_mp4 = output_video_path.rsplit('.', 1)[0] + '.mp4'
!ffmpeg -i {output_video_path} {output_mp4} -y

# Display the output video in Colab
display(Video(output_mp4, embed=True))

#Visualizing/Testing Data Annotation

from google.cloud import storage
from google.colab import auth
import os
import random
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

# Authenticate the user
auth.authenticate_user()

# Set the bucket name
bucket_name = 'facial-expressions_bucket'

# Define the prefixes for data folders and annotation folders
data_prefixes = [
    'Faces Dataset/batch1/',
    'Faces Dataset/batch2/',
    'Faces Dataset/cropped_new_50_vids/'
]
train_annotation_prefix = 'Faces Dataset/ABAW Annotations/EXPR_Recognition_Challenge/Train_Set/'
validation_annotation_prefix = 'Faces Dataset/ABAW Annotations/EXPR_Recognition_Challenge/Validation_Set/'

# Create a client and get the bucket
client = storage.Client()
bucket = client.get_bucket(bucket_name)

# Function to list folders in a given prefix
def list_folders(prefix):
    blobs = bucket.list_blobs(prefix=prefix, delimiter='/')
    folders = set()
    for page in blobs.pages:
        folders.update(page.prefixes)
    return folders

# List all data folders from the specified prefixes
print("Listing data folders...")
data_folders = []
for prefix in data_prefixes:
    folders = list_folders(prefix)
    data_folders.extend(folders)
print(f"Found {len(data_folders)} data folders.")

# Create a dictionary mapping base names to full folder paths
data_folder_dict = {}
for folder in data_folders:
    base_name = folder.split('/')[-2]
    data_folder_dict[base_name] = folder

# Function to list annotation files in a given prefix
def list_annotation_files(prefix):
    blobs = bucket.list_blobs(prefix=prefix)
    return [blob.name for blob in blobs if blob.name.endswith('.txt')]

# List train and validation annotation files
print("Listing training annotation files...")
train_annotation_files = list_annotation_files(train_annotation_prefix)
print(f"Found {len(train_annotation_files)} training annotation files.")

print("Listing validation annotation files...")
validation_annotation_files = list_annotation_files(validation_annotation_prefix)
print(f"Found {len(validation_annotation_files)} validation annotation files.")

# Pair train annotations with data folders
train_pairs = []
for anno_file in train_annotation_files:
    base_name = os.path.splitext(os.path.basename(anno_file))[0]
    if base_name in data_folder_dict:
        train_pairs.append((anno_file, data_folder_dict[base_name]))

# Pair validation annotations with data folders
validation_pairs = []
for anno_file in validation_annotation_files:
    base_name = os.path.splitext(os.path.basename(anno_file))[0]
    if base_name in data_folder_dict:
        validation_pairs.append((anno_file, data_folder_dict[base_name]))

# Define emotion labels
emotions = ["Neutral", "Anger", "Disgust", "Fear", "Happiness", "Sadness", "Surprise", "Other"]

# Select 10 random pairs from train_pairs
selected_pairs = random.sample(train_pairs, min(10, len(train_pairs)))
print(f"Selected {len(selected_pairs)} pairs.")

# Prepare to display 10 images in a 2x5 grid
fig, axes = plt.subplots(2, 5, figsize=(15, 6))
axes = axes.flatten()

# Process each selected pair
for i, (annotation_file, data_folder) in enumerate(selected_pairs):
    print(f"Processing pair {i+1}: {annotation_file} and {data_folder}")

    # Download annotation file
    blob = bucket.blob(annotation_file)
    local_annotation_file = f'/tmp/annotation_{i}.txt'
    blob.download_to_filename(local_annotation_file)

    # Read labels, skipping the header
    with open(local_annotation_file, 'r') as f:
        lines = f.readlines()
        labels = [int(line.strip()) for line in lines[1:]]

    # List and sort image blobs
    image_blobs = [blob for blob in bucket.list_blobs(prefix=data_folder) if blob.name.endswith('.jpg')]
    image_blobs.sort(key=lambda blob: blob.name)

    # Pair images with labels and filter out label = -1
    filtered_pairs = [(img_blob, label) for img_blob, label in zip(image_blobs, labels) if label != -1]

    # Check if there are valid pairs
    if not filtered_pairs:
        print(f"No valid image-label pairs found for pair {i+1} (all labels might be -1).")
        axes[i].text(0.5, 0.5, 'No valid image', ha='center', va='center')
        axes[i].axis('off')
        continue

    # Select a random valid pair
    selected_img_blob, selected_label = random.choice(filtered_pairs)

    # Download the image
    local_image_file = f'/tmp/image_{i}.jpg'
    selected_img_blob.download_to_filename(local_image_file)

    # Display the image with its emotion label
    img = mpimg.imread(local_image_file)
    axes[i].imshow(img)
    axes[i].set_title(emotions[selected_label])
    axes[i].axis('off')

# Adjust layout and show the plot
plt.tight_layout()
plt.show()

#First Model (AU's only) Test on dataset
from google.cloud import storage
from google.colab import auth
import os
import random
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np

# Authenticate the user
auth.authenticate_user()

# Set the bucket name
bucket_name = 'facial-expressions_bucket'

# Define the prefixes for data folders and annotation folder
data_prefixes = [
    'Faces Dataset/batch1/',
    'Faces Dataset/batch2/',
    'Faces Dataset/cropped_new_50_vids/'
]
annotation_prefix = 'Faces Dataset/ABAW Annotations/AU_Detection_Challenge/Train_Set/'

# Create a client and get the bucket
client = storage.Client()
bucket = client.get_bucket(bucket_name)

# Function to list folders in a given prefix
def list_folders(prefix):
    blobs = bucket.list_blobs(prefix=prefix, delimiter='/')
    folders = set()
    for page in blobs.pages:
        folders.update(page.prefixes)
    return folders

# List all data folders
print("Listing data folders...")
data_folders = []
for prefix in data_prefixes:
    folders = list_folders(prefix)
    data_folders.extend(folders)
print(f"Found {len(data_folders)} data folders.")

# Map base names to full folder paths
data_folder_dict = {folder.split('/')[-2]: folder for folder in data_folders}

# Function to list annotation files
def list_annotation_files(prefix):
    blobs = bucket.list_blobs(prefix=prefix)
    return [blob.name for blob in blobs if blob.name.endswith('.txt')]

# List annotation files
print("Listing annotation files...")
annotation_files = list_annotation_files(annotation_prefix)
print(f"Found {len(annotation_files)} annotation files.")

# Pair annotations with data folders
pairs = []
for anno_file in annotation_files:
    base_name = os.path.splitext(os.path.basename(anno_file))[0]
    if base_name in data_folder_dict:
        pairs.append((anno_file, data_folder_dict[base_name]))

# Define AUs in the order they appear in the dataset
aus = ['AU1', 'AU2', 'AU4', 'AU6', 'AU7', 'AU10', 'AU12', 'AU15', 'AU23', 'AU24', 'AU25', 'AU26']

# Define emotions and their expected active AUs (0-based indices)
emotions = {
    "Happiness": {3, 6},  # AU6, AU12
    "Sadness": {0, 2, 7},  # AU1, AU4, AU15
    "Surprise": {0, 1, 11},  # AU1, AU2, AU26
    "Fear": {0, 1, 2, 4, 11},  # AU1, AU2, AU4, AU7, AU26
    "Anger": {2, 4, 8},  # AU4, AU7, AU23
    "Disgust": {5, 7},  # AU10, AU15
    "Neutral": set()  # No AUs active
}

# Function to compute scores for each emotion
def compute_scores(au_scores, emotions):
    O = set([i for i, score in enumerate(au_scores) if score == 1])
    scores = {}
    for emotion, S_E in emotions.items():
        TP = len(S_E.intersection(O))
        FP = len(O.difference(S_E))
        FN = len(S_E.difference(O))
        score = TP - FP - FN
        scores[emotion] = score
    return scores

# Function to compute probabilities using softmax
def compute_probabilities(scores):
    values = np.array(list(scores.values()))
    exp_values = np.exp(values - np.max(values))  # Numerical stability
    probs = exp_values / np.sum(exp_values)
    return {emotion: prob for emotion, prob in zip(scores.keys(), probs)}

# Select 5 random pairs
selected_pairs = random.sample(pairs, min(5, len(pairs)))

# Set up the plot with extra space for labels
fig, axes = plt.subplots(1, 5, figsize=(20, 8))

# Process each selected pair
for idx, (annotation_file, data_folder) in enumerate(selected_pairs):
    print(f"\nProcessing pair {idx+1}: {annotation_file} and {data_folder}")

    # Download annotation file
    blob = bucket.blob(annotation_file)
    local_annotation_file = f'/tmp/annotation_{idx}.txt'
    blob.download_to_filename(local_annotation_file)

    # Read AU scores, skipping header
    with open(local_annotation_file, 'r') as f:
        lines = f.readlines()[1:]  # Skip header
        scores = [list(map(int, line.strip().split(','))) for line in lines]

    # List images
    image_blobs = [blob for blob in bucket.list_blobs(prefix=data_folder) if blob.name.endswith('.jpg')]

    if not image_blobs:
        print(f"No images found for {data_folder}")
        axes[idx].axis('off')
        axes[idx].set_title("No images")
        continue

    # Select a random image
    selected_img_blob = random.choice(image_blobs)
    frame_number = int(os.path.basename(selected_img_blob.name).split('.')[0])

    if frame_number - 1 >= len(scores):
        print(f"Warning: Frame number {frame_number} out of range for annotation with {len(scores)} lines")
        axes[idx].axis('off')
        axes[idx].set_title("Invalid frame number")
        continue

    au_scores = scores[frame_number - 1]

    # Compute scores and probabilities
    scores_dict = compute_scores(au_scores, emotions)
    probs = compute_probabilities(scores_dict)

    # Find predicted emotion and top 3
    predicted_emotion = max(probs, key=probs.get)
    top_3 = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:3]
    top_3_str = ", ".join([f"{emotion}: {prob:.2f}" for emotion, prob in top_3])

    # Download the image
    local_image_file = f'/tmp/image_{idx}.jpg'
    selected_img_blob.download_to_filename(local_image_file)

    # Load and plot the image
    img = mpimg.imread(local_image_file)
    axes[idx].imshow(img)
    axes[idx].set_title(predicted_emotion)
    axes[idx].axis('off')

    # Add AU labels below the image
    au_labels = ", ".join([f"{au}: {score}" for au, score in zip(aus, au_scores)])
    axes[idx].text(0.5, -0.15, au_labels, ha='center', va='top', transform=axes[idx].transAxes, fontsize=8)

    # Add top 3 emotions below the AU labels
    axes[idx].text(0.5, -0.25, f"Top 3: {top_3_str}", ha='center', va='top', transform=axes[idx].transAxes, fontsize=8)

# Adjust layout to make room for labels
plt.subplots_adjust(bottom=0.4)
plt.show()

#Visualizing total dataset (only AU's)
subset_ratio = 0.1  # Adjust this value to use more or less data
train_pairs_subset = random.sample(train_pairs, int(len(train_pairs) * subset_ratio))
val_pairs_subset = random.sample(val_pairs, int(len(val_pairs) * subset_ratio))

# Extract image paths and labels from the subset
train_image_paths, train_labels = zip(*train_pairs_subset)
val_image_paths, val_labels = zip(*val_pairs_subset)

# Convert labels to numpy arrays
train_labels = np.array(train_labels)
val_labels = np.array(val_labels)
val_label_sums = np.sum(val_labels, axis=0)
print("Validation label distribution:", val_label_sums)
train_label_sums = np.sum(train_labels, axis=0)
print("Training label distribution:", train_label_sums)

#AU Detection Model Start
from google.cloud import storage
from google.colab import auth, drive
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import (Input, Conv2D, DepthwiseConv2D, BatchNormalization,
                                     Activation, Add, GlobalAveragePooling2D, Dense, Dropout, ZeroPadding2D,
                                     Reshape, Softmax, Multiply, Layer)
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
import random
import pickle
from sklearn.utils.class_weight import compute_class_weight

# Authenticate the user
auth.authenticate_user()

# Mount Google Drive
drive.mount('/content/drive')

# Set the bucket name
bucket_name = 'facial-expressions_bucket'

# Define the prefixes for data folders and annotation folders
data_prefixes = [
    'Faces Dataset/batch1/',
    'Faces Dataset/batch2/',
    'Faces Dataset/cropped_new_50_vids/'
]
train_annotation_prefix = 'Faces Dataset/ABAW Annotations/AU_Detection_Challenge/Train_Set/'
val_annotation_prefix = 'Faces Dataset/ABAW Annotations/AU_Detection_Challenge/Validation_Set/'

# Create a client and get the bucket
client = storage.Client()
bucket = client.get_bucket(bucket_name)

# Function to list folders in a given prefix
def list_folders(bucket, prefix):
    blobs = bucket.list_blobs(prefix=prefix, delimiter='/')
    folders = []
    for page in blobs.pages:
        folders.extend(page.prefixes)
    return folders

# Function to get dataset pairs (image paths and labels) with -1 handling and error management
def get_dataset_pairs(bucket, annotation_prefix, data_prefixes):
    pairs = []
    skipped = 0
    has_negative_one = False
    for prefix in data_prefixes:
        folders = list_folders(bucket, prefix)
        for folder in folders:
            base_name = folder.split('/')[-2]
            anno_file = f"{annotation_prefix}{base_name}.txt"
            if not bucket.blob(anno_file).exists():
                continue
            try:
                with tf.io.gfile.GFile(f"gs://{bucket_name}/{anno_file}", 'r') as f:
                    lines = f.readlines()[1:]  # Skip header
                    labels = [list(map(int, line.strip().split(','))) for line in lines]
            except Exception as e:
                print(f"Error reading annotation {anno_file}: {e}")
                continue
            image_blobs = [blob for blob in bucket.list_blobs(prefix=folder) if blob.name.endswith('.jpg')]
            for img_blob in image_blobs:
                try:
                    frame_number = int(os.path.basename(img_blob.name).split('.')[0])
                    if frame_number <= len(labels):
                        label = labels[frame_number - 1]
                        if -1 in label:
                            has_negative_one = True
                            # We'll mask in loss, so keep them
                        image_path = f"gs://{bucket_name}/{img_blob.name}"
                        pairs.append((image_path, label))
                    else:
                        print(f"Warning: Frame number {frame_number} out of range for {anno_file}")
                        skipped += 1
                except ValueError:
                    print(f"Invalid frame name: {img_blob.name}")
                    skipped += 1
    if has_negative_one:
        print(f"Warning: -1 labels found in {annotation_prefix}. Will mask them in loss.")
    print(f"Skipped {skipped} invalid pairs.")
    return pairs

# Load or collect pairs
pickle_path = '/content/drive/My Drive/pairs_cache.pkl'
if os.path.exists(pickle_path):
    with open(pickle_path, 'rb') as f:
        train_pairs, val_pairs = pickle.load(f)
    print("Loaded pairs from cache.")
else:
    print("Collecting training pairs...")
    train_pairs = get_dataset_pairs(bucket, train_annotation_prefix, data_prefixes)
    print(f"Found {len(train_pairs)} training pairs.")

    print("Collecting validation pairs...")
    val_pairs = get_dataset_pairs(bucket, val_annotation_prefix, data_prefixes)
    print(f"Found {len(val_pairs)} validation pairs.")

    with open(pickle_path, 'wb') as f:
        pickle.dump((train_pairs, val_pairs), f)
    print("Saved pairs to cache.")

# Use a subset of the data (30%)
subset_ratio = 0.3
train_pairs_subset = random.sample(train_pairs, int(len(train_pairs) * subset_ratio))
val_pairs_subset = random.sample(val_pairs, int(len(val_pairs) * subset_ratio))

# Extract image paths and labels (convert to np for weights)
train_image_paths = [pair[0] for pair in train_pairs_subset]
train_labels = np.array([pair[1] for pair in train_pairs_subset])

val_image_paths = [pair[0] for pair in val_pairs_subset]
val_labels = np.array([pair[1] for pair in val_pairs_subset])

# For class weights, ignore -1 by filtering to 0/1 per AU
label_weights = []
for i in range(12):
    col = train_labels[:, i]
    valid = col[col >= 0]  # Exclude -1
    unique = np.unique(valid)
    if len(unique) < 2:
        label_weights.append(1.0)
    else:
        weights = compute_class_weight('balanced', classes=unique, y=valid)
        pos_weight = weights[np.where(unique == 1)[0][0]] if 1 in unique else 1.0
        label_weights.append(pos_weight)
label_weights = np.array(label_weights)
label_weights = np.clip(label_weights, 0.5, 5.0)
label_weights = tf.constant(label_weights, dtype=tf.float32)
print("Class weights:", label_weights.numpy())

# Define custom weighted binary cross-entropy loss with masking for -1
def weighted_binary_crossentropy(label_weights):
    def loss(y_true, logits):
        y_true = tf.cast(y_true, tf.float32)
        mask = tf.greater_equal(y_true, 0.0)  # True for 0/1, False for -1
        logits = tf.clip_by_value(logits, -5.0, 5.0)  # Tighter clip for stability
        # Compute BCE only on valid labels (set y_true to 0 for masking, but actually mask loss)
        bce = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=logits)
        bce = tf.where(mask, bce, tf.zeros_like(bce))  # Zero loss for masked
        # Apply weights (tile to batch)
        batch_size = tf.shape(bce)[0]
        weights = tf.tile(tf.expand_dims(label_weights, 0), [batch_size, 1])
        weighted_bce = bce * weights
        # Average over valid elements
        num_valid = tf.reduce_sum(tf.cast(mask, tf.float32))
        return tf.reduce_sum(weighted_bce) / tf.maximum(num_valid, 1.0)  # Avoid div by 0
    return loss

loss_function = weighted_binary_crossentropy(label_weights)

# Custom F1 metric for multi-label (average over labels)
class F1Score(tf.keras.metrics.Metric):
    def __init__(self, name='f1', **kwargs):
        super(F1Score, self).__init__(name=name, **kwargs)
        self.precision = tf.keras.metrics.Precision(thresholds=0.5)
        self.recall = tf.keras.metrics.Recall(thresholds=0.5)

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.precision.update_state(y_true, y_pred, sample_weight)
        self.recall.update_state(y_true, y_pred, sample_weight)

    def result(self):
        p = self.precision.result()
        r = self.recall.result()
        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))

    def reset_state(self):
        self.precision.reset_state()
        self.recall.reset_state()

# Function to load and preprocess images with error handling
def load_and_preprocess_image(path, label):
    try:
        img = tf.io.read_file(path)
        img = tf.image.decode_jpeg(img, channels=3)
        img = tf.image.resize(img, [224, 224])
        img = tf.cast(img, tf.float32) / 255.0
        return img, label
    except:
        return tf.zeros((224, 224, 3)), label  # Dummy on error

# Lower batch size further and tweak for memory
batch_size = 16  # Even lower to ensure VRAM stability; test and increase if possible
shuffle_buffer = 500  # Halve to save minor memory

# !nvidia-smi  # Run this in a cell to check baseline VRAM

# Enable GPU memory growth (must be before any TF ops)
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# Enable mixed precision (add before model.compile)
from tensorflow.keras.mixed_precision import set_global_policy
set_global_policy('mixed_float16')

# Create TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((train_image_paths, train_labels))
train_dataset = train_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).ignore_errors()
train_dataset = train_dataset.shuffle(buffer_size=shuffle_buffer).batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)  # Add .repeat() to prevent data exhaustion

val_dataset = tf.data.Dataset.from_tensor_slices((val_image_paths, val_labels))
val_dataset = val_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE).ignore_errors()
val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)  # No cache

def relu(x):
    return tf.nn.relu(x)

# FusedIB block (fused expansion + DW for stem)
def fused_ib(inputs, expansion_factor, stride, output_channels, block_id):
    input_channels = inputs.shape[-1]
    expanded_channels = input_channels * expansion_factor
    x = Conv2D(expanded_channels, kernel_size=3, strides=stride, padding='same', use_bias=False, name=f'fused_{block_id}_expand_conv')(inputs)
    x = BatchNormalization(name=f'fused_{block_id}_bn')(x)
    x = Activation(relu, name=f'fused_{block_id}_relu')(x)
    x = Conv2D(output_channels, kernel_size=1, padding='same', use_bias=False, name=f'fused_{block_id}_project_conv')(x)
    x = BatchNormalization(name=f'fused_{block_id}_project_bn')(x)
    if stride == 1 and input_channels == output_channels:
        x = Add(name=f'fused_{block_id}_add')([inputs, x])
    return x

# UIB block (universal inverted bottleneck)
def uib(inputs, expansion_factor, stride, output_channels, kernel_size, pre_dw, mid_dw, block_id):
    input_channels = inputs.shape[-1]
    expanded_channels = input_channels * expansion_factor

    x = inputs
    if pre_dw:
        x = DepthwiseConv2D(kernel_size=kernel_size, strides=1, padding='same', use_bias=False, name=f'uib_{block_id}_pre_dw')(x)
        x = BatchNormalization(name=f'uib_{block_id}_pre_bn')(x)
        x = Activation(relu, name=f'uib_{block_id}_pre_relu')(x)

    # Expansion
    x = Conv2D(expanded_channels, kernel_size=1, padding='same', use_bias=False, name=f'uib_{block_id}_expand_conv')(x)
    x = BatchNormalization(name=f'uib_{block_id}_expand_bn')(x)
    x = Activation(relu, name=f'uib_{block_id}_expand_relu')(x)

    if mid_dw:
        x = DepthwiseConv2D(kernel_size=kernel_size, strides=1, padding='same', use_bias=False, name=f'uib_{block_id}_mid_dw')(x)
        x = BatchNormalization(name=f'uib_{block_id}_mid_bn')(x)
        x = Activation(relu, name=f'uib_{block_id}_mid_relu')(x)

    # Depthwise
    if stride == 2:
        x = ZeroPadding2D(padding=((0,1),(0,1)))(x)
    x = DepthwiseConv2D(kernel_size=kernel_size, strides=stride, padding='valid' if stride == 2 else 'same', use_bias=False, name=f'uib_{block_id}_dw')(x)
    x = BatchNormalization(name=f'uib_{block_id}_dw_bn')(x)
    x = Activation(relu, name=f'uib_{block_id}_dw_relu')(x)

    # Projection (linear, no activation)
    x = Conv2D(output_channels, kernel_size=1, padding='same', use_bias=False, name=f'uib_{block_id}_project_conv')(x)
    x = BatchNormalization(name=f'uib_{block_id}_project_bn')(x)

    if stride == 1 and input_channels == output_channels:
        x = Add(name=f'uib_{block_id}_add')([inputs, x])
    return x

# Variant helpers (set flags for UIB)
def uib_ib(inputs, exp, stride, out, k, block_id):
    return uib(inputs, exp, stride, out, k, pre_dw=False, mid_dw=False, block_id=block_id)

def uib_convnext(inputs, exp, stride, out, k, block_id):
    return uib(inputs, exp, stride, out, k, pre_dw=True, mid_dw=False, block_id=block_id)

def uib_extradw(inputs, exp, stride, out, k, block_id):
    return uib(inputs, exp, stride, out, k, pre_dw=True, mid_dw=True, block_id=block_id)

# Mobile Multi-Query Attention (MQA) as a custom layer (reduced heads and higher sr_ratio to lower memory)
class MQA_Layer(Layer):
    def __init__(self, num_heads=2, sr_ratio=4, **kwargs):  # Reduced heads, increased sr_ratio for smaller k/v
        super(MQA_Layer, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.sr_ratio = sr_ratio

    def build(self, input_shape):
        channels = input_shape[-1]
        self.q_dense = Dense(channels)
        self.k_dense = Dense(channels // self.num_heads)
        self.v_dense = Dense(channels // self.num_heads)
        self.out_dense = Dense(channels)
        self.sr_dw = DepthwiseConv2D(kernel_size=3, strides=self.sr_ratio, padding='same')
        self.sr_bn = BatchNormalization()

    def call(self, inputs):
        sr = self.sr_dw(inputs)
        sr = self.sr_bn(sr)
        q = self.q_dense(inputs)
        k = self.k_dense(sr)
        v = self.v_dense(sr)
        # Get dynamic shapes
        b = tf.shape(q)[0]
        h = tf.shape(q)[1]
        w = tf.shape(q)[2]
        c = tf.shape(q)[3]
        seq_len = h * w
        head_dim = c // self.num_heads
        q = tf.reshape(q, [b, seq_len, self.num_heads, head_dim])
        # For k/v
        sr_h = tf.shape(k)[1]
        sr_w = tf.shape(k)[2]
        sr_seq = sr_h * sr_w
        k = tf.reshape(k, [b, sr_seq, 1, head_dim])
        v = tf.reshape(v, [b, sr_seq, 1, head_dim])
        # Attention
        attn = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(head_dim, tf.float32))
        attn = tf.nn.softmax(attn)
        out = tf.matmul(attn, v)
        out = tf.reshape(out, [b, h, w, c])
        out = self.out_dense(out)
        return tf.multiply(inputs, out) + inputs  # Residual

# Build MobileNetV4-Conv-L with MQA additions (hybrid-like)
inputs = Input(shape=(224, 224, 3), name='input')

# Stem
x = Conv2D(32, kernel_size=3, strides=2, padding='same', use_bias=False, name='stem_conv')(inputs)
x = BatchNormalization(name='stem_bn')(x)
x = Activation(relu, name='stem_relu')(x)

x = fused_ib(x, expansion_factor=4, stride=2, output_channels=32, block_id=1)
x = fused_ib(x, expansion_factor=4, stride=2, output_channels=64, block_id=2)

# Body (sample config for Conv-L; adjusted last stride to 1 to avoid invalid shapes on small spatial dims)
block_id = 3
x = uib_extradw(x, 4, 2, 96, 5, block_id); block_id += 1
x = uib_ib(x, 4, 1, 96, 3, block_id); block_id += 1
x = uib_ib(x, 4, 1, 96, 3, block_id); block_id += 1
x = uib_convnext(x, 4, 2, 128, 5, block_id); block_id += 1
x = uib_convnext(x, 4, 1, 128, 5, block_id); block_id += 1
x = uib_convnext(x, 4, 1, 128, 5, block_id); block_id += 1
x = uib_extradw(x, 4, 1, 128, 5, block_id); block_id += 1
x = uib_extradw(x, 4, 1, 128, 5, block_id); block_id += 1
x = uib_extradw(x, 4, 1, 128, 5, block_id); block_id += 1
x = uib_ib(x, 4, 2, 192, 3, block_id); block_id += 1
x = uib_ib(x, 4, 1, 192, 3, block_id); block_id += 1
x = uib_ib(x, 4, 1, 192, 3, block_id); block_id += 1
x = uib_ib(x, 4, 1, 192, 3, block_id); block_id += 1
# x = MQA_Layer()(x)  # Temporarily comment out MQA to test if it's the VRAM culprit; uncomment once stable
x = uib_convnext(x, 4, 1, 192, 5, block_id); block_id += 1
x = uib_convnext(x, 4, 1, 192, 5, block_id); block_id += 1
x = uib_convnext(x, 4, 1, 192, 5, block_id); block_id += 1
# x = MQA_Layer()(x)  # Temporarily comment out; uncomment once stable
x = uib_extradw(x, 4, 1, 320, 5, block_id); block_id += 1  # Changed stride to 1 to avoid invalid conv on small spatial size

# Final
x = Conv2D(1280, kernel_size=1, padding='same', use_bias=False, name='final_conv')(x)
x = BatchNormalization(name='final_bn')(x)
x = Activation(relu, name='final_relu')(x)

# Custom top
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
logits = Dense(12, activation=None, dtype='float32')(x)

model = Model(inputs=inputs, outputs=logits)

model.compile(
    optimizer=tf.keras.optimizers.Adam(5e-4),
    loss=loss_function,
    metrics=[
        tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=0.5),
        tf.keras.metrics.Precision(name='precision', thresholds=0.5),
        tf.keras.metrics.Recall(name='recall', thresholds=0.5),
        F1Score(name='f1')
    ]
)

# Define callbacks (monitor f1 for better stopping)
lr_scheduler = ReduceLROnPlateau(monitor='val_f1', factor=0.5, patience=3, verbose=1, mode='max')
early_stopping = EarlyStopping(monitor='val_f1', patience=5, restore_best_weights=True, mode='max')
checkpoint_path = '/content/drive/My Drive/AU_Detection (Emotions) Models/au_detection_model_epoch_{epoch:02d}.keras'
checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_f1', mode='max')

# Compute steps manually
steps_per_epoch = (len(train_image_paths) // batch_size) + (1 if len(train_image_paths) % batch_size else 0)
validation_steps = (len(val_image_paths) // batch_size) + (1 if len(val_image_paths) % batch_size else 0)

# Train the model
print("Starting training...")
model.fit(train_dataset, epochs=20, validation_data=val_dataset, callbacks=[lr_scheduler, early_stopping, checkpoint],
          steps_per_epoch=steps_per_epoch, validation_steps=validation_steps)

# Evaluate the model
loss, accuracy, precision, recall, f1 = model.evaluate(val_dataset)
print(f"Validation Loss: {loss}, Validation Accuracy: {accuracy}, Validation Precision: {precision}, Validation Recall: {recall}, Validation F1: {f1}")

# Save the final model
save_path = '/content/drive/My Drive/AU_Detection (Emotions) Models/au_detection_model_final.keras'
model.save(save_path)
print(f"Final model saved to {save_path}")

#Testing AU Detection on dataset
from google.cloud import storage
from google.colab import auth, drive
import os
import numpy as np
import tensorflow as tf
import pickle
from tensorflow.keras.layers import (Input, Conv2D, DepthwiseConv2D, BatchNormalization,
                                     Activation, Add, GlobalAveragePooling2D, Dense, Dropout, ZeroPadding2D,
                                     Reshape, Softmax, Multiply, Layer)
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt
import glob
import random
from sklearn.utils.class_weight import compute_class_weight

# Authenticate the user
auth.authenticate_user()

# Mount Google Drive
drive.mount('/content/drive')

# Load or collect pairs (from previous code; assume pickle exists)
pickle_path = '/content/drive/My Drive/pairs_cache.pkl'
with open(pickle_path, 'rb') as f:
    train_pairs, val_pairs = pickle.load(f)
print("Loaded pairs from cache.")

# Use the validation pairs for testing (or random subset)
test_pairs = random.sample(val_pairs, 100)

# Function to load and preprocess images (same as before)
def load_and_preprocess_image(path):
    img = tf.io.read_file(path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, [224, 224])
    img = tf.cast(img, tf.float32) / 255.0
    return img

# Define custom weighted binary cross-entropy loss with masking for -1 as a class
@tf.keras.utils.register_keras_serializable()
class WeightedBinaryCrossentropy(tf.keras.losses.Loss):
    def __init__(self, label_weights, name='weighted_binary_crossentropy'):
        super().__init__(name=name)
        # Ensure label_weights is a TensorFlow constant
        if not isinstance(label_weights, tf.Tensor):
             self.label_weights = tf.constant(label_weights, dtype=tf.float32)
        else:
             self.label_weights = label_weights


    def call(self, y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        mask = tf.greater_equal(y_true, 0.0)  # True for 0/1, False for -1
        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1.0 - tf.keras.backend.epsilon()) # Clip for stability with sigmoid output
        logits = tf.math.log(y_pred / (1.0 - y_pred)) # Convert sigmoid output to logits

        bce = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=logits)
        bce = tf.where(mask, bce, tf.zeros_like(bce))  # Zero loss for masked
        # Apply weights (tile to batch)
        batch_size = tf.shape(bce)[0]
        weights = tf.tile(tf.expand_dims(self.label_weights, 0), [batch_size, 1])
        weighted_bce = bce * weights
        # Average over valid elements
        num_valid = tf.reduce_sum(tf.cast(mask, tf.float32))
        return tf.reduce_sum(weighted_bce) / tf.maximum(num_valid, 1.0)  # Avoid div by 0

    def get_config(self):
        config = super().get_config()
        # Store weights as a list for serialization
        config.update({
            'label_weights': self.label_weights.numpy().tolist()
        })
        return config

# Custom F1 metric for multi-label (average over labels) (must match training)
@tf.keras.utils.register_keras_serializable()
class F1Score(tf.keras.metrics.Metric):
    def __init__(self, name='f1', **kwargs):
        super(F1Score, self).__init__(name=name, **kwargs)
        self.precision = tf.keras.metrics.Precision(thresholds=0.5)
        self.recall = tf.keras.metrics.Recall(thresholds=0.5)

    def update_state(self, y_true, y_pred, sample_weight=None):
        self.precision.update_state(y_true, y_pred, sample_weight)
        self.recall.update_state(y_true, y_pred, sample_weight)

    def result(self):
        p = self.precision.result()
        r = self.recall.result()
        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))

    def reset_state(self):
        self.precision.reset_state()
        self.recall.reset_state()

# Find the latest checkpoint automatically
checkpoint_dir = '/content/drive/My Drive/AU_Detection (Emotions) Models/'
checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'au_detection_model_epoch_*.keras'))
if not checkpoint_files:
    raise ValueError("No checkpoints found in the directory.")
latest_checkpoint = max(checkpoint_files, key=os.path.getctime)
print(f"Loading latest checkpoint: {latest_checkpoint}")

# For class weights, ignore -1 by filtering to 0/1 per AU (recalculate for consistent loading)
# This part is needed to correctly instantiate the loss function for loading
temp_train_labels = np.array([pair[1] for pair in train_pairs])
label_weights_list = []
for i in range(12):
    col = temp_train_labels[:, i]
    valid = col[col >= 0]  # Exclude -1
    unique = np.unique(valid)
    if len(unique) < 2:
        label_weights_list.append(1.0)
    else:
        weights = compute_class_weight('balanced', classes=unique, y=valid)
        pos_weight = weights[np.where(unique == 1)[0][0]] if 1 in unique else 1.0
        label_weights_list.append(pos_weight)
label_weights_list = np.array(label_weights_list).tolist() # Convert to list for class constructor

print("Class weights used for loading:", label_weights_list)


# Load the model from the latest checkpoint without compiling
# We don't need to provide the custom loss or metrics here
custom_objects = {
    'WeightedBinaryCrossentropy': WeightedBinaryCrossentropy,
    'F1Score': F1Score
}

model = load_model(latest_checkpoint, custom_objects=custom_objects, compile=False)

# Manually compile the model with the custom loss and metrics
model.compile(
    optimizer=tf.keras.optimizers.Adam(5e-6), # Use the same optimizer and learning rate as the last training epoch
    loss=WeightedBinaryCrossentropy(label_weights_list), # Instantiate the custom loss with weights
    metrics=[
        tf.keras.metrics.BinaryAccuracy(name='accuracy'),
        tf.keras.metrics.Recall(name='recall'),
        F1Score(name='f1')
    ]
)


# Run inference on batches of 5 images
au_labels = ['AU1', 'AU2', 'AU4', 'AU6', 'AU7', 'AU10', 'AU12', 'AU15', 'AU17', 'AU23', 'AU24', 'AU25']

# Adjust batch size for inference back to 16
inference_batch_size = 16

for i in range(0, len(test_pairs), 5):
    batch_pairs = test_pairs[i:i+5]
    images = [load_and_preprocess_image(pair[0]) for pair in batch_pairs]
    labels = [pair[1] for pair in batch_pairs]

    images = np.array(images)
    logits = model.predict(images, batch_size=inference_batch_size)
    preds = (tf.sigmoid(logits) > 0.5).numpy().astype(int)  # Binary predictions

    # Visualize
    fig, axes = plt.subplots(1, 5, figsize=(20, 4))
    for j in range(5):
        if j < len(batch_pairs):
            axes[j].imshow(images[j])
            pred_str = ', '.join([au_labels[k] for k in range(12) if preds[j][k] == 1])
            actual_str = ', '.join([au_labels[k] for k in range(12) if labels[j][k] == 1])
            axes[j].set_title(f"Pred: {pred_str}\nActual: {actual_str}", fontsize=10)
            axes[j].axis('off')
    plt.show()

#Testing AU Detection model on external images
from google.colab import auth, drive
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt
import glob
import random
from sklearn.utils.class_weight import compute_class_weight
import pickle
from tensorflow.keras.applications.mobilenet_v3 import preprocess_input
from tensorflow.keras.layers import Lambda, GlobalAveragePooling2D, Concatenate, MultiHeadAttention, BatchNormalization, Dropout, Dense, Input
from tensorflow.keras.applications import MobileNetV3Small
from tensorflow.keras import regularizers

# Authenticate the user
auth.authenticate_user()
# Mount Google Drive
drive.mount('/content/drive')
# Load or collect pairs (from previous code; assume pickle exists)
pickle_path = '/content/drive/My Drive/pairs_cache.pkl'
with open(pickle_path, 'rb') as f:
    train_pairs, val_pairs = pickle.load(f)
print("Loaded pairs from cache.")
# Define the folder with your custom images
images_folder = '/content/drive/My Drive/Z-images/'
# Get 10 images from the folder
image_files = glob.glob(os.path.join(images_folder, '*.[jJ][pP][gG]')) + glob.glob(os.path.join(images_folder, '*.[pP][nN][gG]')) + glob.glob(os.path.join(images_folder, '*.[hH][eE][iI][cC]'))
if len(image_files) < 10:
    raise ValueError("Less than 10 images found in the folder.")
test_image_paths = random.sample(image_files, 10) # Randomly select 10
# Function to load and preprocess images (same as training, without augmentations, return both preprocessed and display versions)
def load_and_preprocess_image(path):
    img = tf.io.read_file(path)
    img = tf.image.decode_image(img, channels=3)
    display_img = tf.cast(tf.image.resize(img, [224, 224]), tf.float32) / 255.0  # Ensure float32 [0,1] for display
    preprocessed_img = tf.cast(img, tf.float32)
    preprocessed_img = tf.image.resize(preprocessed_img, [224, 224])
    preprocessed_img = preprocess_input(preprocessed_img)  # Apply MobileNetV3 preprocessing
    return preprocessed_img, display_img
# For class weights, ignore -1 by filtering to 0/1 per AU (recalculate for consistent loading)
# This part is needed to correctly instantiate the loss function for loading
temp_train_labels = np.array([pair[1] for pair in train_pairs])
label_weights_list = []
for i in range(12):
    col = temp_train_labels[:, i]
    valid = col[col >= 0] # Exclude -1
    unique = np.unique(valid)
    if len(unique) < 2:
        label_weights_list.append(1.0)
    else:
        weights = compute_class_weight('balanced', classes=unique, y=valid)
        pos_weight = weights[np.where(unique == 1)[0][0]] if 1 in unique else 1.0
        label_weights_list.append(pos_weight)
label_weights = np.clip(np.array(label_weights_list), 0.5, 10.0)
label_weights = tf.constant(label_weights, dtype=tf.float32)
print("Class weights used for loading:", label_weights.numpy())
# Define custom focal loss for multi-label with per-class alpha (integrated weights)
@tf.keras.utils.register_keras_serializable()
def focal_loss(gamma=1.0, alpha=label_weights * 0.75):
    def focal_loss_fn(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        epsilon = 0.1
        num_classes = 2.0  # Binary per AU
        y_true_masked = tf.where(tf.greater_equal(y_true, 0.0), y_true * (1 - epsilon) + (epsilon / num_classes), tf.zeros_like(y_true))
        mask = tf.greater_equal(y_true, 0.0) # Mask -1
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1. - 1e-7) # Stricter clipping
        ce = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true_masked, logits=y_pred)
        pt = tf.exp(-ce) + 1e-8 # Epsilon to avoid pt>1
        alpha_t = y_true_masked * alpha + (1 - y_true_masked) * (1 - alpha / tf.reduce_max(alpha)) # Normalize alpha to [0,1]
        focal_ce = alpha_t * (1 - pt) ** gamma * ce
        focal_ce = tf.where(mask, focal_ce, tf.zeros_like(focal_ce)) # Apply mask to loss
        return tf.reduce_mean(focal_ce)
    focal_loss_fn.__name__ = 'focal_loss_fn' # Assign a name
    return focal_loss_fn
# Custom F1 metric for multi-label (average over labels, lowered threshold to 0.2)
@tf.keras.utils.register_keras_serializable()
class F1Score(tf.keras.metrics.Metric):
    def __init__(self, name='f1', **kwargs):
        super(F1Score, self).__init__(name=name, **kwargs)
        self.precision = tf.keras.metrics.Precision(thresholds=0.2) # Adjusted threshold
        self.recall = tf.keras.metrics.Recall(thresholds=0.2)
    def update_state(self, y_true, y_pred, sample_weight=None):
        self.precision.update_state(y_true, y_pred, sample_weight)
        self.recall.update_state(y_true, y_pred, sample_weight)
    def result(self):
        p = self.precision.result()
        r = self.recall.result()
        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))
    def reset_state(self):
        self.precision.reset_state()
        self.recall.reset_state()
# Custom Layer to expand dimensions
@tf.keras.utils.register_keras_serializable()
class ExpandDimsLayer(tf.keras.layers.Layer):
    def __init__(self, axis=-1, **kwargs):
        super(ExpandDimsLayer, self).__init__(**kwargs)
        self.axis = axis

    def call(self, inputs):
        return tf.expand_dims(inputs, axis=self.axis)

    def get_config(self):
        config = super(ExpandDimsLayer, self).get_config()
        config.update({'axis': self.axis})
        return config
# Custom Layer to squeeze dimensions
@tf.keras.utils.register_keras_serializable()
class SqueezeLayer(tf.keras.layers.Layer):
    def __init__(self, axis, **kwargs):
        super(SqueezeLayer, self).__init__(**kwargs)
        self.axis = axis
    def call(self, inputs):
        return tf.squeeze(inputs, axis=self.axis)
    def get_config(self):
        config = super(SqueezeLayer, self).get_config()
        config.update({'axis': self.axis})
        return config
# Find the latest checkpoint automatically (search both formats)
checkpoint_dir = '/content/drive/My Drive/AU_Detection (Emotions) Models/'
checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'au_detection_model_epoch_*.keras')) + glob.glob(os.path.join(checkpoint_dir, 'au_detection_model_epoch_*.weights.h5'))
if not checkpoint_files:
    raise ValueError("No checkpoints found in the directory.")
latest_checkpoint = max(checkpoint_files, key=os.path.getctime)
print(f"Loading latest checkpoint: {latest_checkpoint}")
# Since checkpoints may be weights-only or full model, build the model architecture first
input_img = Input(shape=(224, 224, 3)) # Explicit input for multi-branch
# Multi-branch regions: Crop input image into eyes (top ~1/3), mouth (bottom ~1/3), full face using Lambda for symbolic compatibility
eyes_crop = Lambda(lambda x: tf.slice(x, [0, 0, 0, 0], [-1, 74, 224, 3]), output_shape=(74, 224, 3))(input_img) # Top ~1/3 (224/3 ≈74)
eyes_branch_model = MobileNetV3Small(weights=None, include_top=False, input_shape=(74, 224, 3), name='eyes_mobilenet')
eyes_branch_output = eyes_branch_model(eyes_crop)
eyes_branch = GlobalAveragePooling2D()(eyes_branch_output)
mouth_crop = Lambda(lambda x: tf.slice(x, [0, 150, 0, 0], [-1, 74, 224, 3]), output_shape=(74, 224, 3))(input_img) # Bottom ~1/3 (starting from ~2/3: 224*2/3≈150)
mouth_branch_model = MobileNetV3Small(weights=None, include_top=False, input_shape=(74, 224, 3), name='mouth_mobilenet')
mouth_branch_output = mouth_branch_model(mouth_crop)
mouth_branch = GlobalAveragePooling2D()(mouth_branch_output)
full_branch_model = MobileNetV3Small(weights='imagenet', include_top=False, input_tensor=input_img, name='full_mobilenet') # Full face
full_branch_output = full_branch_model.output # Access output correctly
full_branch = GlobalAveragePooling2D()(full_branch_output)
# Fuse branches
fused = Concatenate()([eyes_branch, mouth_branch, full_branch])
# Attention/Relation Module: Add self-attention to model AU correlations
fused_with_seq = ExpandDimsLayer(axis=1)(fused) # Add seq dim using custom layer
attention = MultiHeadAttention(num_heads=4, key_dim=128)(fused_with_seq, fused_with_seq) # Self-attention
attention = SqueezeLayer(axis=1)(attention) # Remove seq dim using custom layer
# Add custom layers on top of attention output
x = BatchNormalization()(attention)
x = Dropout(0.5)(x)
# Output Head: 12 binary outputs (logits for focal loss)
logits = Dense(12, activation=None, dtype='float32',
               kernel_regularizer=regularizers.l2(1e-4))(x) # 12 AUs
model = tf.keras.Model(inputs=input_img, outputs=logits) # Use explicit input
# Load the weights (compatible with both .keras and .weights.h5)
model.load_weights(latest_checkpoint)
# Manually compile the model with the custom loss and metrics
model.compile(
    optimizer=tf.keras.optimizers.AdamW(learning_rate=5e-6, weight_decay=0.01), # Match training optimizer style
    loss=focal_loss(),
    metrics=[
        tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=0.2), # Adjusted threshold
        tf.keras.metrics.Precision(name='precision', thresholds=0.2),
        tf.keras.metrics.Recall(name='recall', thresholds=0.2),
        F1Score(name='f1') # Custom F1 with threshold 0.2
    ]
)
print("Model loaded and compiled successfully.")
# AU labels (standard for ABAW)
au_labels = ['AU1', 'AU2', 'AU4', 'AU6', 'AU7', 'AU10', 'AU12', 'AU15', 'AU17', 'AU23', 'AU24', 'AU25']
# Define emotions and their expected active AUs with weights (0-based indices: core at 1.0, optional at 0.5)
emotions = {
    "Happiness": {3: 1.0, 6: 1.0, 11: 0.5},  # Core: AU6+AU12 (cheek raise + lip pull; research prototype). Optional: AU25 (open mouth for joy; data patterns).
    "Sadness": {0: 1.0, 2: 1.0, 7: 1.0, 8: 0.5},  # Core: AU1+AU4+AU15 (inner brow + lower + lip depress; research). Optional: AU17 (chin raise; trimmed to balance FP).
    "Surprised": {0: 1.0, 1: 1.0, 11: 1.0, 10: 0.3},  # Core: AU1+AU2+AU25 (brows raise + lips part; sub for AU5/26). Optional: AU24 low (mouth press; data tension without over-bias).
    "Fear": {0: 1.0, 1: 1.0, 2: 1.0, 4: 1.0, 11: 0.3},  # Core: AU1+AU2+AU4+AU7 (brows mixed + lid tight; research). Optional: AU25 low (open; balanced to match surprise).
    "Anger": {2: 1.0, 4: 1.0, 9: 1.0, 10: 0.5},  # Core: AU4+AU7+AU23 (brow lower + lid tight + lip tight; research). Optional: AU24 (press; no expansion).
    "Disgust": {5: 1.0, 7: 1.0, 8: 0.5, 9: 0.5},  # Core: AU10+AU15 (upper lip raise + corner depress; sub for AU9/16). Optional: AU17+AU23 (chin + tight; data fits without bloat).
    "Neutral": {},  # No AUs; unchanged (penalizes activations equally).
    "Other": {6: 0.5, 9: 0.5, 10: 0.5, 5: 0.3},  # Low for blends/contempt: AU12+AU23+AU24 (smirk + tight/press; research unilateral sub). Optional: AU10 (data mixes).
}
# Function to compute scores for each emotion (adjusted for weights, with normalization)
def compute_scores(au_scores, emotions):
    O = set([i for i, score in enumerate(au_scores) if score == 1])
    scores = {}
    for emotion, au_dict in emotions.items():
        if not au_dict:  # For Neutral
            score = -len(O)  # Penalize any active AUs
            total_weights = 1.0  # Minimal denominator for neutral
        else:
            TP = sum(au_dict.get(au, 0) for au in O if au in au_dict)
            FP = len(O - set(au_dict.keys()))
            FN = sum(au_dict.get(au, 0) for au in set(au_dict.keys()) - O)
            score = TP - FP - FN
            total_weights = sum(au_dict.values())  # Max possible TP
        # Normalize to balance dict size/weight differences (divide by total weights + num keys)
        normalized_score = score / (total_weights + len(au_dict)) if au_dict else score
        scores[emotion] = normalized_score
    return scores
# Function to compute probabilities using softmax
def compute_probabilities(scores):
    values = np.array(list(scores.values()))
    exp_values = np.exp(values - np.max(values)) # Numerical stability
    probs = exp_values / np.sum(exp_values)
    return {emotion: prob for emotion, prob in zip(scores.keys(), probs)}
# Run inference on the 10 images
loaded_images = [load_and_preprocess_image(path) for path in test_image_paths]
images_preprocessed, images_display = zip(*loaded_images)
images_preprocessed = np.array(images_preprocessed)
images_display = np.array(images_display)
logits = model.predict(images_preprocessed)
sigmoid_probs = tf.sigmoid(logits).numpy()  # Get probabilities
# Print debug info: sigmoid probs for each image (only show AUs >0.1 for brevity)
for idx in range(10):
    high_probs = {au_labels[i]: round(float(sigmoid_probs[idx][i]), 3) for i in range(12) if sigmoid_probs[idx][i] > 0.1}
    print(f"Image {idx} ({os.path.basename(test_image_paths[idx])}): Sigmoid probs (>0.1): {high_probs or 'All <0.1'}")
# Binary predictions with lowered threshold for potential fix
threshold = 0.3  # As you mentioned; adjust as needed
preds = (sigmoid_probs > threshold).astype(int)
# Visualize 5 at a time
for i in range(0, 10, 5):
    fig, axes = plt.subplots(1, 5, figsize=(20, 8))
    for j in range(5):
        idx = i + j
        if idx < 10:
            au_scores = preds[idx]
            scores_dict = compute_scores(au_scores, emotions)
            probs = compute_probabilities(scores_dict)
            predicted_emotion = max(probs, key=probs.get)
            top_3 = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:3]
            top_3_str = ", ".join([f"{emotion}: {prob:.2f}" for emotion, prob in top_3])
            au_labels_str = ", ".join([au_labels[k] for k in range(12) if au_scores[k] == 1])

            rotated_image = np.rot90(images_display[idx], k=3)
            axes[j].imshow(rotated_image)
            axes[j].set_title(predicted_emotion)
            axes[j].axis('off')
            # Add AU labels below the image
            axes[j].text(0.5, -0.15, au_labels_str, ha='center', va='top', transform=axes[j].transAxes, fontsize=8)
            # Add top 3 emotions below the AU labels
            axes[j].text(0.5, -0.25, f"Top 3: {top_3_str}", ha='center', va='top', transform=axes[j].transAxes, fontsize=8)
    # Adjust layout to make room for labels
    plt.subplots_adjust(bottom=0.4)
    plt.show()

#More tests on my facial data
from google.colab import auth, drive
import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
import matplotlib.pyplot as plt
import glob
import random
from sklearn.utils.class_weight import compute_class_weight
import pickle
from tensorflow.keras.applications.mobilenet_v3 import preprocess_input
from tensorflow.keras.layers import Lambda, GlobalAveragePooling2D, Concatenate, MultiHeadAttention, BatchNormalization, Dropout, Dense, Input
from tensorflow.keras.applications import MobileNetV3Small
from tensorflow.keras import regularizers

# Authenticate the user
auth.authenticate_user()
# Mount Google Drive
drive.mount('/content/drive')
# Load or collect pairs (from previous code; assume pickle exists)
pickle_path = '/content/drive/My Drive/pairs_cache.pkl'
with open(pickle_path, 'rb') as f:
    train_pairs, val_pairs = pickle.load(f)
print("Loaded pairs from cache.")
# Sample 10 random images from the validation set for variety (replace Z-images)
# if len(val_pairs) < 10:
#     raise ValueError("Less than 10 pairs in validation set.")
# sample_pairs = random.sample(val_pairs, 10)
# test_image_paths = [pair[0] for pair in sample_pairs]  # gs:// paths from bucket

images_folder = '/content/drive/My Drive/Z-images/'
# Get 10 images from the folder
image_files = glob.glob(os.path.join(images_folder, '*.[jJ][pP][gG]')) + glob.glob(os.path.join(images_folder, '*.[pP][nN][gG]')) + glob.glob(os.path.join(images_folder, '*.[hH][eE][iI][cC]'))
if len(image_files) < 10:
    raise ValueError("Less than 10 images found in the folder.")
test_image_paths = random.sample(image_files, 10) # Randomly select 10
# Function to load and preprocess images (same as training, without augmentations, return both preprocessed and display versions)
def load_and_preprocess_image(path):
    img = tf.io.read_file(path)
    img = tf.image.decode_image(img, channels=3)
    display_img = tf.cast(tf.image.resize(img, [224, 224]), tf.float32) / 255.0  # Ensure float32 [0,1] for display
    preprocessed_img = tf.cast(img, tf.float32)
    preprocessed_img = tf.image.resize(preprocessed_img, [224, 224])
    preprocessed_img = preprocess_input(preprocessed_img)  # Apply MobileNetV3 preprocessing
    return preprocessed_img, display_img
# For class weights, ignore -1 by filtering to 0/1 per AU (recalculate for consistent loading)
# This part is needed to correctly instantiate the loss function for loading
temp_train_labels = np.array([pair[1] for pair in train_pairs])
label_weights_list = []
for i in range(12):
    col = temp_train_labels[:, i]
    valid = col[col >= 0] # Exclude -1
    unique = np.unique(valid)
    if len(unique) < 2:
        label_weights_list.append(1.0)
    else:
        weights = compute_class_weight('balanced', classes=unique, y=valid)
        pos_weight = weights[np.where(unique == 1)[0][0]] if 1 in unique else 1.0
        label_weights_list.append(pos_weight)
label_weights = np.clip(np.array(label_weights_list), 0.5, 10.0)
label_weights = tf.constant(label_weights, dtype=tf.float32)
print("Class weights used for loading:", label_weights.numpy())
# Define custom focal loss for multi-label with per-class alpha (integrated weights)
@tf.keras.utils.register_keras_serializable()
def focal_loss(gamma=1.0, alpha=label_weights * 0.75):
    def focal_loss_fn(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)
        epsilon = 0.1
        num_classes = 2.0  # Binary per AU
        y_true_masked = tf.where(tf.greater_equal(y_true, 0.0), y_true * (1 - epsilon) + (epsilon / num_classes), tf.zeros_like(y_true))
        mask = tf.greater_equal(y_true, 0.0) # Mask -1
        y_pred = tf.clip_by_value(y_pred, 1e-7, 1. - 1e-7) # Stricter clipping
        ce = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true_masked, logits=y_pred)
        pt = tf.exp(-ce) + 1e-8 # Epsilon to avoid pt>1
        alpha_t = y_true_masked * alpha + (1 - y_true_masked) * (1 - alpha / tf.reduce_max(alpha)) # Normalize alpha to [0,1]
        focal_ce = alpha_t * (1 - pt) ** gamma * ce
        focal_ce = tf.where(mask, focal_ce, tf.zeros_like(focal_ce)) # Apply mask to loss
        return tf.reduce_mean(focal_ce)
    focal_loss_fn.__name__ = 'focal_loss_fn' # Assign a name
    return focal_loss_fn
# Custom F1 metric for multi-label (average over labels, lowered threshold to 0.2)
@tf.keras.utils.register_keras_serializable()
class F1Score(tf.keras.metrics.Metric):
    def __init__(self, name='f1', **kwargs):
        super(F1Score, self).__init__(name=name, **kwargs)
        self.precision = tf.keras.metrics.Precision(thresholds=0.2) # Adjusted threshold
        self.recall = tf.keras.metrics.Recall(thresholds=0.2)
    def update_state(self, y_true, y_pred, sample_weight=None):
        self.precision.update_state(y_true, y_pred, sample_weight)
        self.recall.update_state(y_true, y_pred, sample_weight)
    def result(self):
        p = self.precision.result()
        r = self.recall.result()
        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))
    def reset_state(self):
        self.precision.reset_state()
        self.recall.reset_state()
# Custom Layer to expand dimensions
@tf.keras.utils.register_keras_serializable()
class ExpandDimsLayer(tf.keras.layers.Layer):
    def __init__(self, axis=-1, **kwargs):
        super(ExpandDimsLayer, self).__init__(**kwargs)
        self.axis = axis

    def call(self, inputs):
        return tf.expand_dims(inputs, axis=self.axis)

    def get_config(self):
        config = super(ExpandDimsLayer, self).get_config()
        config.update({'axis': self.axis})
        return config
# Custom Layer to squeeze dimensions
@tf.keras.utils.register_keras_serializable()
class SqueezeLayer(tf.keras.layers.Layer):
    def __init__(self, axis, **kwargs):
        super(SqueezeLayer, self).__init__(**kwargs)
        self.axis = axis
    def call(self, inputs):
        return tf.squeeze(inputs, axis=self.axis)
    def get_config(self):
        config = super(SqueezeLayer, self).get_config()
        config.update({'axis': self.axis})
        return config
# Find the latest checkpoint automatically (search both formats)
checkpoint_dir = '/content/drive/My Drive/AU_Detection (Emotions) Models/'
checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'au_detection_model_epoch_*.keras')) + glob.glob(os.path.join(checkpoint_dir, 'au_detection_model_epoch_*.weights.h5'))
if not checkpoint_files:
    raise ValueError("No checkpoints found in the directory.")
latest_checkpoint = max(checkpoint_files, key=os.path.getctime)
print(f"Loading latest checkpoint: {latest_checkpoint}")
# Since checkpoints may be weights-only or full model, build the model architecture first
input_img = Input(shape=(224, 224, 3)) # Explicit input for multi-branch
# Multi-branch regions: Crop input image into eyes (top ~1/3), mouth (bottom ~1/3), full face using Lambda for symbolic compatibility
eyes_crop = Lambda(lambda x: tf.slice(x, [0, 0, 0, 0], [-1, 74, 224, 3]), output_shape=(74, 224, 3))(input_img) # Top ~1/3 (224/3 ≈74)
eyes_branch_model = MobileNetV3Small(weights=None, include_top=False, input_shape=(74, 224, 3), name='eyes_mobilenet')
eyes_branch_output = eyes_branch_model(eyes_crop)
eyes_branch = GlobalAveragePooling2D()(eyes_branch_output)
mouth_crop = Lambda(lambda x: tf.slice(x, [0, 150, 0, 0], [-1, 74, 224, 3]), output_shape=(74, 224, 3))(input_img) # Bottom ~1/3 (starting from ~2/3: 224*2/3≈150)
mouth_branch_model = MobileNetV3Small(weights=None, include_top=False, input_shape=(74, 224, 3), name='mouth_mobilenet')
mouth_branch_output = mouth_branch_model(mouth_crop)
mouth_branch = GlobalAveragePooling2D()(mouth_branch_output)
full_branch_model = MobileNetV3Small(weights='imagenet', include_top=False, input_tensor=input_img, name='full_mobilenet') # Full face
full_branch_output = full_branch_model.output # Access output correctly
full_branch = GlobalAveragePooling2D()(full_branch_output)
# Fuse branches
fused = Concatenate()([eyes_branch, mouth_branch, full_branch])
# Attention/Relation Module: Add self-attention to model AU correlations
fused_with_seq = ExpandDimsLayer(axis=1)(fused) # Add seq dim using custom layer
attention = MultiHeadAttention(num_heads=4, key_dim=128)(fused_with_seq, fused_with_seq) # Self-attention
attention = SqueezeLayer(axis=1)(attention) # Remove seq dim using custom layer
# Add custom layers on top of attention output
x = BatchNormalization()(attention)
x = Dropout(0.5)(x)
# Output Head: 12 binary outputs (logits for focal loss)
logits = Dense(12, activation=None, dtype='float32',
               kernel_regularizer=regularizers.l2(1e-4))(x) # 12 AUs
model = tf.keras.Model(inputs=input_img, outputs=logits) # Use explicit input
# Load the weights (compatible with both .keras and .weights.h5)
model.load_weights(latest_checkpoint)
# Manually compile the model with the custom loss and metrics
model.compile(
    optimizer=tf.keras.optimizers.AdamW(learning_rate=5e-6, weight_decay=0.01), # Match training optimizer style
    loss=focal_loss(),
    metrics=[
        tf.keras.metrics.BinaryAccuracy(name='accuracy', threshold=0.2), # Adjusted threshold
        tf.keras.metrics.Precision(name='precision', thresholds=0.2),
        tf.keras.metrics.Recall(name='recall', thresholds=0.2),
        F1Score(name='f1') # Custom F1 with threshold 0.2
    ]
)
print("Model loaded and compiled successfully.")
# AU labels (standard for ABAW)
au_labels = ['AU1', 'AU2', 'AU4', 'AU6', 'AU7', 'AU10', 'AU12', 'AU15', 'AU17', 'AU23', 'AU24', 'AU25']
# Define emotions and their expected active AUs with weights (0-based indices: core at 1.0, optional at 0.5)
#V1
emotions = {
    "Happiness": {3: 1.0, 6: 1.0, 5: 0.5, 10: 0.5, 11: 0.4},  # Core: AU6+AU12 (research). Optional: AU10 (data upper lip), AU24 (data press), AU25 (research/data open).
    "Sadness": {0: 1.0, 2: 1.0, 7: 1.0, 8: 0.5},  # Core: AU1+AU4+AU15 (research). Optional: AU17 (research chin; no data, but standard).
    "Surprised": {0: 1.0, 1: 1.0, 11: 1.0, 4: 0.3, 8: 0.3},  # Core: AU1+AU2+AU25 (research, sub 5/26). Optional: AU7 (data lid), AU17 (data chin).
    "Fear": {0: 1.0, 1: 1.0, 2: 1.0, 4: 1.0, 11: 0.4},  # Core: AU1+AU2+AU4+AU7 (research, sub 5/20). Optional: AU25 (research open; no data).
    "Anger": {2: 1.0, 4: 1.0, 9: 1.0, 10: 0.5, 8: 0.4},  # Core: AU4+AU7+AU23 (research). Optional: AU24 (research/data press), AU17 (data chin).
    "Disgust": {5: 1.0, 7: 1.0, 9: 0.5, 8: 0.5, 10: 0.4},  # Core: AU10+AU15 (research, sub 9/16). Optional: AU23 (data tight), AU17 (research/data chin), AU24 (data press).
    "Neutral": {},  # No AUs (research).
    "Other": {6: 0.5, 9: 0.5, 5: 0.4, 10: 0.4},  # Low for blends/contempt: AU12+AU23 (research unilateral + tight). Optional: AU10 (data), AU24 (data press).
}
#V2
# emotions = {
#     "Happiness": {3: 1.0, 6: 1.0, 4: 0.5, 5: 0.5, 10: 0.5, 11: 0.4},  # Core: AU6+AU12. Optional: AU7 (research squint), AU10 (data/  joy), AU24 (data press), AU25 (open).
#     "Sadness": {0: 1.0, 2: 1.0, 7: 1.0, 3: 0.3, 8: 0.5},  # Core: AU1+AU4+AU15. Optional: AU6 low (data blend), AU17 (chin).
#     "Surprised": {0: 1.0, 1: 1.0, 11: 1.0, 2: 0.3, 4: 0.3},  # Core: AU1+AU2+AU25 (sub 5/26). Optional: AU4/7 low ( wide eyes).
#     "Fear": {0: 1.0, 1: 1.0, 2: 1.0, 4: 1.0, 11: 0.4},  # Core: AU1+AU2+AU4+AU7 (sub 5/20). Optional: AU25 ( stress).
#     "Anger": {2: 1.0, 4: 1.0, 9: 1.0, 5: 0.4, 10: 0.5, 8: 0.4},  # Core: AU4+AU7+AU23. Optional: AU10 (), AU24 (press), AU17 (data chin).
#     "Disgust": {5: 1.0, 7: 1.0, 3: 0.3, 9: 0.5, 8: 0.5, 10: 0.4},  # Core: AU10+AU15 (sub 9/16). Optional: AU6 low (data), AU23/17/24 (data/research ).
#     "Neutral": {},  # No AUs.
#     "Other": {6: 0.5, 9: 0.5, 5: 0.4, 10: 0.4, 7: 0.3},  # Blends: AU12+AU23 (contempt), AU10/24 (data), AU15 low ( blends).
# }
# Function to compute scores for each emotion (adjusted for weights, with normalization)
def compute_scores(au_scores, emotions):
    O = set([i for i, score in enumerate(au_scores) if score == 1])
    scores = {}
    for emotion, au_dict in emotions.items():
        if not au_dict:  # For Neutral
            score = -len(O)  # Penalize any active AUs
            total_weights = 1.0  # Minimal denominator for neutral
        else:
            TP = sum(au_dict.get(au, 0) for au in O if au in au_dict)
            FP = len(O - set(au_dict.keys()))
            FN = sum(au_dict.get(au, 0) for au in set(au_dict.keys()) - O)
            score = TP - FP - FN
            total_weights = sum(au_dict.values())  # Max possible TP
        # Normalize to balance dict size/weight differences (divide by total weights + num keys)
        normalized_score = score / (total_weights + len(au_dict)) if au_dict else score
        scores[emotion] = normalized_score
    return scores
# Function to compute probabilities using softmax
def compute_probabilities(scores):
    values = np.array(list(scores.values()))
    exp_values = np.exp(values - np.max(values)) # Numerical stability
    probs = exp_values / np.sum(exp_values)
    return {emotion: prob for emotion, prob in zip(scores.keys(), probs)}
# Run inference on the 10 images
loaded_images = [load_and_preprocess_image(path) for path in test_image_paths]
images_preprocessed, images_display = zip(*loaded_images)
images_preprocessed = np.array(images_preprocessed)
images_display = np.array(images_display)
logits = model.predict(images_preprocessed)
sigmoid_probs = tf.sigmoid(logits).numpy()  # Get probabilities
# Print debug info: sigmoid probs for each image (only show AUs >0.1 for brevity)
# Binary predictions with lowered threshold for potential fix
threshold = 0.2 # As you mentioned; adjust as needed
preds = (sigmoid_probs > threshold).astype(int)

for idx in range(10):
    au_scores = preds[idx]
    scores_dict = compute_scores(au_scores, emotions)
    probs = compute_probabilities(scores_dict)
    predicted_emotion = max(probs, key=probs.get)
    au_probs_dict = {au_labels[i]: round(float(sigmoid_probs[idx][i]), 3) for i in range(12)}
    print(f"Image {idx} ({os.path.basename(test_image_paths[idx])}): Predicted emotion: {predicted_emotion}, AU probabilities: {au_probs_dict}")

# Visualize 5 at a time
for i in range(0, 10, 5):
    fig, axes = plt.subplots(1, 5, figsize=(20, 8))
    for j in range(5):
        idx = i + j
        if idx < 10:
            au_scores = preds[idx]
            scores_dict = compute_scores(au_scores, emotions)
            probs = compute_probabilities(scores_dict)
            predicted_emotion = max(probs, key=probs.get)
            top_3 = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:3]
            top_3_str = ", ".join([f"{emotion}: {prob:.2f}" for emotion, prob in top_3])
            au_labels_str = ", ".join([au_labels[k] for k in range(12) if au_scores[k] == 1])

            rotated_image = np.rot90(images_display[idx], k=3)
            axes[j].imshow(rotated_image)
            axes[j].set_title(predicted_emotion)
            axes[j].axis('off')
            # Add AU labels below the image
            axes[j].text(0.5, -0.15, au_labels_str, ha='center', va='top', transform=axes[j].transAxes, fontsize=8)
            # Add top 3 emotions below the AU labels
            axes[j].text(0.5, -0.25, f"Top 3: {top_3_str}", ha='center', va='top', transform=axes[j].transAxes, fontsize=8)
    # Adjust layout to make room for labels
    plt.subplots_adjust(bottom=0.4)

    plt.show()

#Testing Thresholds
threshold = 0.45
preds = (sigmoid_probs > threshold).astype(int)
for idx in range(10):
    au_scores = preds[idx]
    scores_dict = compute_scores(au_scores, emotions)
    probs = compute_probabilities(scores_dict)
    predicted_emotion = max(probs, key=probs.get)
    au_probs_dict = {au_labels[i]: round(float(sigmoid_probs[idx][i]), 3) for i in range(12)}
    print(f"Image {idx} ({os.path.basename(test_image_paths[idx])}): Predicted emotion: {predicted_emotion}, AU probabilities: {au_probs_dict}")
# Binary predictions with lowered threshold for potential fix

# Visualize 5 at a time
for i in range(0, 10, 5):
    fig, axes = plt.subplots(1, 5, figsize=(20, 8))
    for j in range(5):
        idx = i + j
        if idx < 10:
            au_scores = preds[idx]
            scores_dict = compute_scores(au_scores, emotions)
            probs = compute_probabilities(scores_dict)
            predicted_emotion = max(probs, key=probs.get)
            top_3 = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:3]
            top_3_str = ", ".join([f"{emotion}: {prob:.2f}" for emotion, prob in top_3])
            au_labels_str = ", ".join([au_labels[k] for k in range(12) if au_scores[k] == 1])

            rotated_image = np.rot90(images_display[idx], k=3)
            axes[j].imshow(rotated_image)
            axes[j].set_title(predicted_emotion)
            axes[j].axis('off')
            # Add AU labels below the image
            axes[j].text(0.5, -0.15, au_labels_str, ha='center', va='top', transform=axes[j].transAxes, fontsize=8)
            # Add top 3 emotions below the AU labels
            axes[j].text(0.5, -0.25, f"Top 3: {top_3_str}", ha='center', va='top', transform=axes[j].transAxes, fontsize=8)
    # Adjust layout to make room for labels
    plt.subplots_adjust(bottom=0.4)
    plt.show()

# Sample 50 random images from the validation set for variety (replace Z-images)
if len(val_pairs) < 50:
    raise ValueError("Less than 50 pairs in validation set.")
sample_pairs = random.sample(val_pairs, 50)
test_image_paths = [pair[0] for pair in sample_pairs]  # gs:// paths from bucket

# Run inference on the 50 images
loaded_images = [load_and_preprocess_image(path) for path in test_image_paths]
images_preprocessed, images_display = zip(*loaded_images)
images_preprocessed = np.array(images_preprocessed)
images_display = np.array(images_display)
logits = model.predict(images_preprocessed)
sigmoid_probs = tf.sigmoid(logits).numpy()  # Get probabilities
# Binary predictions with lowered threshold for potential fix
threshold = 0.4
preds = (sigmoid_probs > threshold).astype(int)
# Print predicted emotions with full AU probabilities
for idx in range(50):
    au_scores = preds[idx]
    scores_dict = compute_scores(au_scores, emotions)
    probs = compute_probabilities(scores_dict)
    predicted_emotion = max(probs, key=probs.get)
    au_probs_dict = {au_labels[i]: round(float(sigmoid_probs[idx][i]), 3) for i in range(12)}
    print(f"Image {idx} ({os.path.basename(test_image_paths[idx])}): Predicted emotion: {predicted_emotion}, AU probabilities: {au_probs_dict}")
# Visualize 5 at a time
for i in range(0, 50, 5):
    fig, axes = plt.subplots(1, 5, figsize=(20, 8))
    for j in range(5):
        idx = i + j
        if idx < 50:
            au_scores = preds[idx]
            scores_dict = compute_scores(au_scores, emotions)
            probs = compute_probabilities(scores_dict)
            predicted_emotion = max(probs, key=probs.get)
            top_3 = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:3]
            top_3_str = ", ".join([f"{emotion}: {prob:.2f}" for emotion, prob in top_3])
            au_labels_str = ", ".join([au_labels[k] for k in range(12) if au_scores[k] == 1])

            rotated_image = np.rot90(images_display[idx], k=0)
            axes[j].imshow(rotated_image)
            axes[j].set_title(predicted_emotion)
            axes[j].axis('off')
            # Add AU labels below the image
            axes[j].text(0.5, -0.15, au_labels_str, ha='center', va='top', transform=axes[j].transAxes, fontsize=8)
            # Add top 3 emotions below the AU labels
            axes[j].text(0.5, -0.25, f"Top 3: {top_3_str}", ha='center', va='top', transform=axes[j].transAxes, fontsize=8)
    # Adjust layout to make room for labels
    plt.subplots_adjust(bottom=0.4)
    plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir ./logs

#Create Dummy Data and RECACHE data for multi-modal training!
import os
from google.cloud import storage
from google.colab import auth, drive
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import (GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Input, Concatenate, MultiHeadAttention, RandomRotation, RandomCrop, RandomZoom)
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
import random
import pickle
from sklearn.utils.class_weight import compute_class_weight
import glob
from tensorflow.keras import regularizers
from tqdm import tqdm
from tensorflow.keras.applications import EfficientNetV2S
from tensorflow.keras.applications.efficientnet_v2 import preprocess_input
from tensorflow.keras import ops

import logging
logging.basicConfig(level=logging.DEBUG)

# Authenticate and mount (unchanged)
auth.authenticate_user()
drive.mount('/content/drive')
bucket_name = 'facial-expressions_bucket'
data_prefixes = [
    'Faces Dataset/batch1/',
    'Faces Dataset/batch2/',
    'Faces Dataset/cropped_new_50_vids/'
]
train_annotation_prefix = 'Faces Dataset/ABAW Annotations/EXPR_Recognition_Challenge/Train_Set/'
val_annotation_prefix = 'Faces Dataset/ABAW Annotations/EXPR_Recognition_Challenge/Validation_Set/'
train_va_prefix = 'Faces Dataset/ABAW Annotations/VA_Estimation_Challenge/Train_Set/'
val_va_prefix = 'Faces Dataset/ABAW Annotations/VA_Estimation_Challenge/Validation_Set/'
train_au_prefix = 'Faces Dataset/ABAW Annotations/AU_Detection_Challenge/Train_Set/'
val_au_prefix = 'Faces Dataset/ABAW Annotations/AU_Detection_Challenge/Validation_Set/'
client = storage.Client()
bucket = client.get_bucket(bucket_name)

# Functions for listing folders and getting pairs (updated to include EVERY possible with dummies for missing)
def get_dataset_pairs(bucket, expr_prefix, va_prefix, au_prefix, data_prefixes, is_train=True):
    pairs = []
    included_folders = 0
    skipped_base_names = 0
    has_negative_one = False
    # List all annotation files for each task
    expr_blobs = [blob for blob in bucket.list_blobs(prefix=expr_prefix) if blob.name.endswith('.txt')]
    va_blobs = [blob for blob in bucket.list_blobs(prefix=va_prefix) if blob.name.endswith('.txt')]
    au_blobs = [blob for blob in bucket.list_blobs(prefix=au_prefix) if blob.name.endswith('.txt')]
    print(f"Found {len(expr_blobs)} EXPR, {len(va_blobs)} VA, {len(au_blobs)} AU annotation files.")
    # Create maps from normalized base_name to full file paths
    def normalize_base_name(name):
        name = name.replace('.txt', '')
        if name.startswith('video'):
            name = name.replace('video', '')
        # Remove dimensions or suffixes after '-'
        if '-' in name:
            name = name.split('-')[0]
        return name.strip()
    expr_map = {normalize_base_name(os.path.basename(blob.name)): blob.name for blob in expr_blobs}
    va_map = {normalize_base_name(os.path.basename(blob.name)): blob.name for blob in va_blobs}
    au_map = {normalize_base_name(os.path.basename(blob.name)): blob.name for blob in au_blobs}
    # Find all unique normalized base_names from any task
    all_norm_base_names = set(expr_map.keys()) | set(va_map.keys()) | set(au_map.keys())
    print(f"Found {len(all_norm_base_names)} unique normalized video base_names across tasks.")
    for norm_base_name in sorted(all_norm_base_names):
        expr_anno_file = expr_map.get(norm_base_name)
        va_anno_file = va_map.get(norm_base_name)
        au_anno_file = au_map.get(norm_base_name)
        # Use the longest base_name for image folder search (likely the extended one)
        candidate_base_names = [os.path.basename(f).replace('.txt', '') for f in [expr_anno_file, va_anno_file, au_anno_file] if f]
        longest_base_name = max(candidate_base_names, key=len) if candidate_base_names else norm_base_name
        # Find matching image folder using longest_base_name first, then others, then norm_base_name
        image_folder = None
        search_bases = [longest_base_name] + [b for b in candidate_base_names if b != longest_base_name] + [norm_base_name]
        for base in search_bases:
            for data_prefix in data_prefixes:
                candidate_folder = data_prefix + base + '/'
                if list(bucket.list_blobs(prefix=candidate_folder, max_results=1)):
                    image_folder = candidate_folder
                    break
            if image_folder:
                break
        if image_folder is None:
            skipped_base_names += 1
            print(f"Skipping normalized {norm_base_name}: No matching image folder. Tried bases: {search_bases}")
            continue
        # Load images first to get length reference
        image_blobs = [blob for blob in bucket.list_blobs(prefix=image_folder) if blob.name.endswith('.jpg')]
        image_blobs.sort(key=lambda blob: int(os.path.basename(blob.name).split('.')[0]))
        num_images = len(image_blobs)
        if num_images == 0:
            skipped_base_names += 1
            print(f"Skipping normalized {norm_base_name}: No images in folder {image_folder}.")
            continue
        # Read available labels, use dummies for missing (length = num_images)
        expr_labels = [-1] * num_images  # Dummy for missing EXPR
        va_labels = [[-2.0, -2.0] for _ in range(num_images)]  # Dummy for missing VA
        au_labels = [[-1] * 12 for _ in range(num_images)]  # Dummy for missing AU
        try:
            if expr_anno_file:
                with tf.io.gfile.GFile(f"gs://{bucket_name}/{expr_prefix + os.path.basename(expr_anno_file)}", 'r') as f:
                    expr_lines = f.readlines()[1:]
                    expr_labels = [int(line.strip()) for line in expr_lines]
            else:
                print(f"Warning for {norm_base_name}: Missing EXPR; using dummies.")
            if va_anno_file:
                with tf.io.gfile.GFile(f"gs://{bucket_name}/{va_prefix + os.path.basename(va_anno_file)}", 'r') as f:
                    va_lines = f.readlines()[1:]
                    va_labels = [[float(val) for val in line.strip().split(',')] for line in va_lines]
            else:
                print(f"Warning for {norm_base_name}: Missing VA; using dummies.")
            if au_anno_file:
                with tf.io.gfile.GFile(f"gs://{bucket_name}/{au_prefix + os.path.basename(au_anno_file)}", 'r') as f:
                    au_lines = f.readlines()[1:]
                    au_labels = [[int(val) for val in line.strip().split(',')] for line in au_lines]
            else:
                print(f"Warning for {norm_base_name}: Missing AU; using dummies.")
        except Exception as e:
            print(f"Error reading annotations for normalized {norm_base_name}: {e}")
            skipped_base_names += 1
            continue
        # Check lengths (now with dummies, should match if read succeeded)
        if len(expr_labels) != num_images or len(va_labels) != num_images or len(au_labels) != num_images:
            print(f"Mismatch in normalized {norm_base_name}: {num_images} images vs {len(expr_labels)} expr, {len(va_labels)} va, {len(au_labels)} au. Skipping.")
            skipped_base_names += 1
            continue
        included_folders += 1
        print(f"Included normalized {norm_base_name} with {num_images} pairs (folder: {image_folder}, EXPR: {expr_anno_file}, VA: {va_anno_file}, AU: {au_anno_file}).")
        for img_blob, expr_lbl, va_lbl, au_lbl in zip(image_blobs, expr_labels, va_labels, au_labels):
            image_path = f"gs://{bucket_name}/{img_blob.name}"
            if expr_lbl == -1 or -2 in va_lbl or -1 in au_lbl:
                has_negative_one = True
            pairs.append((image_path, {'expr': expr_lbl, 'va': va_lbl, 'au': au_lbl}))
    if has_negative_one:
        print(f"Warning: Invalid labels found. Will mask them in loss.")
    print(f"Skipped {skipped_base_names} base names due to missing files, errors, or mismatches.")
    print(f"Included {included_folders} videos/folders.")
    return pairs

# Always collect pairs and cache them (force regeneration)
pickle_path = '/content/drive/My Drive/unified_mtl_pairs_cache.pkl'
print("Collecting training pairs...")
train_pairs = get_dataset_pairs(bucket, train_annotation_prefix, train_va_prefix, train_au_prefix, data_prefixes, is_train=True)
print(f"Found {len(train_pairs)} training pairs.")
print("Collecting validation pairs...")
val_pairs = get_dataset_pairs(bucket, val_annotation_prefix, val_va_prefix, val_au_prefix, data_prefixes, is_train=False)
print(f"Found {len(val_pairs)} validation pairs.")
with open(pickle_path, 'wb') as f:
    pickle.dump((train_pairs, val_pairs), f)
print("Saved unified pairs to cache.")